[
  {
    "person_id": 1,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It creates an automatic audit trail and visual dependency map for messy spreadsheets and mixed code files so you can track changes and understand inherited work.",
    "intent": "agree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The 'Receipt' headline is trying too hard to be clever and it doesn't land. Also 'Before you touch a cell' feels like AI-written copy. The institutional pricing jump from $149/month to $2,500/year is confusing math and seems arbitrary.",
    "objections": "I need to know if this actually works with my VBA macros and nested INDIRECT formulas. The health check sounds nice but what does it actually flag? Does it slow down my workflow? And honestly, $29/month is a lot when I'm already paying for other tools out of pocket and my company won't reimburse this.",
    "dealbreaker": true,
    "dealbreaker_reason": "At $29/month I need to be absolutely sure this won't break my existing workflows or add friction to my daily work, and there's not enough detail here about how it handles complex Excel dependencies. I can't risk introducing a new failure point when I'm already firefighting. Also unclear if I'd need to upload files every time or if it integrates live.",
    "gut_reaction": "This actually understands my pain but I don't trust that it works as smoothly as described. Sounds like it could either save me or create a whole new category of problems.",
    "unanswered_questions": "Does it work with VBA and complex Excel functions? Is this cloud-based or local? Do I have to upload files manually every time I make a change or does it connect live? What happens to my data\u2014where is it stored? Can I export everything if I cancel? How does the 'health check' actually work with legacy formulas?",
    "price_reaction": "$29/month is steep when I'm not sure my manager will see the value or approve expensing it. That's $348/year for something that might just add overhead. The Team plan makes no sense for me, and the Institutional pricing seems aimed at organizations way bigger than mine.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 2,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for mixed spreadsheet and code workflows with automatic change logging.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The hero headline is trying too hard to be clever with the receipt metaphor\u2014just say what it does. Also 'most teams are up and running in an afternoon' feels like classic SaaS optimism that never pans out in reality.",
    "objections": "I need to know if this actually works with the nightmare VBA macros embedded in my files, not just 'Excel.' Does it handle password-protected sheets? What happens to my conditional formatting? The health check sounds great but I've seen tools flag things that aren't actually problems and miss the stuff that matters. And honestly, I don't trust that upload process\u2014what if it can't parse my nested INDIRECT formulas or the custom functions someone wrote five years ago?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks directly to my daily pain but I'm skeptical it can handle the actual chaos of my files. Everyone promises they work with Excel\u2014until they see MY Excel files.",
    "unanswered_questions": "What does 'scans for inconsistencies' actually catch? Does it work offline or is all my data going to your servers? What happens when the tool gets something wrong in the dependency map? Can I export everything if I need to leave? Does it slow down my actual work or is this just adding another layer I have to check?",
    "price_reaction": "$29/month is reasonable if it actually saves me the two days a month I spend on version archaeology, but I'd need the trial to prove it works with my specific mess. Team pricing seems steep for my department budget unless I can show clear ROI to my manager.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 3,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for Excel files and mixed-language data scripts that creates an audit trail so others can understand and run your work.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The health check feature in Step 1 sounds vague and overpromised\u2014what patterns actually matter and how does it know? Also Step 2 repeats the same upload concept as Step 1 which is confusing. The 'most teams up and running in an afternoon' claim feels optimistic for complex legacy systems.",
    "objections": "I need to see it actually parse one of my nightmare files before I believe it works. The healthcare testimonial is the only one that sounds real, the other two feel generic. What happens if the tool misreads dependencies or can't handle some weird VBA macro chain? Does this slow down my actual work or is it just logging in the background?",
    "dealbreaker": "false",
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my exact problem and I hate that I'm interested. But I've been burned by tools that promise to magically understand messy systems before.",
    "unanswered_questions": "How does it handle macros and VBA? What's the performance hit when I'm working in a file it's tracking? Can I exclude certain files or does it try to map everything? What does the visual flow actually look like\u2014is it useful or just decorative? How does rollback work if I need to revert?",
    "price_reaction": "$29 is reasonable to test but if it can't handle my actual file complexity it's wasted money. Team plan seems steep for 10 users\u2014that's basically SMB pricing but positioned for small teams. Institutional makes sense if it actually delivers on compliance exports.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 4,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for messy data workflows across different file types and code languages.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The 'Receipt' headline is pretentious and doesn't land. The Step 2 description promises too much magic with 'No rewrites. No migrations. No manual untangling' \u2014 I need to see proof this actually works with real legacy mess. The testimonials feel slightly too polished, like they were edited to sound better than real frustrated ops people talk.",
    "objections": "I don't believe it can actually parse my Frankenstein workbooks that reference external data sources, have macros written by someone who left in 2019, and pull from a SQL query I barely understand. The 'health check' sounds nice but what does it actually flag? Will it just tell me things are broken that I already know are broken? And who maintains this tool \u2014 if they go under, am I even worse off than I am now?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to my exact pain but I'm skeptical it can handle the actual chaos I'm dealing with. Feels like it was built for startups with Python scripts, not for someone maintaining a decade of Excel hell with Access databases and CSV exports from a system that barely runs.",
    "unanswered_questions": "Does it actually work with Excel macros and VBA? What about files that reference network drives or external databases? How does the 'visual flow' look with 40+ interconnected files? What happens to my data \u2014 is it uploaded to their servers? Can I export everything if I need to leave? What's the learning curve for my team who can barely use Git?",
    "price_reaction": "$29/month is reasonable for individual but I'd need to see it work first. Team at $149 for 10 users seems okay if it actually saves time. Institutional jump to $2,500/year feels steep and oddly positioned \u2014 are they targeting academia or corporate? Not clear who that's for.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 5,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It creates an audit trail and visual map for messy inherited data work across different file types and code languages.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The hero headline is pretentious and confusing\u2014'Every change has a Receipt' doesn't land. Step 2 repeats what was already said in the sub-headline. The CTA 'Stop managing files. Start defending results' sounds like marketing speak, not how I'd actually describe my problem.",
    "objections": "I don't understand how it actually works with my existing workflow. Do I have to upload files every time I make a change? Does it sit on top of Excel or do I work inside CleanSheet? What happens to my formulas\u2014do they still calculate normally? And the 'health check' sounds like it'll flag a hundred things I already know are janky but can't fix.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually gets my pain point about inherited chaos and being the single point of failure. But I'm skeptical it can handle the truly cursed workbooks I deal with without breaking them or making my workflow slower.",
    "unanswered_questions": "Where does my data actually live\u2014on their servers? How does version tracking work if I'm still editing in Excel? What does 'upload a zip file' mean for my daily work\u2014is this a one-time thing or constant? Does it slow down my spreadsheets? What if the dependency mapping is wrong?",
    "price_reaction": "$29/month is reasonable if it actually saves me time, but I need to see it work with my specific mess before committing. Team pricing seems steep for my department's budget. The free trial with no credit card is good\u2014I'll actually try it.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 6,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It's a documentation and version control layer that sits on top of your existing messy spreadsheets and scripts without requiring you to rewrite anything.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I keep the whole process in my head, and that is the problem.",
    "what_feels_off": "The social proof quotes feel sanitized and pitch-perfect in a way real frustrated analysts don't talk. The 'health check' language is vague corporate speak. And 'Start defending results' sounds like marketing trying too hard to be edgy.",
    "objections": "I don't believe it can actually parse my Frankenstein setup. Four years of band-aids across Excel macros, Python scripts someone wrote before they quit, and CSV exports from a database only I know how to query. The 'upload a zip file' promise sounds way too simple for what I'm dealing with. Also, $29/month adds up when I'm already paying for other subscriptions out of pocket because IT won't approve anything.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually understands my problem but I'm skeptical it can deliver on the technical promise. Parsing mixed legacy code across languages sounds like something that would cost way more than $29 if it actually worked.",
    "unanswered_questions": "What happens when it can't parse something? Do I have to fix my code first? How does it handle database connections and external data sources? What if my scripts pull from APIs or internal systems? Is there a file size limit? And who owns my data once I upload it?",
    "price_reaction": "$29/month is another subscription I have to justify or pay myself. For a tool I'm not sure will handle my actual mess. The team tier at $149 assumes I can get budget approval and convince my boss this is worth it. The institutional tier is laughable for my org.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 7,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It's version control and documentation for messy data workflows across different file types and languages.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The how it works section promises a lot but doesn't show me what the interface actually looks like or how complex the setup really is, and Step 1 and Step 2 sound like they're describing the same thing which makes me think they don't know what they're selling.",
    "objections": "I need to see proof this actually works with the nightmare Excel files I have with circular references and VBA macros from 2009, and I don't trust that uploading everything to some cloud tool won't break when it hits my actual mess, plus twenty-nine dollars a month adds up when I'm already stretched thin.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to my exact pain point but I've been burned before by tools that promise to magically understand my Frankenstein reporting system. Show me the actual product working on a real dumpster fire, not polished examples.",
    "unanswered_questions": "Does this actually handle Excel files with VBA macros, what happens to my data security when I upload proprietary client information, can I run this locally instead of cloud, and what does the learning curve really look like when my current system took me two years to master.",
    "price_reaction": "Twenty-nine dollars a month is nearly three hundred fifty a year out of pocket since my company won't reimburse tools, and I can't justify that unless I'm absolutely certain it works, the free trial better be actually free and not require my credit card.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 8,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It's a documentation and version control layer for messy multi-file data workflows that tracks every change without making you migrate off your current tools.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "too_expensive",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The multi-language parsing sounds way too good to be true\u2014my Python scripts reference files with hardcoded paths and comment-based logic that I doubt any tool can just 'map' automatically. Also 'Step 2' basically repeats Step 1 with different wording which feels like filler.",
    "objections": "Pricing is steep for individual use when I'm already stretched thin budget-wise. The $29/month adds up to $348/year which my department won't approve without proof it works, but I also don't have time to properly trial something in 14 days while hitting my actual deadlines. And I'm skeptical the upload-a-zip-and-we'll-figure-it-out thing actually works with the Frankenstein setup I have\u2014three different people's naming conventions, broken file paths, some Excel files that call VBA scripts. What happens when it can't parse something? Do I have to fix it manually anyway?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually describes my exact nightmare scenario, which is validating. But I've been burned by tools that promise to magically understand messy legacy work\u2014they usually choke on the real-world chaos and then I've wasted time on setup instead of just patching another formula.",
    "unanswered_questions": "What does it look like when the parsing fails or can't figure out dependencies? Can I export everything if I need to cancel? Does this work offline or is my data going to some cloud server, and if so, what are the security protocols? How does the 14-day trial actually work if I'm too busy during those two weeks to properly test it?",
    "price_reaction": "Twenty-nine dollars a month is a hard sell when I'm already paying for my own software subscriptions my org won't cover. I'd maybe consider $15. Team pricing at $149 is $15 per person which seems reasonable but I'd have to convince my manager and that's a whole political process. Institutional pricing might actually work if I could get it approved through our research budget but $2,500 upfront is a tough sell without a pilot.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 9,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It logs every change you make to spreadsheets and code so you can trace back what broke and when, across multiple file types and languages.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I keep the whole process in my head, and that is the problem.",
    "what_feels_off": "The Step 2 description is almost identical to Step 1 and reads like feature bloat padding. The 'health check' language feels buzzwordy. The CTA 'Stop managing files. Start defending results' sounds like it was workshopped by a copywriter who doesn't do this work\u2014I manage files AND defend results, those aren't opposites.",
    "objections": "I don't understand what happens to my files when I upload them\u2014are they on your servers? Is this cloud-only? What about data security and PHI compliance for my healthcare reports? The 'upload a zip file' thing sounds manual and clunky for something that's supposed to streamline my workflow. Also, if this works with 'everything,' I'm skeptical it works well with anything.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to my exact pain point\u2014I am the single point of failure on twelve monthly reports. But I'm wary of tools that promise to 'make sense of' complex legacy work, because they usually don't.",
    "unanswered_questions": "Where does my data go? Is this self-hosted or cloud? What does 'compliant audit exports' actually mean\u2014compliant with what? How does it handle proprietary or sensitive data? Does it actually execute my code or just visualize it? What happens if your company goes under?",
    "price_reaction": "Individual tier is reasonable for testing. Team tier seems steep for 10 users when I work alone most of the time. Institutional pricing might make sense if I could get my org to bite, but I'd need way more security and compliance info first.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 10,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It's a version control and documentation layer that sits on top of your existing Excel, scripts, and data files to track changes and map dependencies.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The 'Receipt' headline is trying too hard to be clever and doesn't immediately make sense. Also, 'Stop managing files. Start defending results' feels like marketing speak\u2014I'm not trying to defend results, I'm trying to survive my workload.",
    "objections": "I need to see what the interface actually looks like when it 'maps dependencies' and shows 'visual flow'\u2014those could be useless diagrams or actually helpful. Also not clear if this lives in the cloud or local, and what happens to my sensitive program data. Does IT need to approve this? And honestly, will this actually work with the Frankenstein VBA macros I inherited?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to my exact problem\u2014inheriting other people's messes and being the only one who knows how things work. But I've seen tools promise to 'make sense of' legacy systems before and they just add another layer of complexity.",
    "unanswered_questions": "Where does my data actually go? Is this cloud-based or can I run it locally? Will it actually parse VBA macros or just skip them? What does 'health check' actually flag\u2014is it useful or just noise? How does the trial work if I have to upload real work files to test it properly?",
    "price_reaction": "$29/month is reasonable if it actually saves me the late nights, but I'd need to expense it and my manager will ask for ROI proof. The institutional tier seems aimed at places with budget\u2014I'm not in that world.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 11,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for data projects that involve spreadsheets and mixed-language code.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The health check in Step 1 sounds vague and possibly AI-generated promises\u2014what patterns actually matter and how good is the scanning really? Also Step 2 repeats the upload concept from Step 1 which is confusing structurally.",
    "objections": "I'm skeptical the dependency mapping actually works across languages as seamlessly as claimed\u2014my legacy messes involve hardcoded paths, manual data transfers, and undocumented assumptions that no tool could parse. The testimonials feel slightly sanitized. I'd need to see it handle my actual chaotic scripts before trusting it.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my problem of inheriting AI-generated code I can't verify. But I'm wary of another tool promising to magically understand messy legacy work\u2014I've been burned by overpromises before.",
    "unanswered_questions": "What happens when the tool can't parse something? How does it handle code that relies on manual steps or external data sources? Does the free trial include the multi-language parsing or just basic Excel stuff? What's the learning curve to actually use the visual flow effectively?",
    "price_reaction": "$29/month is reasonable if it actually saves me validation time, but I'm an individual contributor so Team pricing doesn't apply. I'd need the trial to prove it works on real messy code, not just clean examples.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 12,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It creates an auditable version history and dependency map for mixed data workflows so you can prove what changed and why.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The health check feature in Step 1 feels oversold\u2014how does it know what patterns 'actually matter' in my specific policy context? Also Step 1 and Step 2 seem redundant or out of order. And 'complete memory' is marketing fluff.",
    "objections": "I need to know if this actually parses complex Python dependencies or just creates a file tree. Does it understand the logic flow or just track edits? The testimonials are vague about what 'made the logic readable' actually means\u2014did it generate documentation, visualize data flow, or what? I can't tell if this solves my validation problem or just gives me prettier version control.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally someone gets that I'm drowning in validation work, not generation work. But I'm skeptical this actually makes AI outputs more trustworthy versus just organizing my mess better.",
    "unanswered_questions": "Does it validate logic correctness or just track changes? What does 'maps dependencies between files' actually mean\u2014syntax parsing or semantic understanding? Can it catch when AI hallucinates a library function? How does it handle proprietary data security? What's the learning curve for the visual flow interface?",
    "price_reaction": "$29 is reasonable for a trial but I need to see it work on my actual messy inheritance before committing. Team pricing seems steep if it's just fancy Git for spreadsheets.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 13,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for mixed analytics workflows including spreadsheets and scripts.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The 'health check instantly' language feels oversold\u2014what does 'patterns that actually matter' mean concretely? Also 'makes sense of it' in the sub-headline is vague. Step 2 repeats the same promise as the sub-headline almost verbatim, which makes me wonder if there's less functionality here than the copy suggests.",
    "objections": "I need to see what the 'connected visual flow' actually looks like before I trust it can parse my Frankenstein projects. Does it integrate with existing git workflows or replace them? What happens to my files\u2014are they uploaded to your servers or processed locally? The security and data residency questions aren't addressed at all.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to my exact pain point\u2014validating inherited work and AI outputs\u2014but I'm skeptical it can deliver on parsing truly messy legacy code. The testimonials feel slightly generic but plausible enough.",
    "unanswered_questions": "Where is my data stored? Can I use this offline or is it cloud-only? Does it integrate with git or replace it? What does the visual dependency map actually show\u2014function calls, data flows, both? Can I export the audit trail in a format my stakeholders will accept? What happens if I hit the limits of automated parsing\u2014do I get stuck?",
    "price_reaction": "$29/month is reasonable to test this, but I'd need to see it work on a real project before committing. The jump to $149 for team feels steep if I'm the only one who'd use it regularly. Institutional pricing seems aimed at academia, not sure how that applies to my context.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 14,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It tracks changes and maps dependencies across messy multi-language data projects so you can audit what happened and trust inherited work.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The 'health check' feature in Step 1 sounds vague and AI-buzzwordy\u2014what patterns actually matter and how does it decide? Also Step 1 and Step 2 seem weirdly duplicated or out of order. And 'made the logic readable' is doing a lot of work in that testimonial without explaining how.",
    "objections": "I don't actually know if this will handle the weird VBA macros and half-documented Python notebooks I deal with. The testimonials are suspiciously on-message. And I'm skeptical that uploading a zip file will magically map dependencies when I've seen commercial tools choke on this exact thing. What if the parsing is wrong? Do I have to fix that manually?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks directly to my exhaustion but I'm wary because I've been promised 'just upload it' solutions before and they don't handle real-world mess. The validation pain is real though.",
    "unanswered_questions": "What happens when the automated parsing gets it wrong? Can I correct the dependency map? Does this actually run my code or just document it? How does it make AI code 'readable'\u2014is that just comments or something deeper? What's the learning curve on the visual flow interface?",
    "price_reaction": "$29/month is reasonable for individual but I'd need the trial to prove it actually works on my specific nightmare projects before committing. Team pricing seems steep if only 3 of us would use it.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 15,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It creates an auditable history layer on top of existing spreadsheets and mixed-language code so you can track changes and understand inherited projects.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The health check feature in Step 1 sounds too magical\u2014what does 'patterns that actually matter' even mean? Also Step 1 and Step 2 feel redundant or out of order. And 'making sense' of a zip file with mixed languages is a huge claim with zero technical detail.",
    "objections": "I don't trust that it can actually parse my Frankenstein setup of Python scripts calling R functions with hardcoded Excel paths. The testimonials are suspiciously on-the-nose. And I need to know: does this thing execute my code or just read it? What happens to my data? Where does it live?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks directly to my pain of inheriting garbage code and spending forever validating AI fixes. But the actual mechanism of how it works is way too vague for me to trust it won't create more cleanup work.",
    "unanswered_questions": "Does it execute code or just parse it? Where is my data stored? What does the 'visual flow' actually look like? Can it handle my specific mess of Jupyter notebooks with relative imports and shell scripts? What's the learning curve to read its documentation format?",
    "price_reaction": "Individual at $29 is reasonable for a trial run. Team pricing seems steep if only half my team would use it. Institutional jump to $2500 is weird\u2014what's the in-between for like 15-20 people?",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 16,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It tracks and documents every change you make to spreadsheets and code so you can audit your work and understand what happened when things break.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The hero copy is too abstract and poetic\u2014'Every change has a Receipt' sounds like AI trying to be clever. Also Step 2 repeats what Step 1 already said about uploading files. The 'no rewrites, no migrations' promise feels too good to be true for legacy code.",
    "objections": "I don't believe it can actually parse my Frankenstein scripts where I'm calling Python from R and referencing Excel files with hardcoded paths. The testimonials are suspiciously on-brand. And $29/month adds up when I'm already paying for ChatGPT Plus and other subscriptions\u2014I need to see this work first.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to my exact pain point with AI-generated code, but I've been burned by tools that promise to magically understand messy workflows. I need proof it actually works on real garbage code before I pay.",
    "unanswered_questions": "What does 'makes the logic readable' actually mean? Does it just show me a flowchart or does it explain what the code does in plain English? How does it handle when my Python script breaks because ChatGPT used a deprecated library? Can I export the audit trail in a format my boss will actually look at?",
    "price_reaction": "$29/month feels steep for an individual tool when I don't know if it'll save me time or create more work. The institutional pricing seems more reasonable if it actually prevents the documentation nightmare when people leave, but I'm not the decision maker on that.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 17,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It tracks changes and dependencies across mixed-language data workflows so you can audit what happened and hand off work without losing context.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The health check feature in Step 1 sounds vague and probably overpromises\u2014how does it know which patterns 'actually matter' for my specific workflow? Also 'most teams are up and running in an afternoon' feels like classic sales copy exaggeration.",
    "objections": "I need to see what the dependency mapping actually looks like with real messy code, not just be told it works. The testimonials are suspiciously perfect\u2014they hit every pain point like they were written by the product team. And I don't trust 'automated suggestions' until I see exactly what they're suggesting and how often they're wrong.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually addresses my problem\u2014I'm tired of not being able to defend the Frankenstein workflows I inherit. But I've been burned by tools that promise to magically understand messy codebases before.",
    "unanswered_questions": "What does the dependency map look like when code is truly spaghetti? How good is the parsing really\u2014does it handle edge cases or just textbook examples? What happens when the tool can't figure something out? Is there an export I can actually show auditors or is it just a pretty UI?",
    "price_reaction": "$29/month is reasonable if it actually works, but I'd need the trial to prove the parsing isn't garbage. Team pricing seems steep unless my whole team has the same pain. Institutional pricing feels irrelevant to me.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 18,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It creates an audit trail and dependency map for messy data workflows across different file types and programming languages.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The 'health check' and 'makes sense of it' language is vague \u2014 what does the scan actually catch versus miss? The social proof quotes sound a bit too polished and on-message. Also 'most teams up and running in an afternoon' feels like classic underestimation.",
    "objections": "I don't trust that it can actually parse my Frankenstein setup of half-documented Python scripts calling Excel macros that reference CSVs with hardcoded paths. The demo would probably work on clean examples but choke on my actual mess. Also unclear if the 'visual flow' is actually useful or just another diagram I have to maintain.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks directly to my pain of inheriting garbage and debugging AI suggestions I can't verify. But I'm skeptical it works as advertised on real legacy chaos.",
    "unanswered_questions": "What happens when it can't parse something? Do I have to fix my code first to use this? How does the visual dependency map handle dynamic file paths or conditional logic? What's the learning curve to actually use the audit trail effectively?",
    "price_reaction": "$29 is reasonable for a trial but I'd need proof it works on my actual files first. Team tier seems expensive if half my team won't use it. No monthly institutional option is annoying.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 19,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It creates an automatic audit trail and dependency map for messy data workflows across multiple tools and languages.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The health check and dependency mapping sound ambitious to the point of being unbelievable\u2014parsing mixed-language legacy code and flagging 'patterns that actually matter' is a hard AI problem, and I've been burned by tools that promise this before. Also 'most teams are up and running in an afternoon' feels like classic SaaS overselling.",
    "objections": "I need to see proof this actually works on real legacy code, not just clean examples. The testimonials are suspiciously on-message. How accurate is the dependency mapping really? Does it handle the truly horrible nested Excel hell I deal with? And what happens when the tool itself gets something wrong in the parsing\u2014do I spend more time validating the validator?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally someone gets my specific pain point with AI-generated code validation. But I'm skeptical this tool can deliver on the technical promises\u2014automatic parsing of multi-language legacy systems is exactly the kind of thing that sounds great and works on the demo but falls apart on my actual files.",
    "unanswered_questions": "What does it look like when the parsing fails or gets it wrong? How does it handle truly broken legacy code with circular dependencies? Can I export the audit trail in formats my stakeholders actually need? What's the learning curve for the visual interface? Does it slow down my actual work?",
    "price_reaction": "Individual tier is reasonable for a trial but if this becomes another tool I have to maintain and validate, that's just shifting my time around. Would need to see ROI in actual hours saved, not just promised efficiency.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 20,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It's a version control and documentation layer for data analysis files across multiple tools and languages.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The hero headline is too abstract and poetic - 'Every change has a Receipt' sounds like marketing-speak. Also, Step 2 repeats the upload concept from Step 1 in a confusing way. And honestly, if this tool can truly parse legacy code across multiple languages and map dependencies automatically, that sounds too good to be true - I've been burned by tools promising that before.",
    "objections": "I'm already using Git for versioning my scripts, so I'm not clear what this does that Git plus a Jupyter notebook or basic documentation doesn't do. The 'visual flow' sounds nice but I need to see it to believe it actually works with messy real-world code. Also, does this mean uploading proprietary data to your servers? That's a compliance nightmare I can't even start without IT approval.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to my exact pain point with AI-generated code, but I'm skeptical it can actually deliver on the promise of making legacy multi-language code readable. I've tried tools that claim to auto-document code before and they just add another layer of confusion.",
    "unanswered_questions": "Where does my data actually live - is this cloud-based or on-premise? How does it handle sensitive data? What does the 'visual flow' actually look like with real messy code? Can I see a demo before committing? Does it integrate with Git or replace it?",
    "price_reaction": "$29/month is reasonable to try, but if it's just adding another tool to my stack without reducing my debugging time, it's not worth it. The institutional tier seems aimed at academic teams, not corporate research analysts like me.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 21,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "disagree",
    "clarity_response": "A version control and documentation layer for mixed data analysis files that logs changes and maps dependencies across multiple languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Convenience is irrelevant if I cannot defend the method.",
    "what_feels_off": "The health check claims to flag 'patterns that actually matter' with zero explanation of the detection algorithm or false positive rate. The dependency mapping for mixed-language codebases sounds technically ambitious but offers no detail on how parsing actually works or what limitations exist. Social proof quotes are suspiciously vague about methods.",
    "objections": "No technical documentation linked. No explanation of how the automated scanning works under the hood. Claims to parse Python, R, MATLAB, SQL, and Excel with dependency mapping but provides zero evidence this is possible reliably. What parsing libraries are used? What's the accuracy? How does it handle edge cases in legacy code? The 'health check' is a black box. I can't sign off on a tool that makes analytical judgments about my data pipeline without transparent methodology.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot adopt a tool that performs automated analysis of my code and data quality without exposing its detection methods, validation approach, or accuracy metrics. The copy promises convenience but provides zero methodological transparency\u2014exactly what I need to defend adopting this tool to stakeholders or reviewers.",
    "gut_reaction": "This reads like it was written by someone who has never had to defend their statistical methods to a review board. Big promises about parsing multi-language codebases with no technical depth whatsoever.",
    "unanswered_questions": "What algorithms detect inconsistencies and broken references? How accurate is cross-language dependency mapping? What happens when the parser fails? Is the audit log compliant with 21 CFR Part 11 or GxP? Can I export the complete methodological pipeline in a format reviewers will accept? What's the data retention policy? Where is data stored and who has access?",
    "price_reaction": "Pricing seems reasonable for what's promised, but institutional tier at $2500/year for unlimited users actually seems suspiciously cheap if this tool does what it claims\u2014makes me question either the infrastructure costs or the actual capability.",
    "transparency_trust": 1,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 22,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It's a version control and dependency mapping tool for mixed analytical workflows that logs changes and visualizes relationships between files.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Convenience is irrelevant if I cannot defend the method.",
    "what_feels_off": "The health check claims to flag 'patterns that actually matter' but doesn't explain the detection logic or false positive rate. The testimonials are suspiciously aligned to messaging points. 'Makes the logic readable' is vague\u2014what does that mean technically?",
    "objections": "No information about the algorithms used for dependency mapping, how it parses multi-language code accurately, or what 'inconsistencies' it detects. I can't audit a tool that doesn't explain its own methodology. What happens to my data\u2014is it uploaded to your servers? What's the security model? How does the version control differ from Git with proper documentation? This feels like a wrapper around existing tools without proving it adds methodological rigor.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot defend using a tool that doesn't document its own detection methods. If I can't explain how CleanSheet identifies 'broken references' or 'formatting gaps,' I can't trust its output in an audit scenario. The copy prioritizes convenience over methodological transparency.",
    "gut_reaction": "This reads like it's solving a project management problem, not a methodological one. I need to know *how* it works, not just *that* it works.",
    "unanswered_questions": "What algorithms detect inconsistencies? How accurate is cross-language parsing? Where is data stored and processed? How does this integrate with actual version control systems? What file size limits exist? Can I export the full audit log in a machine-readable format? What happens when the tool misinterprets a dependency?",
    "price_reaction": "Institutional pricing seems reasonable if this actually works, but I'd need a proof of concept with our actual messy legacy code before committing. Individual tier is irrelevant\u2014this is an institutional risk problem.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 23,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's supposed to create version control and dependency mapping for mixed-language data analysis projects, but I can't tell what's actually happening under the hood.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "If I cannot inspect the logic, I cannot sign off on the result.",
    "what_feels_off": "The entireHow It Works section is vague hand-waving. 'Maps dependencies,' 'organizes the logic into a connected visual flow,' 'scans for inconsistencies' \u2014 how? What algorithms? What assumptions is it making about my code structure? The copy promises magic without explaining the method, which is exactly what I'm trained to distrust.",
    "objections": "Zero technical documentation. No explanation of how it parses cross-language dependencies or what 'health check' criteria it uses. No sample outputs. No discussion of edge cases. The testimonials are generic and don't address reproducibility standards I'd need to meet. What happens to my data? Where is it processed? Is the audit trail exportable in a format reviewers will accept?",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot evaluate a tool that promises to analyze my analysis without showing me its methodology. If I can't inspect how CleanSheet is making decisions about what constitutes a 'broken reference' or 'inconsistency,' I absolutely cannot use it for work that requires methodological transparency. This is the exact problem I'm trying to avoid.",
    "gut_reaction": "This reads like it was written by someone who doesn't understand that people in my role are paid to be paranoid about black boxes. Promising to 'make sense of' my messy code without explaining how is an instant red flag.",
    "unanswered_questions": "What parsing engine does it use? How does it handle ambiguous references? What standards does the audit trail meet? Can I export the dependency graph? What happens if its analysis is wrong? Is my code uploaded to your servers? What's your data retention policy? Can I validate its output against my own QA processes?",
    "price_reaction": "The pricing is reasonable if it worked as advertised, but the institutional tier at $2,500/year suggests they're targeting people who don't ask hard questions. 'Compliant audit exports' means nothing without specifying which compliance standards.",
    "transparency_trust": 1,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 24,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "disagree",
    "clarity_response": "A version control and documentation layer for multi-format data analysis files that claims to parse dependencies across languages automatically.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "Every automated suggestion requires your sign-off before anything changes.",
    "what_feels_off": "The 'health check' and dependency mapping claims are completely hand-waved\u2014no mention of what algorithms detect inconsistencies, how cross-language parsing works, or what assumptions the tool makes when interpreting legacy code. The phrase 'flags the patterns that actually matter' is meaningless marketing speak. Also 'make sense of it' is doing a lot of unexplained heavy lifting.",
    "objections": "Zero transparency about the underlying methodology. How does it parse R, Python, MATLAB, and Excel simultaneously? What heuristics determine a 'broken reference' versus intentional design? What data leaves my environment? Is this cloud-based? On-prem? How is provenance maintained when the tool itself is a black box? The testimonials mention outcomes but not technical details.",
    "dealbreaker": true,
    "dealbreaker_reason": "A tool that claims to automatically analyze and map my analytical code without explaining its own assumptions and methodological choices is exactly the kind of opaque tooling I've spent my career fighting against. I need to audit the auditor.",
    "gut_reaction": "This reads like a solution to real pain points but makes extraordinary technical claims without showing any of the work. If you can parse multi-language legacy codebases and extract meaningful dependency graphs, that's a research-grade problem\u2014show me the white paper.",
    "unanswered_questions": "What is the technical architecture? Where is data processed and stored? What parsing methodology is used for each language? How are cross-language dependencies inferred? What assumptions does the 'health check' make? Is there an API for programmatic access? Can I export the full audit log in a standard format? What happens to my IP?",
    "price_reaction": "At $29/month individual, I'd need to see a working demo with my actual files before committing. The $2,500/year institutional tier is steep for something with no technical documentation visible on the landing page.",
    "transparency_trust": 1,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 25,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It creates an audit trail and visual map for mixed analytical files and code across multiple languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Convenience is irrelevant if I cannot defend the method.",
    "what_feels_off": "The health check claims to flag 'patterns that actually matter' but gives no methodology for how it determines importance\u2014this is exactly the kind of black-box scoring I'd need to audit before trusting. The testimonials are too perfect and conveniently address every objection without specifics. 'Made the logic readable' is subjective and unmeasurable.",
    "objections": "No technical documentation on how it parses dependencies across languages, what algorithms determine inconsistencies, or how version control differs from Git with proper commit discipline. If I can't see the detection logic, I can't validate it. The 'health check' is doing analysis I would need to reproduce independently. Also unclear if this creates vendor lock-in for the audit trail itself.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot defend using a tool that makes methodological judgments\u2014like identifying which inconsistencies 'actually matter' or parsing complex dependencies\u2014without transparent, reproducible algorithms I can audit myself. This adds an unvalidated layer between me and my work.",
    "gut_reaction": "This sounds like it's trying to automate judgment calls that require domain expertise. I'd need to see the actual detection algorithms and parsing logic before I could even consider it, and that's completely absent.",
    "unanswered_questions": "What specific algorithms detect inconsistencies? How does cross-language dependency mapping work technically? Can I export the full audit logic for independent verification? What happens to my audit trail if I stop paying? How does this compare to version control systems I can already defend?",
    "price_reaction": "Pricing is reasonable for what's promised, but institutional tier seems aimed at orgs that prioritize compliance theater over methodological rigor. I'd need the individual tier just to audit the tool itself before recommending it.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 26,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It's a version control and audit logging system for mixed data analysis files that creates dependency maps and change histories.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Every automated suggestion requires your sign-off before anything changes. You keep full methodological control \u2014 CleanSheet handles the documentation and logging so you don't have to.",
    "what_feels_off": "The health check claims to flag 'patterns that actually matter' but provides zero technical detail about the heuristics used or false positive rates. The dependency mapping across mixed languages sounds technically ambitious but there's no methodology transparency. The testimonials are too convenient and lack institutional identifiers that would let me verify them.",
    "objections": "No technical documentation linked. No explanation of how the parsing algorithms work across different languages. No sample audit report. No information about data privacy, where files are stored, or if processing is local vs cloud. The 'health check' feature is a black box\u2014what statistical methods does it use to determine inconsistencies? What are the detection thresholds? Can I customize them? I can't defend a tool whose methods I can't inspect.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot adopt a tool that automatically scans and flags data issues without transparent documentation of its detection methodology. If I can't explain how it identified a problem, I can't use it in any pipeline I'd need to defend. The copy promises auditability for my work but provides none for the tool itself.",
    "gut_reaction": "This promises exactly what I need\u2014audit trails and documentation\u2014but ironically provides no methodological transparency about its own processes. I'd need to see technical specs before I could even consider a trial.",
    "unanswered_questions": "What algorithms detect inconsistencies? How are dependencies parsed across languages? Where is data stored and processed? Can I export the complete methodology used by the health check? What's the false positive rate? Can I see a sample audit log format? Is there API documentation? What compliance standards does it meet?",
    "price_reaction": "Pricing seems reasonable for what's promised, especially the institutional tier for unlimited users. But price is irrelevant if I can't validate the underlying methods.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 27,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It's supposed to create an audit trail and dependency map for mixed spreadsheets and code files, but I don't know how it actually does that.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Every automated suggestion requires your sign-off before anything changes.",
    "what_feels_off": "The 'health check' is completely opaque\u2014what algorithms detect 'patterns that actually matter'? How does it parse dependencies across languages? The testimonials are suspiciously vague about methodological specifics. 'Made the logic readable' tells me nothing about what transformation occurred.",
    "objections": "Zero transparency about what's happening under the hood. How does it map dependencies? What parsing engine? Can I export the full provenance chain in a standard format? What assumptions does the 'health check' make? Is the history stored in a vendor-locked format or something I can audit independently? No mention of data security, encryption, or where files are processed.",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't audit the auditing tool. If I don't know what assumptions CleanSheet makes when it 'scans for inconsistencies' or 'maps dependencies,' I'm just replacing one black box with another. Show me the methodology documentation first.",
    "gut_reaction": "This sounds like it's solving a real problem I have, but the page is all benefits and no technical substance. I need to see how it works, not just what it claims to do.",
    "unanswered_questions": "What file formats for audit export? Is processing local or cloud? What dependency parsing methodology? Can I validate its 'health check' logic? What happens to version history if I cancel? Is the lineage graph reproducible outside the tool?",
    "price_reaction": "Institutional pricing seems reasonable for a team tool, but I won't pay anything until I see technical documentation and can validate the methodology myself.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 28,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's supposedly a version control and dependency mapping tool for mixed analytical workflows, but the technical architecture is completely opaque.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Upload a zip file containing your Python scripts, R code, MATLAB files, Excel workbooks, CSVs, or any combination. CleanSheet parses the whole thing, maps dependencies between files, and organizes the logic into a connected visual flow",
    "what_feels_off": "The entire 'health check' and 'dependency mapping' section makes massive technical claims with zero methodology explanation. How does it parse MATLAB? What's the AST strategy? How does it handle cross-language dependencies between Python and R? This reads like AI-generated feature promises without any engineering substance.",
    "objections": "No technical documentation linked. No methodology white paper. No examples of what the 'visual flow' actually looks like. No explanation of how it handles complex dependencies or statistical workflows. The testimonials are suspiciously vague\u2014no one mentions actual features, just feelings. Where's the provenance tracking implemented? Client-side? Cloud? What's the data residency model?",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot evaluate a tool that claims to analyze and map my code and data without showing me how it works under the hood. For $2,500/year institutional, I need to see the parsing engine, understand the analysis methods, and review security architecture. This is a black box asking me to trust it with research integrity.",
    "gut_reaction": "This promises exactly what my world needs but provides zero technical credibility to back it up. Feels like a landing page written by someone who interviewed data scientists but isn't one.",
    "unanswered_questions": "What parsing libraries does it use? How does it handle proprietary formats? What's the security model for uploaded code? How does version control work technically\u2014git-based or proprietary? What happens to my files\u2014are they stored, analyzed locally, sent to cloud? Is there an API? Can I export the dependency graph? What compliance certifications exist?",
    "price_reaction": "Pricing is reasonable if the product actually works as described, but institutional tier needs way more transparency about what 'compliant audit exports' means technically. Format? Standards? SOC 2?",
    "transparency_trust": 1,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 29,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It's a version control and documentation layer for mixed-format analytical work that logs changes and maps dependencies across files.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Show exactly what happened, why, and in what order.",
    "what_feels_off": "The health check claims to flag 'patterns that actually matter' but doesn't explain the methodology or thresholds used\u2014that's a black box I can't audit. The testimonials are anonymous beyond job titles. Step 2 promises to parse and map dependencies across Python, R, MATLAB, Excel 'automatically' which is an extremely hard technical problem being glossed over with marketing language.",
    "objections": "No transparency on how the dependency mapping actually works, what algorithms detect 'inconsistencies,' or how the tool decides what matters versus noise. I need to see the assumptions built into the scanning engine. What does 'CleanSheet parses the whole thing' actually mean technically? What heuristics? What edge cases fail? The promise feels too clean for messy reality.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot sign off on a tool that makes automated assessments of data quality and dependencies without documented, auditable methodology behind those assessments. If I can't audit your audit tool, it's useless to me.",
    "gut_reaction": "This solves a real problem I have, but the copy promises magical parsing without showing me how the magic works. I need provenance on the tool itself before I trust it with my provenance.",
    "unanswered_questions": "What specific algorithms detect issues? How does cross-language dependency mapping work technically? What are known failure modes? Can I export the audit logic itself? What data leaves my environment? Is the scanning deterministic or probabilistic?",
    "price_reaction": "Institutional pricing seems reasonable if it actually works, but I'd need a deep technical demo and documentation review before recommending a purchase order. Individual tier is affordable for testing, but the 14-day trial won't be enough to stress-test it on legacy codebases.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 30,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's a version control and dependency mapper for mixed analytics files that claims to auto-detect issues and log changes.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Convenience is irrelevant if I cannot defend the method.",
    "what_feels_off": "The health check claims are vague\u2014what algorithms detect 'inconsistencies that actually matter' and how do I validate that the tool isn't missing critical issues or flagging false positives? The social proof quotes are generic corporate testimonials without methodology details. The phrase 'patterns that actually matter' is hand-wavy marketing speak.",
    "objections": "No technical documentation on how dependency mapping works across languages, no explanation of the detection algorithms, no information on data security or where files are processed, no validation studies or accuracy metrics, and critically\u2014no way to audit the auditing tool itself. How do I verify CleanSheet's own logic is sound?",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot defend a methodology that relies on a black-box tool whose detection and mapping algorithms are unexplained. If I can't audit the auditor, I can't use it professionally. The copy asks me to trust automated scans and dependency parsing without showing me how they work or letting me validate their accuracy.",
    "gut_reaction": "This reads like a Git wrapper marketed to non-technical analysts, but the claims about automated issue detection across multiple languages require extraordinary proof. Where's the methodology paper?",
    "unanswered_questions": "What specific checks does the health scan perform? How does cross-language dependency mapping work technically? Where is data processed\u2014local or cloud? What's the false positive rate? Can I configure detection rules? Is there a validation dataset? How do I export audit trails in a format my institution's compliance office will accept?",
    "price_reaction": "Pricing is reasonable if the product works as claimed, but institutional tier needs more detail on what 'compliant audit exports' means\u2014compliant with what standards? SOC 2? 21 CFR Part 11? GDPR?",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 31,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It tracks changes in spreadsheets and code files so you have an audit trail of what was edited.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me.",
    "what_feels_off": "The whole thing reads like it was written for corporate data teams, not researchers. The 'operations analyst' and 'growth-stage startup' testimonials make it obvious this isn't for me. Also 'Stop managing files. Start defending results' sounds like consulting-speak, not how actual grad students talk about their work.",
    "objections": "I don't have Python scripts or MATLAB files or 'legacy codebases' \u2014 I have interview transcripts, survey data in SPSS maybe, and some Excel files with descriptive stats. This feels built for engineers managing production systems, not someone doing qualitative-heavy social science research. The pricing is completely out of reach for a post-bacc making maybe $35k. And I have no idea if it even works with the kind of messy, mixed-methods data I actually use.",
    "dealbreaker": true,
    "dealbreaker_reason": "$29/month is almost 1% of my monthly income just to track spreadsheet changes when I'm not even sure it handles qualitative data or SPSS files. The whole page assumes I'm working with code and 'mixed-language legacy codebases' when my biggest fear is just running a regression correctly in SPSS.",
    "gut_reaction": "This immediately felt like it wasn't for me. All the language is about 'inherited scripts' and 'code' and 'operations analysts' \u2014 I'm trying to finish a thesis with interview data and basic stats, not maintain corporate reporting systems.",
    "unanswered_questions": "Does this even work with qualitative data? What about SPSS or NVivo files? Can it help me understand if my statistical tests are set up right, or is it just version control? Why would a researcher need this versus GitHub, which is free?",
    "price_reaction": "$29/month is groceries for a week. The institutional tier at $2,500/year assumes my department has budget for this, which is laughable. Even the 'academic licensing' mention feels like an afterthought, not like this was built with actual broke grad students in mind.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 32,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's supposed to track changes and map dependencies across messy data files and scripts in different languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole thing feels written for corporate analysts, not researchers. The 'Receipt' headline is trying too hard to be clever and just confuses the value prop. The testimonials sound polished but vague\u2014no one talks about what the tool actually did, just abstract benefits. 'Stop managing files. Start defending results' sounds like consultant-speak.",
    "objections": "I don't have twelve monthly reports or a team\u2014I have one thesis chapter with messy R scripts and SPSS outputs. This feels way too enterprise for my actual problem. I need to understand my methods well enough to defend them in front of my committee, not just 'audit' them. The pricing assumes I either pay $29/month for months (adds up fast on a stipend) or convince my department to buy an institutional license, which is laughable. Also, how does it actually 'make sense of' a zip file with mixed languages? That's a huge technical claim with zero proof.",
    "dealbreaker": true,
    "dealbreaker_reason": "$29/month is $348/year minimum, which is over a week of my stipend just to track changes I could document myself if I had better habits. The product seems built for people managing inherited corporate workflows, not grad students trying to finish a thesis on a deadline with methods I'm still figuring out.",
    "gut_reaction": "This sounds like version control meets data documentation for corporate teams, not academic researchers. The price is way too high for someone in my position, and nothing about it addresses the actual intimidation I feel around methods\u2014it just tracks what I do, doesn't help me understand if what I'm doing is right.",
    "unanswered_questions": "Does it actually help me understand statistical methods or just log what I clicked? Can it integrate with qualitative data or just quant? What happens to my data\u2014is it stored on their servers? Do universities actually buy these institutional licenses or is that aspirational? How does the 'health check' work for social science analysis versus business reporting?",
    "price_reaction": "Individual tier is too expensive for a grad student budget, especially when I might only need it for 6-8 months. Team tier is irrelevant. Institutional at $2,500/year assumes my underfunded social science department has budget for software licenses, which they don't\u2014we can barely get SPSS renewed.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 33,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It tracks changes in spreadsheets and code files so you can see what was edited and go back to previous versions.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "CleanSheet turns what lives in someone's head into a documented, visual history \u2014 so the team can run it, not just the original author.",
    "what_feels_off": "This feels written for IT managers at corporations, not doctoral students doing qualitative work with some stats. The testimonials are all from analysts and directors with teams and budgets. The language is very corporate\u2014'audit layer,' 'institutional knowledge,' 'compliant audit exports'\u2014it sounds like enterprise software, not something that would help me understand my own thesis data.",
    "objections": "I don't have twelve monthly reports or a team or legacy codebases. I have one messy thesis dataset, some SPSS output I don't fully trust, and maybe some Excel files. This seems like overkill. Also, $29/month adds up when I'm on a TA stipend, and I don't know if seeing a change history will actually help me understand what my statistical tests mean.",
    "dealbreaker": true,
    "dealbreaker_reason": "The pricing is aimed at professionals with budgets, and the entire framing is about 'teams' and 'institutional workflows.' I'm one person trying to finish a dissertation, not manage a healthcare system's reporting infrastructure. Nothing here speaks to whether this will help me actually understand my statistics or just track changes I make.",
    "gut_reaction": "This reads like version control software dressed up for people who work with data at companies. I came from an Instagram ad, so I expected something friendlier and more educational, not enterprise audit trails.",
    "unanswered_questions": "Will this actually help me understand what my statistics mean, or just show me what changed? Does it explain anything, or just track it? Can it handle qualitative coding software exports? What happens to my data\u2014is it stored on their servers?",
    "price_reaction": "$29/month is almost $350 a year for a grad student budget, and the 'Institutional' tier at $2,500 suggests this is really aimed at funded organizations. The individual tier feels like an afterthought.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 34,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation system for data analysis files across multiple languages and tools.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole thing reads like AI-generated SaaS copy\u2014tons of jargon about 'audit trails' and 'methodology' but I still don't actually understand what I'm looking at or how it would help me finish my thesis. The testimonials sound fake, especially that grant committee one. And 'Stop managing files. Start defending results' feels like generic startup-speak that doesn't mean anything concrete.",
    "objections": "I'm a single grad student on a thesis deadline, not managing a team's legacy codebase. The Individual tier is $29/month which is real money when I'm living on a fellowship stipend. I don't know if this actually helps me understand my statistical methods better or if it's just fancy file organization. The 'health check' sounds useful but I have no idea what it actually does or shows me. Does it teach me anything or just track changes?",
    "dealbreaker": true,
    "dealbreaker_reason": "$29/month is too much when I don't even know if this solves my actual problem\u2014understanding methods under deadline pressure. This seems built for teams and institutional users, not solo researchers trying to learn. I need office hours and peer help to understand what I'm doing, not an audit trail of my confusion.",
    "gut_reaction": "This feels like enterprise software pretending to be for individuals. I can't tell if it would actually help me understand my analysis or just create fancy documentation of work I already don't fully grasp.",
    "unanswered_questions": "Does this actually help me understand statistical methods or just track what I did? Can it explain why certain approaches are correct? Does it integrate with actual learning resources? What does the 'health check' concretely show? Is there student pricing?",
    "price_reaction": "$29/month is groceries for a week. The Individual tier feels like a psychological anchor to make the Team tier look reasonable. No student discount mentioned despite the academic institutional tier existing. I'd need to be absolutely certain this helps me finish faster to justify that cost.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 35,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control system for data files and code that logs changes and makes messy projects easier to audit.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The whole thing reads like it's written for corporate data analysts or operations people, not researchers. The testimonials are all from business contexts. The 'Receipt' headline is trying too hard to be clever and just confused me. The language about 'defending results' feels adversarial in a way that doesn't match academic work.",
    "objections": "I don't know if this actually helps me understand statistics or if it just tracks what I'm doing. I need help interpreting my analysis and making sure I'm using the right tests, not just documenting my mistakes in an audit trail. Also $29/month adds up when I'm on a grad student stipend and I don't even know if this works with SPSS or NVivo or the qualitative coding software I actually use.",
    "dealbreaker": true,
    "dealbreaker_reason": "It doesn't address my actual problem which is understanding what statistical methods to use and whether I'm doing them correctly. This is just fancy documentation for people who already know what they're doing. I need a tool that teaches or validates my approach, not one that creates a paper trail of my confusion.",
    "gut_reaction": "This feels like it's solving a corporate compliance problem, not a research problem. I don't have a team of analysts or inherited code\u2014I have thesis data and imposter syndrome about whether I'm analyzing it right.",
    "unanswered_questions": "Does this actually help me understand my data or just track my edits? Does it work with qualitative analysis software? Can it flag when I'm using the wrong statistical test? Is there educational support or just audit logging?",
    "price_reaction": "$29/month is $348 a year which is a lot on my budget for something that might just be Git for spreadsheets. The institutional license is way out of reach and my department wouldn't pay for this when they barely fund conference travel.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 36,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It tracks changes in spreadsheets and code files so you can see what changed and when.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole thing feels like it's written for corporate analysts and tech people who work with legacy code systems, not researchers. The language is all about 'operations,' 'institutional knowledge,' 'admin controls,' and 'audit exports' \u2014 none of which are my problems. The testimonials are from healthcare systems and startups, not academics.",
    "objections": "I don't have Python scripts and Excel workbooks working together. I have interview transcripts, field notes, and maybe some basic Excel sheets with demographic data. This feels like overkill for qualitative work, and I'm not sure it even handles the kind of data I work with. The 'health check' for broken references sounds like it's scanning formulas, not helping me organize my analysis framework or coding scheme.",
    "dealbreaker": true,
    "dealbreaker_reason": "This isn't built for social science research. It's built for people managing complex computational workflows with code dependencies. I don't need to 'validate AI-generated code' or track 'which formula changed' \u2014 I need help organizing my qualitative analysis and maybe some basic stats. The individual plan at $29/month is expensive for a grad student stipend when I'm not even sure it addresses my actual workflow.",
    "gut_reaction": "This looks like version control for business analysts who inherited messy Excel systems. I'm a social science doctoral student \u2014 my 'data work' is completely different and this page doesn't speak to me at all.",
    "unanswered_questions": "Does this work with qualitative data at all? Can it handle NVivo files, interview transcripts, or coding schemes? What does 'health check' mean for non-computational research? Is there actual academic pricing or support for dissertation-level research?",
    "price_reaction": "$29/month is a lot when I'm on a TA stipend and not convinced this is for me. The 'institutional' tier mentions academic licensing but at $2,500/year that's clearly for departments, not individual students. Where's the student pricing?",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 37,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's a version control and dependency tracker for data analysis files across multiple programming languages and spreadsheets.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I know my research question cold, but the tooling still intimidates me.",
    "what_feels_off": "The whole thing reads like it was written for corporate analysts and data engineers, not researchers. The language is all about 'operations,' 'reporting time,' 'institutional knowledge risk,' and 'admin controls' \u2014 nothing about actually doing research or understanding my data. The testimonials are from healthcare systems and startups, not grad students or academics. It feels like enterprise software cosplaying as a research tool.",
    "objections": "I'm a solo researcher transitioning to dissertation, not managing a team's monthly reports. I don't have 'legacy codebases' \u2014 I have one messy R script I'm terrified to touch and some Excel sheets from my adviser. The 'Individual' tier is still $29/month when I'm living on a stipend, and I don't even know if this would help me understand statistics better or just track changes I make. Also, what does 'health check' even mean for my thesis data? Will it tell me my analysis is wrong, or just that my file names are inconsistent?",
    "dealbreaker": true,
    "dealbreaker_reason": "$29/month is a textbook I can't afford, and nothing here convinces me this solves my actual problem \u2014 which is understanding what statistical tests to run and how to interpret results, not documenting version history. This feels built for people who already know what they're doing.",
    "gut_reaction": "This is clearly not for me. It's talking to people who manage workflows and teams, not someone who's anxious about whether they're even running the right regression model. The price makes it a non-starter anyway.",
    "unanswered_questions": "Does this actually help me understand my analysis, or just document it? Will it explain what my inherited R code is doing, or just show me a pretty diagram? Can it help me figure out if I'm using the right statistical approach? Is there any student pricing? What happens to my data if I can't afford to keep paying after my trial?",
    "price_reaction": "Twenty-nine dollars a month is laughable for a grad student on a $18k stipend. The 'Institutional' tier mentions academic licensing but costs $2,500/year \u2014 my department would never pay that for one student's thesis work. There's no student tier, no academic discount for individuals, nothing.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 38,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It tracks changes in spreadsheets and code files so you can see what was edited and go back to old versions.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "CleanSheet turns what lives in someone's head into a documented, visual history \u2014 so the team can run it, not just the original author.",
    "what_feels_off": "The whole thing reads like it was written for corporate data teams managing legacy systems, not researchers. The testimonials are all from analysts and directors, not actual academics doing thesis work. The 'Receipt' metaphor in the hero is confusing and feels forced.",
    "objections": "I don't have Python scripts or MATLAB files or a 'mixed-language legacy codebase' \u2014 I have SPSS output, some Excel files with my survey data, and a lot of anxiety about whether I'm running the right statistical tests. This feels like overkill for what I need, and I can't tell if it would even help me understand which statistical approach to use or just track changes I make. Also $29/month adds up when I'm on a grad student stipend.",
    "dealbreaker": true,
    "dealbreaker_reason": "This is solving a problem I don't have. I'm not inheriting someone else's messy system or managing reports for a team. I need help understanding and trusting my own statistical analysis, not tracking file versions. Git already does version control if I needed that.",
    "gut_reaction": "This is clearly built for corporate teams dealing with inherited spreadsheet nightmares. I'm a solo researcher who needs to understand statistics, not audit a multi-person workflow.",
    "unanswered_questions": "Does this actually help me understand which statistical tests to use? Can it check if my analysis approach is valid for my research question? Does it work with SPSS or just the languages listed? What does 'health check' mean for qualitative coding?",
    "price_reaction": "$29/month is almost $350/year on a graduate stipend for what sounds like fancy version control. The institutional tier at $2,500 might work if my department bought it, but I doubt they would for one grad student.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 39,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control system for spreadsheets and data scripts that tracks changes and keeps audit trails.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me.",
    "what_feels_off": "The whole thing reads like it's for corporate analysts and IT departments, not researchers. All the testimonials are from business roles. The language about 'institutional licenses' and 'compliant audit exports' feels aimed at organizations with procurement budgets, not fellows like me trying to finish a study.",
    "objections": "I don't have inherited codebases or broken workflows from multiple people\u2014I have my own messy qualitative coding and some basic stats I'm nervous about. This feels like overkill for someone doing their first independent study. The 'health check' sounds scary\u2014what if it flags things I don't understand how to fix? And $29/month adds up when I'm on a fellowship stipend.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This looks built for teams managing legacy corporate systems, not a solo researcher trying to get through thesis analysis. I came from an Instagram ad but nothing here speaks to my actual problem\u2014learning to trust my own statistical work.",
    "unanswered_questions": "Does this actually help me understand statistics better or just track my mistakes? What happens to my data when I upload it? Is there any educational component or does it just assume I know what I'm doing? Can it explain what the 'health check' findings mean in plain language?",
    "price_reaction": "$29/month is nearly $350/year on a fellowship budget for something I might only need for 6-8 months during analysis. The institutional tier is laughable\u2014my department doesn't have that kind of money for one fellow's project.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 40,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It tracks changes in spreadsheets and code files so you have an audit trail of what happened.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I do not want to fake competence in software just to finish my thesis.",
    "what_feels_off": "The whole thing reads like it was written for data engineers at tech companies, not researchers. The testimonials are all from analysts and operations people. The language about 'institutional knowledge' and 'person-dependent workflows' feels like corporate consulting speak. Who are the 'teams' you keep mentioning? I work alone on my dissertation.",
    "objections": "This feels built for people who already know Python, R, MATLAB, and SQL. I barely use Excel and I'm certainly not uploading 'legacy codebases' or 'mixed-language' anything. I have interview transcripts, survey data in SPSS maybe, and notes. The examples don't match my world at all. Also $29/month adds up when I'm on a TA stipend and this feels like overkill for what I actually need.",
    "dealbreaker": true,
    "dealbreaker_reason": "Nothing on this page suggests it understands qualitative research or the actual anxiety I have about statistics. It's all about code, scripts, formulas, and technical debt. I don't have 'inherited scripts' \u2014 I have a dataset I'm scared to analyze wrong. This product isn't for me.",
    "gut_reaction": "This is clearly targeting corporate data teams or maybe bioinformatics labs. I kept waiting for something that acknowledged qualitative work or basic statistical analysis anxiety, but it never came. Wrong audience entirely.",
    "unanswered_questions": "Does this even work with qualitative data? What about SPSS or NVivo or the tools social scientists actually use? Can it help me understand if I'm running the right test, or just track that I ran something? What does 'health check' mean for my kind of data?",
    "price_reaction": "Twenty-nine dollars a month is $348 a year, which is real money on my income, especially for something I'd only use during analysis phases. The institutional tier assumes I have institutional buy-in, which is laughable \u2014 my department can barely afford toner. The team tier is irrelevant.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 41,
    "bucket": "budget_gatekeeper",
    "resonance": "agree",
    "clarity_response": "It creates an auditable history and visual map of messy data workflows across different file types and coding languages so projects survive staff turnover.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The headline 'Every change has a Receipt' is trying too hard to be clever and just feels vague. Also 'Stop managing files. Start defending results' sounds like marketing copy instead of solving my actual problem. The mixed testimonials feel a bit manufactured\u2014why is a growth-stage startup analyst lumped with a public health nonprofit?",
    "objections": "I need to know if this actually works with our specific grant reporting templates and whether the audit trail meets funder requirements. The 'health check' feature sounds good but I don't know if it catches the issues we actually have. Also unclear if the institutional license includes training materials for new staff or if we're on our own after setup.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks directly to my nightmare scenario of staff leaving mid-grant cycle. But I'm skeptical it can actually handle our Frankenstein setup without significant handholding.",
    "unanswered_questions": "Does the audit trail format meet typical grant compliance standards? Can I export documentation in a format our funders will accept? What happens to our data if we cancel? How much support comes with institutional licensing? Can new staff actually use this without training, or is that just a claim?",
    "price_reaction": "Institutional at $2,500/year is reasonable if it truly reduces consultant dependency and speeds up onboarding, but I'd need proof it works before committing to annual. The team tier seems overpriced at $149/month for 10 users compared to individual\u2014that math doesn't scale well for small nonprofits.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 42,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It creates an audit trail and dependency map for mixed data workflows including spreadsheets and code across multiple languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is trying too hard to sound profound. 'Every change has a Receipt' feels like copywriting gymnastics. Also unclear how a tool can actually parse legacy MATLAB and Python and Excel all together without serious manual setup\u2014that's a huge technical claim buried in casual language.",
    "objections": "No proof this actually works with messy real-world files. The health check and auto-parsing sounds like magic, which means it probably fails on edge cases. What happens when it can't parse something? Do I have to fix it manually anyway? Also $29/month per user adds up fast for my team, and I don't see ROI math anywhere. How much time does this actually save in hours?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to a real pain point I have with inherited reports, but I don't believe the tool can deliver on the parsing promise without a lot of configuration work they're not mentioning. Feels like the hard parts are glossed over.",
    "unanswered_questions": "What happens when the auto-parse fails? How much manual setup is really required? What does 'health check' actually flag\u2014can I see examples? Is there a learning curve for my team? What's the export format for audit trails\u2014will our compliance office accept it? Does it handle sensitive data securely?",
    "price_reaction": "Individual tier seems low-risk but Team at $149 for 10 users is actually $15/user which conflicts with the $29 individual price\u2014confusing. Institutional at $2500/year is interesting but I'd need to see a real pilot first. No SMB tier for 3-5 users, which is my actual team size.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 43,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It's a version control and audit tool for data files and analysis scripts across multiple languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is trying too hard to be clever with the receipt metaphor. The testimonials feel conveniently perfect for each objection. The 'Step 2' repeats the same message as the sub-headline and feels padded. 'Stop managing files. Start defending results' sounds like AI-written corporate speak.",
    "objections": "I don't understand what happens to my actual files\u2014do they live in your system now or stay in ours? How does this work with our existing Google Drive structure? The institutional tier is way over our annual software budget and I'd need at least 6 months of proof it works before I could justify that to a grant committee. No clear implementation timeline or what 'up and running in an afternoon' actually means for someone who inherited a mess.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to a real problem we have with staff turnover, but I can't tell if this is enterprise software dressed up as accessible or if it actually works for small teams. The price jump from team to institutional is massive and unclear.",
    "unanswered_questions": "Where does the data actually live? Is this cloud-only? What are the security and compliance specs for grant-funded work? Can I try institutional features before committing $2,500? What happens if we cancel\u2014do we lose all the audit history? How does this integrate with our existing file structure without disrupting current workflows?",
    "price_reaction": "Individual is reasonable but useless for a team problem. Team at $149/month might work but feels risky without knowing implementation cost. Institutional at $2,500/year is a non-starter without a pilot program\u2014that's a line item I'd have to justify in a grant application, and I can't do that based on a landing page.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 44,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It logs changes to spreadsheets and scripts so you can see what happened when someone edits data or code.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is trying too hard to sound clever and doesn't tell me what it does. The testimonials feel vague and convenient\u2014no real names, no verifiable organizations. The 'health check' feature sounds like it does AI analysis but then you say it doesn't make decisions, so which is it?",
    "objections": "I manage grant-funded research with strict budget oversight. $2,500/year for my team is a non-starter when I don't know if this actually integrates with our IRB systems, our data governance policies, or our existing backup procedures. I have zero idea what the implementation timeline looks like or what happens to our data when we upload it. Where is it stored? Who can access it? What if we need to cancel\u2014do we lose the audit trail we built?",
    "dealbreaker": true,
    "dealbreaker_reason": "No information about data security, compliance standards, or what happens to our files when uploaded. I work with human subjects data under federal grants. I cannot put anything into a tool without knowing it meets NIH/NSF data management plan requirements and HIPAA considerations. This is a liability I can't take on without those answers up front.",
    "gut_reaction": "This sounds like version control for spreadsheets, which we need, but I have no confidence this won't create more overhead than it solves. The price jump from Team to Institutional is massive and I don't see what justifies it.",
    "unanswered_questions": "Where is data stored and who has access? What compliance certifications do you have? What's the onboarding time cost for my team? Can I export everything if we cancel? Does this actually work with restricted/sensitive data? What's the learning curve for non-technical staff?",
    "price_reaction": "The institutional tier is the only one that fits my use case but $2,500/year is 8-10% of some of our smaller grant budgets. I'd need to see ROI proof and a pilot period. The individual tier is irrelevant to me and Team caps at 10 users which won't cover a multi-year research program.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 45,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It's a documentation and version control layer for mixed data analysis files that tracks changes and creates audit trails.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is too clever and vague. The 'health check' feature appears in Step 1 then disappears\u2014unclear if that's the core value or a side feature. Social proof quotes sound polished but don't include specific org names or verifiable details. The CTA 'Stop managing files. Start defending results' feels like marketing speak, not how I'd describe my actual problem.",
    "objections": "No clarity on what happens during implementation\u2014do I need IT approval? How does this integrate with our existing grant reporting requirements? What's the actual time investment to get twelve monthly reports uploaded and functional? The institutional tier is $2,500/year but I have no idea if that covers compliance documentation I'd need for NIH or NSF grants. No mention of data security, hosting location, or whether this meets federal data handling requirements.",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't justify $2,500/year from grant funds without understanding compliance certification, implementation timeline, and whether this actually reduces reporting burden enough to offset the cost. I need a pilot with measurable ROI before committing budget, and there's no clear path to prove value within our grant cycle constraints.",
    "gut_reaction": "This might solve a real problem but feels like it's selling a vision rather than a proven workflow replacement. I've been burned by tools that promise easy integration and turn into multi-month implementation projects.",
    "unanswered_questions": "Does this meet federal grant compliance requirements? What's the actual implementation timeline for a research team with legacy MATLAB and R code? Can I export audit trails in formats our grant administrators accept? What happens to our data if we cancel? Is there a pilot program or academic discount beyond just 'academic licensing'?",
    "price_reaction": "Individual tier is irrelevant to me. Team tier at $149/month for 10 users might work but that's still $1,788/year versus the $2,500 institutional\u2014why would I not just pay the extra $712 for unlimited users? The institutional price feels arbitrary and I have no comparable benchmark. Need to see a cost-benefit analysis showing how many reporting hours this actually saves.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 46,
    "bucket": "budget_gatekeeper",
    "resonance": "agree",
    "clarity_response": "It logs and tracks every change made across spreadsheets and scripts so there's an audit trail when people leave.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "too_expensive",
    "strongest_line": "If it saves staff hours and survives turnover, I can justify the spend.",
    "what_feels_off": "The health check feature gets mentioned twice in Steps 1 and 2 which feels redundant and makes me wonder if the product is thinner than they're making it sound. Also 'Stop managing files. Start defending results' sounds like consultant-speak, not how real people talk.",
    "objections": "The institutional tier is $2,500/year for unlimited users but I have a small team of 4-5 people, so I'm stuck paying $149/month ($1,788/year) when I should be closer to the individual tier price point. There's a huge gap between 10 users and unlimited. Also, I need to know if this actually works with our grant compliance requirements before I can even trial it\u2014'compliant audit exports' is too vague. What standards? Which funders have accepted this?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my exact pain point with inherited reports and turnover documentation. But the pricing feels like it's built for tech companies, not nonprofits operating on restricted grants.",
    "unanswered_questions": "Does this meet specific grant compliance standards like federal audit requirements? Can I export in formats my grant officers actually want? What happens to my data if I cancel? Is there a nonprofit discount or a middle tier for small teams under 10?",
    "price_reaction": "$149/month is $1,788/year which is nearly what the institutional tier costs, but we only have 5 people who'd use this. The institutional pricing only makes sense at scale. I need a tier between Team and Institutional for small nonprofits\u2014maybe $800-1000/year for up to 15 users.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 47,
    "bucket": "budget_gatekeeper",
    "resonance": "agree",
    "clarity_response": "It adds an audit trail and version control layer on top of existing spreadsheets and scripts without requiring migration.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The 'health check' feature description is vague about what it actually detects and how accurate it is. The testimonials are suspiciously on-message and could easily be manufactured. Step 2 promises to parse 'any combination' of languages which sounds oversold\u2014how well does this actually work with niche or old code?",
    "objections": "Institutional tier at $2,500/year is steep when I need to prove value first. No clear indication of implementation time or whether this requires IT approval for data security. The 'most teams up and running in an afternoon' claim conflicts with the complexity of what it's supposedly doing. I need to see a detailed security and compliance doc before I can even pilot this.",
    "dealbreaker": true,
    "dealbreaker_reason": "Cannot justify $2,500/year institutional cost without a proof-of-concept on the team tier first, but team tier at $149/month for up to 10 users means I'm paying $1,788/year and still don't get compliant audit exports\u2014which is the entire reason I'd need this for grant reporting. The pricing structure forces me to overpay or get incomplete functionality.",
    "gut_reaction": "This actually speaks to my problem with staff turnover and undocumented workflows, but the pricing makes me either commit big upfront or settle for a version that doesn't solve my compliance needs. I'm stuck.",
    "unanswered_questions": "What does 'compliant audit exports' actually mean and why is it locked behind the institutional tier? What are the data security protocols? Does this require cloud upload or can it run locally? What happens to my data if I cancel? How long does real implementation take for a team of 6 with mixed Excel/Python workflows?",
    "price_reaction": "Individual tier is irrelevant to me. Team tier seems almost workable until I realize audit exports\u2014the core value prop for my grant compliance\u2014are gated behind institutional. The institutional price isn't outrageous for unlimited users, but I can't get budget approval without piloting first, and the pilot tier doesn't give me the features I need to prove ROI.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 48,
    "bucket": "budget_gatekeeper",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for mixed data workflows including spreadsheets and code in multiple languages.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The health check step and step 2 seem to describe the same thing twice, and I'm skeptical about the 'instant' health check claim\u2014what does that actually flag? Also 'made the logic readable' is vague. The testimonials feel a bit too perfectly aligned with my exact pain points.",
    "objections": "I need to know what happens to our data when we upload it. Is it cloud-only? Can we self-host? What about HIPAA or IRB compliance for research data? The institutional tier at $2,500/year for unlimited users sounds too cheap to be sustainable, which makes me wonder about the company's stability. And I need IT to approve any new tool before I can even trial it.",
    "dealbreaker": "false",
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to a real problem I have with inherited analyses and staff turnover. But I'm wary of the setup time they're glossing over and whether this creates vendor lock-in once we've documented everything in their system.",
    "unanswered_questions": "Where is the data stored? What are the security certifications? Can we export everything if we cancel? How does the 'health check' work technically? What's the learning curve for non-technical staff? Does this work offline or require internet? What happens when YOU get acquired or shut down?",
    "price_reaction": "Individual tier is irrelevant to me. Team tier at $149/month might work for a pilot with my core analysts. Institutional at $2,500/year is suspiciously low\u2014that's cheaper than most single-user research software licenses. Makes me question if they'll be around in two years or if there are hidden costs.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 49,
    "bucket": "budget_gatekeeper",
    "resonance": "agree",
    "clarity_response": "It creates an audit trail and version control layer for mixed data workflows including Excel, scripts, and code without requiring migration.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The health check feature in Step 1 sounds vague and possibly oversold\u2014what does 'patterns that actually matter' mean exactly? Also 'made the logic readable' from the testimonial feels generic. The mixed-language parsing claim is bold but zero technical evidence provided.",
    "objections": "I need proof this works with the specific legacy mess we have\u2014our MATLAB code is fifteen years old and references network drives. What happens when the parsing fails? No mention of data security, institutional compliance standards, or whether this thing even runs on-premise or is cloud-only. The institutional pricing seems reasonable but I'd need to see a contract and understand support SLAs before I could defend this to finance.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to a real pain point I live with daily, but I've been burned by tools that promise magic parsing and deliver half-baked results. I need to see it work on our actual files before I'd consider budgeting for it.",
    "unanswered_questions": "Where does the data live\u2014cloud or on-premise? What are the actual compliance certifications? What happens when the parser can't make sense of legacy code? Is there an export function if we need to leave? What does 'priority support' actually mean in hours/response time?",
    "price_reaction": "Institutional at $2500/year is actually competitive if it works as described and covers unlimited users\u2014that's cheaper than most research tools we license. But I'd need a pilot period with our actual data and a clear ROI calculation tied to report prep time before I could justify it in a grant budget.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 50,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It tracks changes and dependencies in spreadsheets and code so you have an audit trail when staff turn over.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is trying too hard to be clever with the 'Receipt' thing. The testimonials feel suspiciously on-the-nose for every objection I'd have. The 'mixed languages' promise sounds like overreach\u2014parsing Python, R, MATLAB, and Excel together is a massive technical claim with zero proof shown.",
    "objections": "I don't see any proof this actually works. No demo video, no screenshot, no example of what this 'visual flow' looks like. The institutional tier is $2,500/year but I have no idea how many projects we can run or what the limits are. What does 'compliant audit exports' even mean? Which compliance standards? And if this is so good at untangling legacy code, why are the testimonials all about simple use cases like monthly reports?",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't justify $2,500/year to my grant committee without seeing the actual product work. No screenshots, no demo, no trial for institutional tier mentioned. I've been burned by tools that promise to magically understand messy legacy systems. This needs proof before I even consider it.",
    "gut_reaction": "Sounds like it's solving a real problem I have, but I don't trust that it can deliver on the technical promises. Too much marketing speak, not enough evidence.",
    "unanswered_questions": "What does the interface actually look like? How does it handle proprietary data security? Can I trial the institutional tier before committing $2,500? What are the actual limits on file size, number of projects, or complexity? Does it work offline or require cloud upload? What happens to my data if I cancel?",
    "price_reaction": "$2,500/year for unlimited users sounds reasonable if it works, but the jump from $149/month Team to $2,500/year Institutional is confusing. Why is institutional pricing annual only? What if I only need it for 10 users\u2014do I pay $1,788/year for Team or $2,500 for Institutional? The pricing structure doesn't match how grants work\u2014I need per-project or per-year clarity.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 51,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for mixed analytical workflows including spreadsheets and code.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I trust workflows that show their work, not ones that hide it.",
    "what_feels_off": "The 'health check' feature in Step 1 sounds like AI magic without explaining how it determines what 'actually matters' versus noise\u2014that's exactly the kind of black box judgment I don't trust. Also Step 1 and Step 2 seem to describe the same upload action twice which is confusing.",
    "objections": "How does it parse dependencies across languages without making assumptions? What if its mapping is wrong\u2014can I manually override? The testimonials are suspiciously on-brand. I need to see actual screenshots of the visual flow and history UI before I believe this works as described. And what happens to my data\u2014is it uploaded to their servers or processed locally?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally someone gets that the problem isn't the analysis itself, it's the institutional knowledge loss and audit trail. But I'm skeptical they can actually deliver cross-language dependency mapping without requiring rewrites.",
    "unanswered_questions": "Where is my data stored and processed? Can I export the audit trail in a format I control? How does the 'health check' actually work under the hood? What does the UI actually look like? Can I override automated mappings if they're wrong?",
    "price_reaction": "Individual tier is reasonable for trying it out. Team pricing seems high for 10 users\u2014that's $15/user/month which adds up fast. Institutional makes sense for nonprofits with grant compliance needs but I'd need to see ROI proof.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 52,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for mixed-file analytical workflows including spreadsheets and scripts.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The hero headline is too cute\u2014'Every change has a Receipt' sounds like copywriter wordplay rather than explaining what you actually do. Also skeptical about the claim that it can parse and map dependencies across Python, R, MATLAB, SQL, and Excel all in one upload\u2014that's a huge technical promise with no proof.",
    "objections": "How exactly does it parse legacy code across all those languages? That's an enormous claim. Does it actually execute the code or just read it statically? What happens if my analysis uses proprietary packages or obscure libraries? And where does my data live\u2014on your servers or mine? The security and data residency piece is completely missing.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally someone gets that the problem isn't the analysis itself, it's the institutional memory and handoff documentation. But I need to see this actually work on a real messy codebase before I believe it can handle mixed-language dependency mapping.",
    "unanswered_questions": "Where is my data stored? Can I run this locally or air-gapped? Does it execute code or just read it? What if my scripts use custom libraries or connect to internal databases? How does the dependency mapping actually work technically\u2014is it parsing imports or something deeper?",
    "price_reaction": "Individual at $29/month is reasonable for what it promises. Institutional at $2,500/year seems low if it actually delivers\u2014makes me wonder if it's not fully baked yet or if they're in land-grab mode.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 53,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for mixed data workflows, with an audit trail layer on top of your existing files.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The hero headline tries too hard to be poetic\u2014'Receipt' in quotes feels forced. Also 'Stop managing files. Start defending results' sounds like consultant-speak. And I'm suspicious that Step 2 promises to parse multi-language legacy code without rewrites\u2014that's a huge technical claim with no proof.",
    "objections": "I don't see any actual screenshots or examples of what the 'connected visual flow' looks like. The multi-language parsing claim is massive and unsubstantiated\u2014how does it handle undocumented dependencies or hard-coded paths? What happens when it can't parse something? And 'health check' is vague\u2014what specifically does it flag?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally someone who gets that the problem isn't the tools, it's the handoff and the audit trail. But I need to see proof that the multi-language parsing actually works on real messy code before I trust it.",
    "unanswered_questions": "What does the interface actually look like? How granular is the version history\u2014cell-level or file-level? What happens when it can't parse my legacy MATLAB scripts? Is there an export format that's portable if I leave? How does it handle sensitive data\u2014is everything cloud-based?",
    "price_reaction": "Individual tier is reasonable for testing. Institutional at $2500/year is actually cheaper than I expected for unlimited users\u2014makes me wonder if there are hidden costs or if they're underpricing to gain market share. Team tier seems oddly capped at 10 users.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 54,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It's a version control and documentation layer for mixed analytical workflows that tracks changes across spreadsheets and code files.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I trust workflows that show their work, not ones that hide it.",
    "what_feels_off": "The whole 'health check' and 'makes sense of it' language is vague hand-waving about what the tool actually does technically. How does it parse dependencies across languages? What's the actual method? Also 'the patterns that actually matter' \u2014 who decides what matters? Feels like AI-written marketing copy trying to sound reassuring without explaining the mechanics.",
    "objections": "I don't understand how it actually parses cross-language dependencies or what methodology it uses to flag 'inconsistencies.' The social proof quotes are suspiciously perfect \u2014 they hit every pain point like they were written by marketing. No technical depth about how the audit trail works or what format exports are in. I need to see the work, not promises about showing work.",
    "dealbreaker": true,
    "dealbreaker_reason": "If I can't understand the technical methodology behind how this tool analyzes my work, I'm not uploading anything. This is exactly the kind of black box I avoid. The copy prioritizes reassurance over technical transparency, which makes me trust it less.",
    "gut_reaction": "This sounds like someone selling me convenience when what I need is control and visibility. Too many promises about making things simple without showing me how it actually works under the hood.",
    "unanswered_questions": "What algorithms or methods does the health check use? What format is the audit trail stored in? Can I export raw logs? How does cross-language parsing actually work technically? Is any of my data processed on your servers or is this local? What does 'compliant audit exports' mean specifically?",
    "price_reaction": "Individual pricing is reasonable if it actually works. Team seems high for something I can't verify technically. Institutional pricing would never fly without a detailed technical spec and security review.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 55,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "A version control and auditing layer that sits on top of existing analytics files and code to track changes and document workflows.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I trust workflows that show their work, not ones that hide it.",
    "what_feels_off": "The 'health check' and dependency mapping claims sound good but there's zero technical detail about how it actually works\u2014parsing mixed-language codebases is hard and I don't see any evidence they can deliver on that promise. The testimonials are suspiciously vague and read like they were written by the same person.",
    "objections": "I have no idea what the actual underlying technology is. How does it parse R and Python and MATLAB simultaneously? What's the data model? Where is my data stored\u2014cloud or local? What happens if this company goes under and I've built my audit trail on their platform? The pricing for individual at $29/month adds up to $348/year for something Git and a well-maintained README could handle for free.",
    "dealbreaker": true,
    "dealbreaker_reason": "No transparency about where my data goes, how the parsing actually works, or what I'm locked into. I need explicit control and to see intermediate outputs\u2014this is a black box promising magic on messy legacy code without showing its work.",
    "gut_reaction": "This sounds like someone took Git, added an AI layer to parse code, and wrapped it in consulting-speak. I've seen too many tools promise to 'just understand' complex analytical workflows and fail.",
    "unanswered_questions": "Where is data stored? What's the actual parsing engine? Can I export everything if I cancel? What does 'visual flow' actually look like? How does it handle proprietary data formats or custom functions? What's the vendor lock-in risk?",
    "price_reaction": "Individual tier is expensive for what might just be fancy version control. Institutional at $2,500/year is laughable without seeing a working demo first\u2014that's real budget I'd have to justify without technical specs.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 56,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "agree",
    "clarity_response": "It creates an audit trail and visual map of spreadsheets and mixed-language code so you can track changes and understand inherited work.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The hero headline is trying too hard to be clever with the receipt metaphor and it made me pause instead of getting to the point. Also 'Drop in your code' appears twice as separate steps which is confusing. Step 1 and Step 2 feel redundant.",
    "objections": "I don't understand how it actually integrates with my existing workflow. Do I upload files once or does it continuously sync? What happens to my original files? Is this cloud-based, and if so, what are the security implications for sensitive lab data? The 'health check' sounds like AI magic \u2014 I need to know what rules it's using to flag issues.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my problem \u2014 inherited mess that nobody documented. But I'm skeptical about how it handles the technical details without me having to trust some black box analysis.",
    "unanswered_questions": "Where does my data live? What specific 'inconsistencies' does it flag and based on what logic? Can I see and customize those rules? Does it modify my original files or create copies? How does version control work if multiple people edit simultaneously?",
    "price_reaction": "Individual pricing is reasonable for testing. Team price jumps significantly but makes sense if it actually works. Institutional at $2,500/year is cheap if it really handles unlimited users \u2014 that makes me wonder what the catch is.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 57,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It's a version control and audit logging system for mixed data analysis files and scripts that tracks changes across Excel, Python, R, and other tools.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I trust workflows that show their work, not ones that hide it.",
    "what_feels_off": "The 'health check' and 'scans for inconsistencies' language is vague \u2014 what exactly is it checking and how? The testimonials feel polished but don't explain technical specifics. 'Made the logic readable' \u2014 how? What does that actually look like?",
    "objections": "I don't understand how it handles the actual parsing of legacy code across languages without breaking things. What if its automated scan flags false positives or misses real issues? The 'visual flow' sounds like abstraction that might hide what's actually happening in the code. I need to see the work, not a diagram of it.",
    "dealbreaker": true,
    "dealbreaker_reason": "It says 'every automated suggestion requires your sign-off' but earlier it claims to automatically parse, map dependencies, and organize logic. Which is it? If it's making interpretations of my code structure automatically, that's exactly the kind of black box I don't trust. I need to know what it's doing under the hood before I'd ever upload proprietary analysis methods.",
    "gut_reaction": "This sounds like Git meets documentation automation, which I'd want, but it's too vague about how the parsing actually works. I don't trust tools that promise to 'make sense of' complex legacy code without showing me their methodology.",
    "unanswered_questions": "What algorithms does the health check use? How does it parse mixed-language dependencies without errors? Can I export everything in plain text to verify what it logged? What happens if its interpretation of my code logic is wrong? Is my data uploaded to their servers or processed locally?",
    "price_reaction": "The individual tier is reasonable for what it claims, but I wouldn't pay for something I can't verify. The institutional pricing seems steep without knowing if it actually works as promised. No mention of data security or where files are stored.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 58,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "agree",
    "clarity_response": "It creates an automated audit trail and dependency map for spreadsheets and multi-language analytical code so you can track changes and understand inherited work.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I trust workflows that show their work, not ones that hide it.",
    "what_feels_off": "The health check feature in Step 1 is vague about what patterns actually matter and how it decides what to flag. Step 2 claims it can parse and map dependencies across multiple languages without rewrites, which sounds optimistic given how messy legacy code actually is. The testimonials are a bit too on-the-nose for my exact pain points.",
    "objections": "I need to see what the visual flow actually looks like and whether I can export the audit trail in a format I control. How does it handle proprietary data security? What happens if CleanSheet goes under and my documentation is locked in their system? The free trial is good but I want to know about data retention and export capabilities first.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally someone who gets that the problem isn't the analysis, it's the documentation and handoff. But I'm skeptical they can actually deliver on parsing mixed legacy code without manual setup work.",
    "unanswered_questions": "What does the audit trail export format look like? Can I run this locally or is it cloud-only? What's the actual parsing accuracy for mixed-language code? How granular is the version control? What happens to my data if I cancel?",
    "price_reaction": "Individual tier is reasonable for testing. Team pricing seems fair if it actually works as advertised. Institutional is steep but acceptable if the compliance exports are genuinely audit-ready and not just glorified PDFs.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 59,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It's a version control and documentation layer for mixed analytical files that logs changes and maps dependencies across languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it.",
    "what_feels_off": "The 'health check' feature sounds like AI magic that I'd have to validate anyway. 'Flags the patterns that actually matter' - who decides what matters? Also 'mixed-language legacy codebases' parsing sounds too good to be true. If it were that simple, we'd all be doing it.",
    "objections": "I don't trust that it can actually parse my messy MATLAB scripts and Excel macros correctly without breaking something. The testimonials are vague - 'made the logic readable' doesn't tell me HOW. No mention of data security or where files are stored. The pricing jump from $149/month to $2,500/year for institutional feels arbitrary.",
    "dealbreaker": true,
    "dealbreaker_reason": "No concrete evidence this works with truly legacy code, no security details, and I'm not uploading sensitive lab data to a black box that claims it can magically parse everything. I need to see exactly what it does before it touches my files.",
    "gut_reaction": "This sounds like another tool promising to solve institutional knowledge problems with automation. I've seen version control systems before - this feels like Git wrapped in buzzwords about 'audit trails' to sell to non-technical managers.",
    "unanswered_questions": "Where is my data stored? What happens to files after upload? Can I run this locally? What exactly does the 'health check' algorithm do? Does it execute my code or just read it? What if it misinterprets a dependency?",
    "price_reaction": "$29/month adds up to $348/year for something I could approximate with Git and documentation discipline. The institutional tier at $2,500 seems aimed at grant-funded teams with loose budgets, not working analysts.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 60,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "agree",
    "clarity_response": "It creates an audit trail and dependency map for mixed-language data projects so you can track changes and understand inherited work.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I trust workflows that show their work, not ones that hide it.",
    "what_feels_off": "The 'health check instantly' claim feels oversold - what patterns actually matter is judgment work, not something a scan determines. Also 'made the logic readable' quote is vague about how.",
    "objections": "I need to see what the visual flow actually looks like and whether the dependency mapping is accurate or just another layer of abstraction. How does it parse legacy code that's poorly documented? What does 'sign off' actually mean in practice?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally something that acknowledges the real problem - inherited mess and institutional knowledge loss. But I'm skeptical about the 'upload anything' promise until I see it handle actual legacy chaos.",
    "unanswered_questions": "What does the interface actually look like? How accurate is the dependency mapping on undocumented code? What happens when it can't parse something? Can I export the audit trail in a format I control? What's the learning curve for the visual flow system?",
    "price_reaction": "Individual tier is reasonable for testing. Team pricing seems fair if it actually works. Institutional at $2,500/year is cheap enough that it signals this might be early-stage and under-resourced for enterprise reliability.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  }
]