[
  {
    "person_id": 1,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across messy multi-format workflows and creates audit trails so handoffs don't break everything.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The social proof quotes are too perfectly aligned to different use cases, feels curated or fake. Also 'founding cohort perks' language is startup-y and makes me worry this will pivot or die before I get value out of migrating.",
    "objections": "Waitlist means I can't test it now when I actually need it. No pricing transparency beyond 'free trial' makes me think it'll be expensive later. I don't trust that it can actually parse my frankensteined Excel macros and Python scripts without me doing a ton of setup work they're not mentioning. The 'one week to document handoff' claim feels optimistic for how gnarly my stuff actually is.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually gets my pain but I'm skeptical it works as smoothly as described. Waitlist friction is annoying when I need solutions now, not in two months when they get around to me.",
    "unanswered_questions": "What does it cost after trial? Does it handle Excel macros and VBA or just formulas? What happens to my data - is it uploaded to their servers? How much manual tagging or setup do I actually have to do? Can it handle 50+ interconnected files or just simple workflows?",
    "price_reaction": "Free trial with no pricing shown means they'll reveal cost after I'm invested. Founding cohort language signals early stage risk. I need to know if this is $50/month or $500/month before I invest time.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 2,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across your messy multi-format workflows and logs every change so other people can pick up where you left off without breaking things.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The testimonials sound slightly polished and the 'one sprint' success story feels optimistic\u2014most inherited messes take longer to untangle than that. Also 'Change Receipt' as a feature name is trying too hard to be clever.",
    "objections": "I don't know how long this waitlist is, what the actual pricing will be after the trial, or whether this thing can really handle the Frankenstein monster of nested VLOOKUP hell and half-documented Python scripts I'm dealing with. The 'upload a zip' sounds simple but my stuff has hardcoded file paths and database connections that won't just work.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually understands my problem\u2014the handoff nightmare is real. But I'm skeptical it works as smoothly as described because every tool promises easy integration and then you spend two weeks troubleshooting why it can't read your actual files.",
    "unanswered_questions": "What does it cost after the trial? How does it handle database connections and credentials in scripts? Does it actually run the code or just parse it? What happens if my workflow has 50+ files?",
    "price_reaction": "Free trial waitlist is fine but tell me the real price range so I know if this is even in my budget before I invest time. If it's enterprise-only pricing I'm wasting my time.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 3,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across mixed file types and scripts, and logs changes so someone else can understand my workflow when I'm gone.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The social proof quotes sound polished to the point of being generic\u2014no one talks like 'sign off with confidence' in real life, and the healthcare analyst quote is suspiciously on-message. Also 'Stop reverse-engineering old files' feels like copy someone workshopped, not how I'd describe my pain.",
    "objections": "Waitlist means I can't test it now when I need it. No pricing transparency makes me wonder if this will be enterprise-expensive once the trial ends. 'Automated suggestions' is vague\u2014what exactly is it suggesting? And does this actually work with my specific mess of nested Excel references and SQL stored procedures, or just simple stuff?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually understands my problem\u2014the handoff documentation piece is real\u2014but I'm skeptical it can handle the Frankenstein setup I actually have. Feels like it might work great for clean messes, not my mess.",
    "unanswered_questions": "What happens after the free trial ends and how much will it cost? Does it handle Excel macros and VBA? Can it map SQL stored procedures or just basic queries? What's the learning curve for my team who barely understands Git? How does approval workflow actually work day-to-day?",
    "price_reaction": "Free trial waitlist is fine but deliberately hiding future pricing makes me nervous this will be $500+/month per seat once they have me hooked. Need at least a range.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 4,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across Excel, Python, SQL, and other files so you can see how your messy workflows connect and hand them off without tribal knowledge.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The social proof quotes sound slightly sanitized and the 'founding cohort perks' language feels like startup marketing speak\u2014just tell me what I get. Also 'Change Receipt' as a brand phrase is trying too hard to be clever.",
    "objections": "I don't know if this actually works with my specific nightmare\u2014Excel workbooks with VBA macros calling stored procedures that pull from three different databases. The 'upload a zip' sounds too simple for my reality. Also, waitlist means I can't even test it now when I have the problem today.",
    "dealbreaker": "false",
    "dealbreaker_reason": null,
    "gut_reaction": "This actually describes my exact pain\u2014I have Python scripts, SQL queries, and Excel macros all duct-taped together and nobody else can touch it. But I'm skeptical it can handle the real complexity without me spending weeks configuring it.",
    "unanswered_questions": "Does it handle VBA macros or database connections? What happens to my actual data\u2014does it stay local or go to your servers? How long is the waitlist really? What's the price after the free trial ends?",
    "price_reaction": "Free trial via waitlist is fine but feels like they're gatekeeping. I need to know what this costs after trial before I invest time learning another tool that my boss won't pay for.",
    "transparency_trust": "3",
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 5,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It maps and documents how your messy collection of spreadsheets and scripts connect to each other so you can hand off work without tribal knowledge.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I keep the whole process in my head, and that is the problem.",
    "what_feels_off": "The social proof quotes feel a bit too polished and hit every pain point perfectly\u2014real analysts don't talk like that. Also 'Change Receipt' as branding feels forced and I'm not sure why I'd call it that instead of version history.",
    "objections": "I need to know if this actually works with the specific mess I have\u2014will it choke on 15-year-old macros and Access databases? The waitlist model means I can't just test it now when I'm thinking about it. What happens after the free trial ends and what will pricing actually be? I don't have budget authority so if this costs real money I'm dead in the water.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually understands my problem which is rare. But I've been burned by tools that promise to make sense of legacy chaos and then can't handle the actual chaos I'm dealing with.",
    "unanswered_questions": "What does it cost after trial? Does it handle VBA macros and Access queries? What if my scripts have hardcoded paths and passwords I can't share? How much does this slow down my actual work while I'm setting it up? Will my IT department even let me upload our data somewhere?",
    "price_reaction": "Free trial waitlist is fine but tells me nothing about whether I can afford this long-term or convince my manager to pay for it\u2014I need a ballpark number.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 6,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It's a tool that maps out how all your spreadsheets and code files connect to each other and logs what changes are made.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I keep the whole process in my head, and that is the problem.",
    "what_feels_off": "The social proof quotes sound too polished and perfect, like they were written by marketing not actual analysts. 'Sign off with confidence' and 'shipped defensible results' feel like buzzwords. The 'Receipt' metaphor in the headline is trying too hard to be clever when the actual problem is simpler than that.",
    "objections": "I don't understand how it actually works with my SQL queries and Excel formulas without me rewriting everything. The 'automated suggestions' part is vague - what is it suggesting exactly? And I'm the only one on my team who touches this stuff, so the handoff angle doesn't help me right now. Also, waitlists mean I can't even test if this solves my specific mess.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This sounds like it could help with my documentation nightmare, but I'm skeptical it can actually understand the Frankenstein setup I'm running. The waitlist means I can't verify any of these claims myself before committing time to it.",
    "unanswered_questions": "Does it actually execute my code or just map files? What happens to my SQL connections and database credentials? How does it handle Excel macros and VBA? What's the actual pricing after the free trial? Can I use it solo or does it require a team?",
    "price_reaction": "Free trial via waitlist is fine but tells me nothing about what I'll pay later, which makes me assume it'll be expensive SaaS pricing my department won't approve.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 7,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It maps and documents messy multi-file workflows across different languages and tools so other people can understand them without you.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The testimonials feel a bit too perfectly worded and the 'one sprint' fix sounds unrealistic for how complicated handoff documentation actually is in practice.",
    "objections": "I need to see what the actual interface looks like and whether it can really handle my specific nightmare of Excel macros, SQL stored procedures, and Python scripts that reference each other. Also 'waitlist' means I can't even test this now, and who knows how long that'll take. What if it can't read my VBA code or handle my specific SQL dialect?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally someone gets the actual problem I have. But I've been burned by tools that promise to understand complex workflows and then choke on real-world messiness. The waitlist thing is annoying because I need this now, not in three months.",
    "unanswered_questions": "Does it actually parse VBA macros or just see Excel files as black boxes? What happens if my Python scripts use obscure libraries? How much manual work is the 'mapping' really going to require from me? What's the actual timeline for getting off the waitlist?",
    "price_reaction": "Free trial through waitlist is fine but tells me nothing about what I'll actually pay later, and if this becomes critical to my workflow I need to know if my company will balk at the real price.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 8,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across your messy multi-format workflow and logs changes so someone else can actually understand what you built.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "If I am out for one week, nobody can run this workflow without breaking something.",
    "what_feels_off": "The social proof quotes sound like they were written by the same marketing person trying to hit feature bullets\u2014no real analyst says 'sign off with confidence' or 'fragile' that cleanly. Also 'Every change has a Receipt' is trying too hard to be clever when I just need to know if this actually works.",
    "objections": "I don't see pricing, timeline to actual product access, or whether this thing chokes on the truly horrifying nested VLOOKUP nightmare I'm actually dealing with. 'Waitlist' and 'guided cohort' sounds like vaporware or months of waiting while I'm drowning now. What happens to my data when I upload it? Where does it live? Do I need IT approval?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is the first tool that actually understands my problem isn't just Excel, it's Excel plus SQL plus someone's undocumented Python script from 2019. But I'm skeptical it can handle real legacy chaos, and a waitlist doesn't help me today.",
    "unanswered_questions": "What's the actual timeline from waitlist to using this? Does it handle VBA macros? What about password-protected workbooks? Where is my data stored and do I need security review? How much does it cost after trial? Can I trial it on my actual work or just a sanitized demo dataset?",
    "price_reaction": "Free trial via waitlist is fine but tells me nothing about whether my budget can handle this long-term. If it's enterprise pricing I'm dead in the water. Need a ballpark.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 9,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across mixed file types and languages to create auditable documentation of analysis workflows.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I keep the whole process in my head, and that is the problem.",
    "what_feels_off": "The testimonials feel too perfectly aligned with the pitch\u2014especially the healthcare one that sounds like it was written by the marketing team. Also 'defensible results' is consultant-speak that makes me tired.",
    "objections": "I need to know if this actually works with my specific setup\u2014we have Excel files that reference SharePoint locations and SQL views that pull from three different databases. The 'upload a zip' sounds too simple for reality. Also, what happens when the trial ends? No pricing means I can't budget for this or justify it upfront.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is the first thing I've seen that actually understands the inherited mess problem. But I'm skeptical it can handle the complexity of what I actually maintain without me spending weeks configuring it.",
    "unanswered_questions": "What's the actual post-trial pricing? Does it handle database connections or just static files? What if my scripts reference network paths? How does approval work when I'm the only one who understands the logic anyway? What's the learning curve to get value from this?",
    "price_reaction": "Free trial waitlist is fine but I need to see real pricing before I invest time. If this is going to be enterprise SaaS pricing I can't get budget approved anyway.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 10,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "It maps and documents dependencies across mixed Excel, Python, R, SQL files so you can audit and hand off messy inherited workflows.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I maintain monthly reporting across Excel and SQL plus inherited Python scripts. The first upload showed exactly where our process was fragile.",
    "what_feels_off": "The 'Receipt' metaphor in the headline is trying too hard to be clever and just confused me at first\u2014say what it does plainly. Also 'your existing mess as-is' feels like forced relatability copywriting.",
    "objections": "Waitlist means I can't even try it now when I need it. No pricing transparency\u2014what happens after the trial? How much will this actually cost my org? And I'm skeptical it can actually parse my Frankenstein setup of macros and half-documented R scripts without breaking things.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my exact pain of inheriting garbage workflows, but I'm tired of waitlists and vague 'guided trials.' I need to know real pricing and see it work on my actual files before committing time to another tool evaluation.",
    "unanswered_questions": "What's the actual post-trial pricing? Does it handle Excel macros and VBA? What if my scripts reference local file paths or databases it can't access? How much setup time does this really take versus just documenting manually?",
    "price_reaction": "Free trial waitlist is fine but tells me nothing about whether my budget can handle this long-term. Need ballpark pricing to even know if this conversation is worth having with leadership.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 11,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It creates a visual map and audit trail of mixed-language scripts and data files so you can debug and hand off work without getting lost.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The hero headline about receipts and trails is trying too hard to sound clever\u2014just tell me what it does. Also 'existing mess as-is' feels like it's pandering. The social proof quotes are suspiciously on-message, especially the one that mirrors my exact problem.",
    "objections": "I don't know if this actually works with the debugging hell I'm in or if it just visualizes things I already know are broken. Does it help me find WHY the generated code fails or just show me a pretty map? Also, waitlist means I can't try it now when I need it.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to my problem but I'm skeptical it can actually solve the debugging loop I'm stuck in. Visualization is nice but I need something that helps me verify correctness, not just trace where things are.",
    "unanswered_questions": "Does it actually help debug AI-generated code or just map what's already there? Can it validate logic or just show dependencies? What happens when the AI code has subtle errors that don't break references?",
    "price_reaction": "Free trial waitlist is fine but also frustrating because I need help now, not whenever they get around to inviting me. No idea what paid pricing looks like after trial.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 12,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across multiple file types and languages to create an auditable trail of analysis work.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The social proof quotes are suspiciously on-message and feel crafted rather than real. The 'Receipt' metaphor in the headline feels forced. Section 2 promises I'll see all this 'in 5 minutes' but doesn't explain how that's possible with complex workflows.",
    "objections": "I don't know if this actually works with the kind of messy, half-documented workflows I deal with. The waitlist model means I can't even test it to see if it's vaporware. No pricing transparency beyond 'free trial' makes me wonder what the real cost will be. The claim that it maps cross-language dependencies sounds too good to be true\u2014that's an extremely hard technical problem.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This directly speaks to my validation hell, but I'm skeptical it can actually deliver on mapping complex dependencies automatically. Feels like it could be overselling capabilities that will disappoint in practice.",
    "unanswered_questions": "What does the actual pricing look like post-trial? How does it technically map dependencies across different languages\u2014what's the accuracy rate? What happens to my data\u2014is it stored on their servers? Can I export everything if I need to leave? What's the learning curve for my team?",
    "price_reaction": "The waitlist-only model is frustrating because I can't evaluate it now, and there's zero indication of what I'll actually pay later. This could be $50/month or $5000/month for all I know.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 13,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across mixed-language analysis files and creates an audit trail so you can hand off work without losing context.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The social proof quotes are too perfectly aligned with the marketing messages\u2014they sound like coached testimonials rather than real user feedback. Also 'your existing mess as-is' is trying too hard to be relatable.",
    "objections": "I need to see the actual interface doing this with a real mixed codebase before I believe it can parse dependencies accurately across Python, R, SQL, and MATLAB. The walkthrough better show edge cases, not a sanitized demo. Also, what happens when it can't parse something? Do I get partial maps or does it fail silently?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my pain point of spending hours validating outputs, but I'm skeptical it can deliver on the cross-language dependency mapping without significant manual correction. The approval-based workflow is good though.",
    "unanswered_questions": "How accurate is the dependency mapping in practice? What languages and patterns does it actually support versus aspirationally? What's the pricing after trial? Can I export everything if I need to leave? How does it handle complex SQL queries or obscure R packages?",
    "price_reaction": "Waitlist with no pricing disclosed makes me wary they're still figuring out the model or it's going to be expensive. I need a ballpark before investing time in onboarding.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 14,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across mixed-language analysis files and logs every change for audit trails and team handoffs.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The social proof quotes are almost too perfectly aligned with the messaging\u2014they read like they were written by the same person who wrote the copy. Also 'Change Receipt' feels like forced branding that nobody would actually say.",
    "objections": "I need to see the actual product before joining a waitlist. 'Guided free trial cohort' sounds like it's going to require meetings and handholding when I just want to test if it works. Also unclear if this actually solves the trust problem or just documents my distrust better. Does it validate the logic or just track it?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This speaks to my exact pain point, but I'm suspicious it's more documentation theater than actual validation help. I need proof it catches the errors AI makes, not just logs that I approved them.",
    "unanswered_questions": "Does it actually validate logic correctness or just track changes? What happens when dependencies span versions of packages? How does it handle when AI suggests something that looks right but is subtly wrong? Can I test this myself or do I have to sit through demo calls?",
    "price_reaction": "Waitlist with no pricing is fine for now but makes me wonder if this will be enterprise-only expensive once it launches. 'Founding cohort perks' feels like startup speak for 'we'll charge you later.'",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 15,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It organizes messy multi-language analysis projects into a documented workflow with version control and audit trails.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The social proof quotes sound a bit too polished and hit every pain point perfectly - they read like composite personas rather than real messy user feedback. Also 'Receipt' in the headline feels forced as metaphor.",
    "objections": "I don't understand how it actually 'maps dependencies' across Python, R, SQL, and Excel without me doing manual work to tell it how things connect. That's the hard part. Also unclear if this just creates more tool overhead when I'm already juggling too many tabs.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my problem of inheriting undocumented code soup and spending forever validating AI fixes. But I'm skeptical it can auto-detect cross-language dependencies without me teaching it my specific setup.",
    "unanswered_questions": "How does it actually parse and understand relationships between a Python script, an Excel macro, and a SQL query without me manually annotating? What happens when it gets the dependency map wrong? Does this add latency to my workflow or is it a separate documentation layer I update async?",
    "price_reaction": "Waitlist with no pricing shown makes me worried this will be enterprise-tier expensive once they reveal it, but at least the trial is actually free.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 16,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It maps how your messy spreadsheets and scripts connect across languages and logs changes so you can audit and hand off work without reverse-engineering everything.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The social proof quotes feel engineered to hit every persona perfectly\u2014too polished. Also 'Receipt' metaphor in the headline is trying too hard to be clever when the actual problem is simpler. The 'your existing mess' language feels like it's trying to be relatable but comes off as manufactured casual.",
    "objections": "Waitlist means I can't even try it now when I'm motivated. No clarity on what happens after free trial or what it actually costs. The 'auto-detected issues' claims sound good but I've heard this before\u2014how often does it actually work versus giving me more noise to debug? What if the dependency mapping is wrong and now I'm debugging their visualization on top of my code?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my pain point of inheriting unmaintainable workflows, but I'm skeptical it won't just add another layer of complexity. The waitlist is annoying\u2014I need solutions now, not in some undefined future cohort.",
    "unanswered_questions": "What does it actually cost after trial? How does it handle when the AI mapping gets dependencies wrong? Can I use it solo or is this team-only? What if I upload something and it just doesn't work with my specific stack\u2014am I stuck debugging their tool too?",
    "price_reaction": "Free trial via waitlist is standard but frustrating when I have problems today. Zero transparency on post-trial pricing makes me assume it'll be expensive. 'Founding cohort perks' sounds like they're not ready yet.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 17,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across messy multi-language analysis files and creates an audit trail so you can hand off work without tribal knowledge.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The 'Receipt' metaphor in the headline feels forced and doesn't land immediately. 'Every change has a Receipt' sounds like marketing trying too hard to be clever. Also 'your existing mess as-is' feels like it's trying to be relatable but comes off a bit patronizing.",
    "objections": "I need to see the actual product before joining a waitlist. The walkthrough CTA is better but I'm skeptical about how well it handles Python dependencies when even I can't always trace them. What happens when the automated mapping gets it wrong? How do I know this won't just add another layer of tooling I have to maintain? And 'guided free trial cohort' sounds like it requires hand-holding I don't have time for.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my problem but I've been burned by tools that promise to fix my workflow chaos before. I want to see proof it works on real tangled codebases, not just the happy path demo.",
    "unanswered_questions": "What happens when it can't map a dependency correctly? Does this require installing anything or changing my current setup? How does it handle custom Python libraries or R packages? What's the actual time investment to get value? Is the free trial actually useful or just a teaser?",
    "price_reaction": "Waitlist with no pricing visibility makes me nervous. I need to know if this will require budget approval before I invest time learning it. 'Founding cohort perks' tells me pricing isn't set yet which means it could get expensive fast.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 18,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It maps and documents dependencies across messy multi-language code and data files so you can audit and hand off workflows without reverse-engineering everything.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The testimonials feel a bit too perfectly worded\u2014like they were cleaned up heavily or AI-polished. Also 'Workflow Map' and 'Data Health Report' sound like every other B2B SaaS feature list. The security section reads like checkbox copy.",
    "objections": "I've seen tools promise dependency mapping before and they miss edge cases or can't handle the weird legacy stuff I actually work with. Also, waitlist means I can't even test if it works on my actual files right now. What if it chokes on my specific mess of inherited MATLAB and Python 2.7 scripts with hardcoded paths?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my pain\u2014I do waste days in that 70-to-100% trust gap. But I'm skeptical it can handle the truly janky stuff I inherit, and I can't verify that until I'm off a waitlist.",
    "unanswered_questions": "What happens if the dependency mapping is wrong or incomplete? Can I correct it manually? How does it handle really old or non-standard code? What's the actual pricing after the trial ends?",
    "price_reaction": "Free trial is fine but tells me nothing about whether I can afford it long-term or if my org will pay for it. Need real pricing to evaluate seriously.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 19,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It creates a visual map and audit trail of mixed-format analysis files so you can understand, validate, and hand off work without rewriting everything.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The social proof quotes sound a bit too polished and perfectly on-message, especially the second one that mirrors my exact pain point. Also 'defensible results' in the final CTA feels like consultant-speak trying too hard to sound serious.",
    "objections": "I need to see the actual product interface, not just promises. The 'waitlist for free trial' model means I can't even test if this actually works with my specific mess of files. Also unclear how well it handles proprietary code structures or non-standard SQL dialects. What happens when the trial ends and they hit me with enterprise pricing?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally, someone who gets that the problem isn't writing code, it's trusting and documenting it. But I've seen too many tools promise dependency mapping and deliver basic file tree diagrams.",
    "unanswered_questions": "Does it actually parse and understand code logic or just scan imports? How does it handle custom functions or spaghetti code? What's the real pricing after the founding cohort honeymoon ends? Can it detect when AI-generated code is hallucinating connections?",
    "price_reaction": "Free trial waitlist is fine but deliberately vague about what comes after. 'Founding cohort perks' usually means they haven't figured out pricing yet or it's going to be expensive once they do.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 20,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across multiple file types and languages so you can trace what changed and hand off work without losing context.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I stopped losing days validating AI-generated code because I could trace each transformation step and sign off with confidence.",
    "what_feels_off": "The testimonials feel a bit too perfectly worded for real analysts\u2014especially that last one about 'method notes and decision history' sounds like marketing copy, not how someone actually talks. Also 'workflow mapped in 5 minutes' but then 'complete first handoff documentation in the first week'\u2014which is it?",
    "objections": "I've been burned by tools that promise to understand my messy workflows but actually need everything structured first. How does it really handle Python scripts that reference hardcoded file paths or Excel files with macros? The 'auto-detected issues' sound great but I need to know if it actually catches the subtle stuff or just the obvious missing values anyone can see. And waitlist means I can't even test this to see if it works with my actual chaos.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually speaks to my problem\u2014I do lose days in that debugging hell with AI-generated code. But I'm skeptical it can handle the real mess I work with, not some sanitized demo version of messy.",
    "unanswered_questions": "Does it actually execute the code or just map static references? What happens when my Python script has dynamic imports or reads from a database? Can it handle Excel with VBA macros? What's the learning curve to get value from it versus just being another tool I have to learn and maintain?",
    "price_reaction": "Waitlist with no pricing is frustrating because I can't budget for this, but at least it's not asking for money upfront. I'm worried 'founding cohort' means they'll jack up prices once they figure out what enterprises will pay.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 21,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It's supposed to map dependencies across multiple file types and languages while creating an audit trail, but the actual analysis methodology is totally opaque.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The 'Data Health Report' claims to detect 'suspicious outliers' but gives zero information about the detection algorithm, thresholds, or false positive rates. The whole thing reads like a project management tool pretending to do statistical work. 'Surface data issues' is dangerously vague\u2014what constitutes an issue? By whose standards?",
    "objections": "No transparency on how dependency mapping actually works across languages, no information on what algorithms detect data quality issues, no validation studies or benchmarks, no explanation of how it parses different syntaxes without breaking semantic meaning. The Methods + Validation Notes link is mentioned but I can't actually see what's in there. This is a black box asking me to trust its judgment on my analysis pipeline.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot defend a method I cannot interrogate. If I can't see the detection logic, parameter settings, and validation approach before adoption, I'm introducing unauditable assumptions into my own auditable workflow. That's backwards. The irony of an 'audit tool' that won't show me its own methods is not lost on me.",
    "gut_reaction": "This solves a real coordination problem but completely dodges the methodological transparency it claims to provide. I need to see under the hood before I let any tool judge my data quality.",
    "unanswered_questions": "What algorithms detect outliers and with what sensitivity/specificity? How does cross-language parsing work without semantic errors? What assumptions does the dependency mapper make? Can I export the detection parameters? Is there published validation? What happens when it misidentifies a dependency or flags a legitimate edge case?",
    "price_reaction": "Waitlist with no pricing is fine for exploratory interest, but I'd need extensive technical documentation before committing any institutional resources or migrating real workflows.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 22,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It ingests multi-language code and data files, maps dependencies, and logs changes for audit trails.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Convenience is irrelevant if I cannot defend the method.",
    "what_feels_off": "The entire 'Data Health Report' section is vague promises\u2014what algorithms detect outliers, what constitutes 'suspicious', what's the false positive rate? The testimonials read like they were written by the same person. 'Auto-detected issues' without methodology is a red flag, not a feature.",
    "objections": "No technical documentation visible upfront, no explanation of how cross-language dependency mapping actually works, no sample audit export to evaluate, no information about how transformations are traced or what validation methods are used. 'Methods + Validation Notes' is buried as a link with no preview. I need to see the methodology before I upload proprietary analysis.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot evaluate auditable tooling without access to the actual audit methodology. Asking me to join a waitlist to maybe see how this works is backwards. Show me the technical validation documentation first, or this is just another black box I'd have to defend without being able to explain it.",
    "gut_reaction": "This promises auditability but doesn't demonstrate it. I audit tools that make claims like this\u2014where's the white paper, where's the methodology doc, where's proof this isn't just parsing files and calling it analysis?",
    "unanswered_questions": "What statistical methods detect data issues? How are cross-language dependencies actually traced\u2014AST parsing, regex, manual tagging? What happens when the mapper gets it wrong? Can I export the full methodology used in the audit trail? What's your false discovery rate on 'suspicious outliers'?",
    "price_reaction": "Waitlist-only with no pricing transparency means I can't budget or justify this to procurement. 'Founding cohort perks' sounds like beta testing their unproven system.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 23,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It's supposed to map dependencies across mixed analysis files and create audit trails, but I don't understand the actual mechanism.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The 'Data Health Report' claims to detect 'suspicious outliers' without explaining the detection logic\u2014that's exactly the kind of black box assumption I refuse to sign off on. The social proof quotes are conveniently vague about methodological validation. And 'Receipt' as a metaphor feels marketing-cute but doesn't tell me what's actually being logged or how.",
    "objections": "I cannot inspect the logic of how dependencies are mapped, how 'suspicious outliers' are flagged, or what assumptions drive the automated suggestions. The page promises auditability but doesn't show me the audit itself\u2014no technical documentation preview, no methodology white paper, no sample validation output. If I cannot see how it reaches conclusions, I cannot trust it for regulated or peer-reviewed work.",
    "dealbreaker": true,
    "dealbreaker_reason": "No access to underlying detection algorithms or validation methodology before committing to a trial. The 'Methods + Validation Notes' link is mentioned but I need to see that documentation NOW to evaluate if this tool meets methodological standards. I've seen too many tools that promise transparency but hide assumptions in proprietary logic.",
    "gut_reaction": "This sounds like it could solve a real problem, but it's asking me to trust automated detection and mapping without showing me how any of it works. That's a non-starter for anyone doing work that requires methodological defense.",
    "unanswered_questions": "What algorithms detect outliers and schema mismatches? How are cross-language dependencies parsed\u2014can I validate the dependency map against my own understanding? What's logged in the change receipt beyond surface metadata? Can I export the actual validation logic for peer review, or just a summary report? What happens when the automated detection is wrong?",
    "price_reaction": "Free trial is fine but tells me nothing about long-term cost or whether this scales to institutional budgets. Need concrete pricing before I can advocate internally.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 24,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It's supposed to create a visual map and audit trail of mixed-language analytical workflows without rewriting code.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The social proof quotes sound fabricated or heavily edited\u2014no real analyst says 'sign off with confidence' like that. The 'Data Health Report' claims to detect 'suspicious outliers' which is methodologically vague and probably statistically naive. What threshold? What distribution assumptions? The whole thing reads like it was written by someone who hasn't actually done the work.",
    "objections": "Zero technical detail on how dependency mapping actually works across languages with different execution models. No information on how it handles statistical assumptions, model parameters, or distributional choices. 'Schema mismatches' and 'suspicious outliers' detection without methodology is a red flag. I don't see how this doesn't just create another black box layer on top of my existing work.",
    "dealbreaker": true,
    "dealbreaker_reason": "No transparency on the methods used for automated detection and mapping. If I can't inspect the assumptions your tool makes about my code and data, I'm just trading one opacity problem for another. The 'Methods + Validation Notes' link is mentioned but not shown\u2014that should be front and center for someone like me, not buried as a side link.",
    "gut_reaction": "This sounds like someone built a dependency visualizer and is overselling the audit and validation capabilities. I've seen too many tools claim to 'surface data issues' without showing me their statistical methods.",
    "unanswered_questions": "What algorithms detect dependencies across languages? How do you identify 'suspicious' outliers\u2014what's the statistical method? What happens to execution context and environment variables? Can I export the full lineage graph in a machine-readable format? How do you handle stochastic processes or simulations? What about version control integration?",
    "price_reaction": "Waitlist-only pricing is fine for early stage, but I need to see actual pricing structure and whether this scales per-user or per-project before committing time to a trial.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 25,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It visualizes dependencies across multi-language analysis files and logs changes for audit purposes.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Convenience is irrelevant if I cannot defend the method.",
    "what_feels_off": "The data health report claims are vague\u2014what constitutes a 'suspicious outlier' and what statistical criteria are used? The social proof quotes feel generic and unverifiable. The phrase 'organize the workflow without forcing rewrites' is marketing fluff that tells me nothing about the underlying methodology.",
    "objections": "No technical documentation visible upfront. How does it parse cross-language dependencies\u2014static analysis, runtime tracing, heuristics? What validation methods underpin the data health detection? The 'Methods + Validation Notes' link is referenced but I cannot evaluate it now. I need to see the methodology before I upload proprietary data to a black box. The testimonials lack institutional identifiers I can verify.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot justify uploading client data to a system whose analytical methods are not documented upfront. The waitlist model prevents me from evaluating whether this tool's definitions of data quality align with defensible statistical practice. If I cannot inspect the validation logic before committing, I cannot use it.",
    "gut_reaction": "This addresses a real pain point but treats methodology as a secondary concern. I need to see detection algorithms and dependency mapping logic documented before I trust it with analysis I have to defend.",
    "unanswered_questions": "What are the exact methods for detecting data issues? How are cross-language dependencies parsed\u2014AST analysis or pattern matching? What statistical thresholds define anomalies? Is the dependency graph deterministic? Can I export the raw lineage data for independent verification?",
    "price_reaction": "Free trial via waitlist is fine but delays my ability to evaluate whether the methodology is sound, which is my primary purchase criterion.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 26,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It ingests multi-language analysis files and creates dependency maps with change logs for audit purposes.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The 'Data Health Report' claims to detect 'suspicious outliers' which is methodologically vague and concerning\u2014what distribution assumptions are being made? The testimonials feel crafted rather than authentic, especially the phrase 'sign off with confidence' which no real analyst would say without specifying what validation occurred.",
    "objections": "Zero transparency on the actual methods used for dependency mapping, outlier detection algorithms, or how schema mismatches are identified. What parser are you using for R versus Python? How do you handle namespace collisions? The Methods + Validation Notes link is mentioned but I can't see what's actually in there. 'Auto-detected issues' means nothing without knowing the detection logic and false positive rates.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot evaluate tooling for auditable workflows when the tool itself provides no methodological transparency about its own detection and mapping algorithms. Convenience is irrelevant if I cannot defend the method, and this page gives me no technical specification to defend.",
    "gut_reaction": "This sounds useful in theory but reads like it was written by someone who thinks 'audit trail' is sufficient without explaining what's actually being logged or how. I need parameter visibility and you're selling me convenience.",
    "unanswered_questions": "What specific algorithms detect outliers and schema mismatches? What are the technical specs of the dependency parser? How are cross-language variable mappings handled? What gets logged in the change receipt\u2014diffs, metadata, execution environment? Can I export the detection logic itself for peer review?",
    "price_reaction": "Waitlist model is fine but tells me nothing about long-term cost structure or whether this will price out academic or nonprofit use cases after the founding cohort.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 27,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It maps dependencies across Excel, code files, and scripts while creating an audit trail of changes and decisions.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The data health report claims to detect 'suspicious outliers' \u2014 that's a methodological minefield they're glossing over. What's the detection logic? What assumptions? The testimonials are generic and vague, particularly the 'fragile process' one. Also 'methods + validation notes' is mentioned twice but never shown or defined \u2014 if this is your key differentiator for my persona, prove it exists.",
    "objections": "I need to see actual examples of these 'Methods + Validation Notes' before trusting this. How does cross-language dependency mapping work technically? What if the tool misinterprets my R code or SQL logic? The 'data health report' sounds like black-box statistical flagging, which is exactly the kind of tool that obscures assumptions. Show me the parameters it uses to flag outliers or I assume it's garbage.",
    "dealbreaker": true,
    "dealbreaker_reason": "They keep promising transparency and auditability but won't show me what the actual audit trail or validation documentation looks like. If you can't prove your documentation exists and is methodologically sound on the landing page itself, I'm not joining a waitlist to find out it's just another black box that produces pretty charts.",
    "gut_reaction": "This speaks my language about auditability and provenance, but then fails to actually demonstrate it. I've been burned by tools that promise transparency but deliver automated magic with hidden logic.",
    "unanswered_questions": "What statistical methods underpin the 'data health report'? Can I customize detection parameters? What does a real 'Methods + Validation Note' output look like? How does it parse code semantics across languages without introducing errors? What happens when it gets dependency mapping wrong?",
    "price_reaction": "Waitlist-only with no pricing transparency is frustrating but acceptable for beta. I'm more concerned about whether the product delivers on methodology than cost.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 28,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It ingests mixed-format analysis files and maps dependencies while logging changes for audit purposes.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Show me assumptions, parameters, and provenance, then we can talk.",
    "what_feels_off": "The 'Data Health Report' claims to detect 'suspicious outliers' but provides zero detail on the statistical methods, thresholds, or false positive rates\u2014that's a red flag for any serious analyst. The testimonials are too clean and lack institutional identifiers that would let me verify them. 'Auto-detected issues' without methodology is marketing speak, not product documentation.",
    "objections": "No technical white paper or validation study linked. No information about how cross-language dependency mapping actually works\u2014is this static analysis, runtime tracing, heuristics? What does 'suspicious outliers' mean algorithmically? The 'Methods + Validation Notes' link appears twice but goes nowhere and isn't explained. I need to see the detection logic, not just promises of detection. Also unclear if this handles proprietary institutional code or just toy examples.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot evaluate a tool that claims to audit my work without first auditing the tool itself. No published methodology, no validation benchmarks, no false positive/negative rates, no handling of edge cases. The page asks me to trust automated analysis suggestions without showing me how those suggestions are derived. That's backwards for anyone doing serious quantitative work.",
    "gut_reaction": "This reads like someone identified a real pain point but hasn't done the hard work of explaining how the solution actually functions under the hood. For a product targeting analysts who care about rigor, the lack of technical depth is disqualifying.",
    "unanswered_questions": "What algorithms detect data issues? How are cross-language dependencies parsed\u2014AST analysis, regex, manual tagging? What's the accuracy rate? Can I customize detection rules? How does versioning handle merge conflicts in collaborative environments? What happens to proprietary or sensitive analysis logic? Is there a self-hosted option for regulated environments?",
    "price_reaction": "Free trial via waitlist is fine for vetting, but I'd need to see enterprise pricing and whether there's lock-in before committing a team to migration effort.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 29,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "neutral",
    "clarity_response": "It's supposed to map dependencies across multi-language analysis files and create audit trails, but I don't know what engine is doing the parsing or how it handles methodology documentation.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The 'Data Health Report' claims to detect 'suspicious outliers' but gives zero detail on the statistical methods used. The testimonials are too vague about what was actually validated. 'We fixed handoff risk in one sprint' tells me nothing about what was measured or how validation actually worked. The Methods + Validation Notes link is mentioned twice but never shown\u2014classic vaporware move.",
    "objections": "I have no idea what algorithms are detecting schema mismatches or how cross-language dependency mapping actually works. Does it parse AST? Use static analysis? Regex matching? The 'why' field in the change receipt is user-entered or automated? Who defines what's a 'suspicious outlier'? What assumptions does the workflow mapper make about execution order?",
    "dealbreaker": true,
    "dealbreaker_reason": "I audit tools for a living. This page asks me to trust a black box that claims to understand my methodology across five languages without showing me a single technical specification, validation study, or algorithmic approach. I can't hand off results I can't defend, and I can't defend a tool whose methods are hidden behind marketing copy.",
    "gut_reaction": "This reads like a product manager wrote copy about auditability without consulting anyone who actually does audits. Big promises about traceability with zero transparency about how the tool itself works.",
    "unanswered_questions": "What parsing engines handle each language? How are dependencies inferred versus declared? What statistical methods detect data issues? Can I configure detection parameters? What metadata gets captured in the audit trail? Is there version control integration? What happens to proprietary algorithms in my code\u2014does this phone home?",
    "price_reaction": "Free trial waitlist is fine but tells me nothing about future pricing structure or whether this will be affordable for small teams once the honeymoon period ends.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 30,
    "bucket": "skeptical_quant_methodologist",
    "resonance": "disagree",
    "clarity_response": "It maps dependencies across analysis files in different languages and logs changes, but I cannot tell what algorithms it uses or how it validates correctness.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The entire data health report section is vague marketing language\u2014what qualifies as a 'suspicious outlier' and by what statistical criteria? The testimonials read like copywriting templates, not real methodologists speaking. 'Auto-detected issues' without mentioning detection methods is a red flag.",
    "objections": "Zero transparency on how dependency mapping works across languages, no explanation of what algorithms detect data issues, no validation studies or accuracy benchmarks, no information on how it handles complex statistical transformations or ensures mathematical correctness. I cannot defend a method I do not understand.",
    "dealbreaker": true,
    "dealbreaker_reason": "As someone who audits analyses, I need to know the exact methodology behind automated issue detection and dependency mapping. If I cannot explain how the tool reaches its conclusions, I cannot use it for work that requires methodological defense. The lack of technical documentation available upfront is disqualifying.",
    "gut_reaction": "This reads like someone describing automation features without understanding what rigor actually requires. I need methods documentation before I even consider testing, not after I join a waitlist.",
    "unanswered_questions": "What specific algorithms detect data issues? How does cross-language dependency mapping work technically? What validation has been done on detection accuracy? What happens to statistical integrity during transformations? Where is the peer-reviewed methods paper or technical specification?",
    "price_reaction": "Free trial through waitlist is fine, but I need to see technical documentation before wasting time in a cohort. Priority for legacy workflows means nothing if the tool cannot handle methodological complexity correctly.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 31,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It organizes multiple file types and coding languages into a visual workflow map with change tracking.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Stop reverse-engineering old files. Start shipping defensible results.",
    "what_feels_off": "The whole thing reads like it's written for corporate operations analysts and data engineers, not researchers. The social proof is all business roles. The language feels very tech-corporate and nothing about this speaks to qualitative research or mixed-methods social science workflows.",
    "objections": "I don't use Python, R, MATLAB, or SQL regularly enough to have a 'mess' of legacy code. My anxiety is about learning stats tools in the first place, not managing complex multi-language workflows I've already built. This feels like overkill for someone who's still figuring out basic regression analysis. Also no pricing means I have no idea if this will be $20/month or $500/month when the trial ends.",
    "dealbreaker": true,
    "dealbreaker_reason": "This product assumes I already have complicated multi-file, multi-language workflows to organize. I'm in coursework-to-dissertation transition and still intimidated by the tooling itself. I need something that helps me DO the analysis, not manage enterprise-level handoffs. The target user is clearly someone with way more technical infrastructure than I have.",
    "gut_reaction": "This is clearly built for data teams at companies, not grad students or early-career researchers. Nothing here addresses my actual problem which is learning the stats tools in the first place.",
    "unanswered_questions": "What does this cost after the trial? Does it actually help me run analyses or just document ones I've already figured out how to do? Is there any support for learning statistical methods or is this purely workflow management?",
    "price_reaction": "Free trial is fine but hiding actual pricing makes me nervous I can't afford it long-term. Feels like enterprise SaaS pricing strategy.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 32,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It makes a visual map of how your analysis files connect across different coding languages and tracks changes for auditing purposes.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I maintain monthly reporting across Excel and SQL plus inherited Python scripts. The first upload showed exactly where our process was fragile.",
    "what_feels_off": "The whole thing feels aimed at teams and operations people, not solo researchers writing a thesis. All the social proof is from analysts at organizations with 'teams' and 'sprints' and 'handoffs'\u2014I don't have a team to hand off to, I just need to finish my analysis and defend it to my committee. The language about 'inherited workflows' and 'tribal knowledge' doesn't match my situation at all.",
    "objections": "This seems built for corporate teams doing recurring reports, not academic research. I'm not managing handoffs or monthly dashboards\u2014I'm trying to finish one analysis for my thesis defense. The 'waitlist' and 'cohort' language makes it sound like it won't be available when I actually need it, which is now. No pricing means I can't budget for this, and as a post-bacc fellow I'm broke. Also unclear if this actually helps me learn the methods or just wraps my confusion in a prettier package.",
    "dealbreaker": true,
    "dealbreaker_reason": "Waitlist access means I can't use it now when I'm under deadline pressure, and the entire pitch is for teams doing operational workflows, not solo academic researchers trying to finish a thesis. I need something I can start using today, not join a cohort for.",
    "gut_reaction": "This looks like it's solving problems for data teams at companies, not grad students trying to finish their thesis. The 'team handoff' angle is everywhere and I don't have a team.",
    "unanswered_questions": "Will this actually help me understand the statistical methods I'm using, or just document what I'm already doing? What does it cost after the trial? Can I even get access before my defense deadline? Does it work for academic research workflows or just business analytics?",
    "price_reaction": "Free trial via waitlist is frustrating because I need help now, not when they decide to let me in. No actual pricing listed means I can't know if this will be affordable on a fellowship stipend once the trial ends.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 33,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It organizes messy code and spreadsheets from multiple programs and tracks changes, but I honestly don't fully understand how.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "When someone new joins, they can navigate the logic history instead of reverse-engineering filenames and tribal knowledge.",
    "what_feels_off": "This feels like it's written for engineers and data analysts at tech companies, not researchers. The language is all about 'legacy code' and 'workflows' and 'sprints' - I don't talk like that. The social proof is all from analysts and healthcare networks, not academics doing thesis work. Also 'every change has a receipt' is trying way too hard to sound clever.",
    "objections": "I don't have a 'team' or mixed-language workflows - I have my thesis data in Excel and I'm terrified of learning R for my statistical analysis. This seems like overkill for a solo doctoral student. Also, what even is MATLAB? I don't know if this helps ME understand statistics better or just organizes files I don't have yet.",
    "dealbreaker": true,
    "dealbreaker_reason": "This product seems built for teams managing complex inherited code systems, not for a solo grad student who needs help understanding basic statistical concepts and learning one analysis tool from scratch. I need something that teaches me, not just organizes what I already know how to do.",
    "gut_reaction": "This looks like enterprise software for data teams at companies. I'm a teaching assistant trying to run my first regression analysis and I'm intimidated by stats, not by 'legacy workflows.'",
    "unanswered_questions": "Will this actually help me learn statistics or just organize files? Does it work if I'm only using Excel and maybe R? Is this even meant for individual grad students or only for teams? What does it cost after the trial?",
    "price_reaction": "Free trial is good but no actual pricing shown makes me nervous - probably means it's expensive and aimed at institutions, not broke grad students.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 34,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It organizes files and scripts from multiple languages into a visual workflow map with change tracking.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Stop reverse-engineering old files. Start shipping defensible results.",
    "what_feels_off": "The whole thing screams corporate software trying to pitch to analysts, not academics. The social proof is all business roles, the language is all about 'teams' and 'handoffs' and 'compliance'\u2014I'm one person trying to finish a thesis, not managing a healthcare network. The 'mess' framing feels condescending when I'm just trying to understand my own methodology under time pressure.",
    "objections": "This feels built for enterprise teams with legacy systems, not a solo researcher trying to document methods for a thesis committee. I don't have mixed-language workflows or a team to hand off to. I need help understanding what my analysis is actually doing methodologically, not just mapping file dependencies. Also, waitlist means I can't even try it now when I'm under deadline.",
    "dealbreaker": true,
    "dealbreaker_reason": "Waitlist access only when I need help right now in thesis crunch time, and the entire product seems aimed at corporate teams not solo academic researchers trying to understand and defend their statistical methods.",
    "gut_reaction": "This looks like enterprise workflow software cosplaying as something for researchers. I don't have a team or legacy code to manage\u2014I have three weeks to understand if my regression model is even appropriate.",
    "unanswered_questions": "Does this actually help me understand my statistical methods and assumptions, or just show me which files connect? Can it explain why a method is valid for my research question? Is there any academic use case here at all or is this pure corporate tooling?",
    "price_reaction": "Free trial waitlist means I can't access it during the exact window I need it, which makes the pricing model useless for my situation regardless of cost.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 35,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It organizes and tracks changes across multiple file types and scripts so you can audit your analysis workflow.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "When someone new joins, they can navigate the logic history instead of reverse-engineering filenames and tribal knowledge.",
    "what_feels_off": "The entire tone is written for corporate teams with SQL and Python scripts, not for solo PhD candidates doing qualitative research with basic Excel data management. The testimonials are all from analysts at companies, not researchers. The 'mixed-language legacy workflows' and 'operations analyst' language makes this feel like enterprise software that won't understand my actual problem.",
    "objections": "I don't have a team. I don't use Python, R, MATLAB, or SQL. I have messy Excel files with interview coding and some basic stats I'm terrified of, not 'legacy code.' This feels built for data engineers, not social scientists who are just trying to document their analysis decisions for committee review. I need help understanding what my basic regression output means, not mapping dependencies.",
    "dealbreaker": true,
    "dealbreaker_reason": "This product is clearly not designed for my use case. I'm a solo PhD student doing qualitative work with some basic quantitative analysis, not managing 'cross-language dependencies' or 'inherited Python scripts.' The entire value proposition assumes I have technical infrastructure I don't have and skills I'm anxious about lacking.",
    "gut_reaction": "This looks like it's for tech companies and data teams, not academics writing dissertations. I got hopeful at 'scattered spreadsheets' but then it immediately went into Python, R, MATLAB, SQL\u2014none of which I use confidently.",
    "unanswered_questions": "Does this help me understand my statistical analysis or just document what I did? Can it work with just Excel files and maybe one basic R script I got from my methods professor? Will this help me explain my analysis to my committee, or is it just version control I could get elsewhere?",
    "price_reaction": "Free trial is fine but I'm worried about what happens after\u2014if this is enterprise-focused, the paid tiers are probably way out of my stipend budget.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 36,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It organizes messy analysis files from different tools into a visual map with version tracking, but I don't actually understand what the interface looks like or how it handles qualitative work.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "When someone new joins, they can navigate the logic history instead of reverse-engineering filenames and tribal knowledge.",
    "what_feels_off": "The whole thing assumes I'm working with Python, R, MATLAB, SQL\u2014I'm doing qualitative coding in NVivo and some basic Excel tables. The testimonials are all from analysts and healthcare operations people, not researchers. The 'mess' framing feels condescending when my problem isn't mess, it's that I don't know statistics tooling well enough yet.",
    "objections": "This feels built for data engineers and analysts, not social science researchers. I don't have 'inherited Python scripts' or 'cross-language dependencies'\u2014I have interview transcripts, coding frameworks, and basic demographic tables. The waitlist means I can't even see if this would work for my actual workflow. No pricing transparency beyond 'free trial' makes me wonder what I'm committing to later.",
    "dealbreaker": true,
    "dealbreaker_reason": "Nothing on this page speaks to qualitative research workflows or mixed-methods dissertation work. It's all quantitative analysis jargon that reinforces my intimidation around 'real' data tools rather than meeting me where I am.",
    "gut_reaction": "This reads like it's for corporate data teams who inherited a bunch of scripts, not doctoral students trying to organize interview data and learn basic stats. I feel more intimidated after reading it.",
    "unanswered_questions": "Does this work with qualitative data at all? What about NVivo or Atlas.ti files? What happens after the free trial ends and how much will it cost? Can my advisor or committee members view exports without needing accounts?",
    "price_reaction": "Waitlist with no actual pricing feels evasive. As a doctoral student, I need to know if this will cost $20/month or $200/month before I invest time learning another tool.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 37,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It organizes files and code from different programs into a visual map with change tracking, but I'm not sure how that actually works or if it understands research logic.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Stop reverse-engineering old files. Start shipping defensible results.",
    "what_feels_off": "The testimonials sound too polished and corporate\u2014none are from academics doing qualitative work. The whole thing feels written for business analysts and engineers who inherit messy code, not researchers writing a dissertation. All the examples are SQL, Python, MATLAB\u2014where's the social science use case?",
    "objections": "I don't code in Python or R regularly, I mostly work in SPSS and Excel with some qualitative coding software. I have no idea if this even applies to my workflow. The 'legacy code' and 'cross-language dependencies' language assumes I'm managing scripts I inherited from a dev team, not building a methods chapter. No mention of IRB documentation, research methods, or qualitative data\u2014just 'data health reports' that sound like they're looking for database errors.",
    "dealbreaker": true,
    "dealbreaker_reason": "This doesn't speak to dissertation research at all. It's clearly built for business/tech teams managing production workflows, not grad students doing mixed-methods research. I need help with research documentation and methods defense, not 'handoff-ready workflows' for a team I don't have.",
    "gut_reaction": "This looks like enterprise software for data engineering teams, not research tools. I got here from an Instagram ad but nothing here addresses thesis anxiety or research methods\u2014just corporate workflow handoffs.",
    "unanswered_questions": "Does this work with qualitative data? Does it understand research methodology vs just code dependencies? Can it help with IRB compliance or methods chapters? What does this cost after the trial? Is there academic pricing?",
    "price_reaction": "Waitlist-only with no pricing shown makes me assume it'll be enterprise-priced and out of reach once they reveal the real cost. 'Founding cohort perks' feels like a tactic to avoid showing the price.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 38,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It organizes multiple file types and code languages into a visual workflow map with audit trails.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Grant reporting got easier because we could export a clean audit trail with method notes and decision history.",
    "what_feels_off": "The whole thing screams 'built for engineers and operations teams' not researchers. The headline about receipts and trails sounds like accounting software. The testimonials are all from analysts and healthcare networks, not anyone doing actual thesis work. It's solving a corporate handoff problem, not a qualitative researcher trying to document their methods for a committee.",
    "objections": "I don't use Python, R, MATLAB, or SQL regularly\u2014I use qualitative software and maybe basic Excel. This feels like it's for people inheriting complex codebases, not someone building a thesis from scratch. I don't have a 'team' or 'legacy workflows' to hand off. I have one advisor who wants to see my analytical process, and this seems wildly overbuilt for that. Also, 'waitlist' means I can't even see if it would work for my actual use case before committing time to learn another tool.",
    "dealbreaker": true,
    "dealbreaker_reason": "This is not built for social science thesis work. It's enterprise software for teams managing technical debt across programming languages. I need something that helps me document qualitative coding decisions and statistical steps in SPSS or basic R, not map dependencies in inherited MATLAB scripts.",
    "gut_reaction": "This looks like it solves a real problem, but not my problem. It's for data engineers cleaning up corporate messes, not grad students trying to prove methodological rigor to a thesis committee.",
    "unanswered_questions": "Does this work with qualitative analysis software like NVivo or Atlas.ti? Can it handle non-code workflows like annotated literature reviews or interview coding? What does 'waitlist' mean in terms of actual timeline? Is there any support for solo researchers vs teams?",
    "price_reaction": "Free trial is fine but waitlist plus 'guided cohort' makes me think this will eventually be expensive enterprise pricing I can't afford on a GA stipend.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 39,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It organizes messy code and spreadsheets into a visual map with audit trails, but I'm not sure how or if it actually helps me do analysis.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Stop reverse-engineering old files.",
    "what_feels_off": "The whole thing feels built for engineers managing production pipelines, not researchers doing actual analysis. The testimonials are all about 'handoff risk' and 'validating AI-generated code' \u2014 I don't have a team or inherited Python scripts. I have a research question and data I need to analyze but don't know how yet.",
    "objections": "This doesn't solve my problem. I'm not trying to audit or hand off existing workflows \u2014 I'm trying to build my first one and I'm scared of the statistics tooling. This product seems to assume I already have code and spreadsheets working, just messily. I don't even know which statistical tests to run or how to structure my data properly.",
    "dealbreaker": true,
    "dealbreaker_reason": "This is for people who already know how to code and analyze data but need better organization. I need help actually doing the analysis in the first place, not documenting code I haven't written yet.",
    "gut_reaction": "I saw the Instagram ad and thought this might help me get over my statistics anxiety, but this is clearly for data engineers or analysts with complex existing setups. I'm still at 'how do I even start' and this feels like it's for people way past that.",
    "unanswered_questions": "Does this help me actually run statistics or just organize existing files? Can it guide me through choosing and running the right tests for my qualitative-heavy research? What if I don't have legacy workflows to upload?",
    "price_reaction": "Free trial is fine but it doesn't matter since this isn't solving my actual problem of needing hand-holding through statistical analysis.",
    "transparency_trust": 3,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 40,
    "bucket": "social_science_thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It maps relationships between different data files and code scripts so you can document what you did for auditing purposes.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I do not want to fake competence in software just to finish my thesis.",
    "what_feels_off": "This whole thing feels written for corporate teams with 'inherited workflows' and 'sprint cycles' - I'm one person trying to finish a dissertation, not managing a healthcare network's monthly reporting. The language is so tech-forward it makes me more anxious, not less. 'Mixed-language legacy workflows' isn't my problem, I just need to run some regressions and document my methods without breaking everything.",
    "objections": "I don't have a 'team' to hand off to. I don't have Python AND R AND MATLAB AND SQL - I barely manage SPSS and maybe some basic R scripts my advisor sent me. This feels like overkill for a solo doctoral student. Also, what does 'approve suggestions' even mean if I don't understand the statistics well enough to know if a suggestion is valid? The testimonials are all from analysts and operations people, not researchers. And 'onboarding support' sounds like I'll need to do training calls when I'm already drowning.",
    "dealbreaker": true,
    "dealbreaker_reason": "The product seems built for corporate data teams, not solo researchers struggling with basic statistical anxiety. The complexity it promises to organize feels bigger than the complexity I actually have, and I don't trust that I'd know how to validate its 'automated suggestions' anyway.",
    "gut_reaction": "This looks like enterprise software cosplaying as something accessible. I came here anxious about spreadsheets and I'm leaving more convinced I need skills I don't have.",
    "unanswered_questions": "Can this actually help someone doing basic qualitative coding and simple statistical tests for a social science thesis? Does it work with SPSS? What if I don't have 'workflows' plural, just one messy analysis? How much statistics knowledge do I need to use this safely?",
    "price_reaction": "Free trial is fine but 'priority invitations for teams with mixed-language legacy workflows' means I'm not the priority user and will probably wait forever or get deprioritized.",
    "transparency_trust": 2,
    "manual_friction_relief": "no",
    "clarity_score": "partial"
  },
  {
    "person_id": 41,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It maps relationships between different code files and spreadsheets to create an audit trail for analysis workflows.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "When someone new joins, they can navigate the logic history instead of reverse-engineering filenames and tribal knowledge.",
    "what_feels_off": "The headline 'Every change has a Receipt' is trying too hard to be clever and just confuses the main value. The whole thing feels AI-written\u2014lots of jargon like 'defensible results' and 'tribal knowledge' without showing me actual before/after screenshots or real numbers on time saved.",
    "objections": "I have no idea what this actually costs after the trial, which is a huge problem when I'm managing grant budgets. The testimonials are suspiciously vague\u2014no names, no real organizations. 'We fixed handoff risk in one sprint' tells me nothing about staff hours saved. Also unclear if this requires technical setup that would need consultant support, which defeats my whole purpose.",
    "dealbreaker": true,
    "dealbreaker_reason": "No pricing transparency means I can't budget for this or justify it in a grant proposal. Waitlist-only access means I can't even test it when I need it. If I can't see real pricing and availability timeline, I can't plan around it.",
    "gut_reaction": "Sounds useful in theory but feels like vaporware with the waitlist model and zero pricing info. Too many buzzwords and not enough concrete proof it works for non-technical staff.",
    "unanswered_questions": "What does this actually cost per user or per team after trial? How much technical knowledge do staff need to use it? Does it work offline or require constant cloud access? What happens to our data if we stop paying? How long is typical implementation?",
    "price_reaction": "Free trial is fine but hiding all pricing information is a red flag. I need to know if we're talking $50/month or $5000/month to even consider this. Grant budgets require line items months in advance.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 42,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It ingests mixed file types and scripts to create a visual map of dependencies and audit trail for handoffs.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Show me lower risk and clearer accountability, then budget becomes possible.",
    "what_feels_off": "The social proof quotes sound like they were written by the same person trying to hit different personas\u2014too polished and convenient. 'Inherited Python scripts' and 'fragile process' feel like buzzword insertion. The 'Every change has a Receipt' headline is trying too hard to be clever.",
    "objections": "Waitlist means I can't actually see pricing or test it now when I need budget approval. No idea what this costs at scale or per user. 'Guided free trial cohort' sounds like I'm signing up to be a beta tester doing free QA work. What happens after trial ends? The 'upload your existing mess' language feels unprofessional for something I'd need to defend to finance.",
    "dealbreaker": true,
    "dealbreaker_reason": "No transparent pricing and waitlist-gated access means I can't build a business case or budget request. I need numbers and commitment timelines, not 'founding cohort perks' and setup calls. Can't go to leadership with 'maybe we'll get invited to try something someday.'",
    "gut_reaction": "Sounds like it could solve real problems but the whole waitlist approach screams 'not ready for enterprise use' and I don't have time to be someone's design partner when I have compliance deadlines.",
    "unanswered_questions": "What does this actually cost per month? How many users? What's the data retention policy? Do you store our files on your servers or is this local? What happens when the trial ends if we're mid-project? Integration requirements?",
    "price_reaction": "Free trial waitlist tells me nothing. I need to know if this is $50/month or $5000/month to even consider it. 'Priority invitations' is marketing speak that wastes my time.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 43,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It organizes messy data files and scripts across different tools into one visual workflow with audit trails.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Grant reporting got easier because we could export a clean audit trail with method notes and decision history.",
    "what_feels_off": "The hero headline is trying too hard to be clever with 'Receipt' capitalized\u2014just say what it does. Section 2 promises I'll see all this 'in 5 minutes' but I have no idea if that's realistic or marketing fluff. The social proof quotes are suspiciously on-message and feel constructed rather than genuine.",
    "objections": "I don't understand what the actual cost will be after the free trial ends. 'Guided free trial cohort' and 'waitlist' tells me nothing about budget impact. Also, if my team of three part-timers uploads our grant reporting mess, will we need technical expertise to make sense of the suggestions? The copy says 'teams' everywhere but doesn't clarify minimum viable team size or skill level.",
    "dealbreaker": true,
    "dealbreaker_reason": "No pricing transparency whatsoever. I manage grant budgets with strict constraints and zero flexibility for surprise costs. I can't justify joining a waitlist when I have no idea if this will cost $50/month or $5,000/month post-trial. 'Founding cohort perks' sounds like a setup for expensive tiered pricing I can't predict or defend to my board.",
    "gut_reaction": "This sounds useful for our messy Excel-plus-inherited-scripts situation, but I've been burned by 'free trial' products that turn into enterprise pricing we can't afford. The waitlist model feels like they're hiding the real cost until I'm invested.",
    "unanswered_questions": "What will this actually cost per month after trial? Can a small team of 3-4 non-technical staff use this without hiring consultants? What happens to our uploaded data if we can't afford to continue after the trial? Is there a nonprofit discount? How long does the free trial last?",
    "price_reaction": "The complete absence of pricing is a red flag. 'Guided free trial cohort' and 'founding cohort perks' suggests they're testing pricing and I'll be locked into whatever they decide. I need a number before I waste time on a waitlist.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 44,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It organizes and maps dependencies across mixed analysis files and scripts to create audit trails.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Grant reporting got easier because we could export a clean audit trail with method notes and decision history.",
    "what_feels_off": "The whole hero section sounds AI-written and vague\u2014'Receipt' in quotes feels forced, 'existing mess' is trying too hard to be relatable, and I still don't know if this is a platform, a desktop app, or what. The social proof is suspiciously perfect and generic.",
    "objections": "I have no idea what this actually costs after the trial, how long implementation really takes with our grant deadlines, whether it works with our IRB compliance requirements, or if it locks us into a vendor we can't afford next fiscal year. 'Guided free trial cohort' sounds like they're still figuring out their product.",
    "dealbreaker": true,
    "dealbreaker_reason": "No transparent pricing means I can't budget for it. I need to know costs before I invest time in a trial, especially with grant constraints. 'Founding cohort' signals this is beta software\u2014I can't risk our compliance documentation on something unproven when auditors review our work.",
    "gut_reaction": "This sounds like it could solve a real problem but feels half-baked. I need actual numbers and proof this won't become another tool I have to migrate off of in 18 months when funding gets tight.",
    "unanswered_questions": "What does it actually cost after trial? What's the minimum contract term? Does it meet IRB and grant compliance standards? What happens to our data if we need to leave? How many staff hours for real implementation?",
    "price_reaction": "Waitlist with no pricing is a red flag for budget planning. I need to know if this is $500/year or $5000/month before I waste anyone's time. 'Founding cohort perks' means they'll charge more later\u2014when?",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 45,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It maps dependencies across mixed analysis files and creates audit trails for workflow handoffs.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Upload a zip file of Excel, CSV, Python, R, MATLAB, and SQL files \u2014 we map dependencies, surface data issues, and organize the workflow without forcing rewrites.",
    "what_feels_off": "The testimonials are too perfectly aligned with the product pitch and feel crafted rather than real. The 'Receipt' metaphor in the headline is trying too hard to be clever. The objection handling feels defensive before I've even objected.",
    "objections": "No actual pricing information means I can't do budget planning. Waitlist access means delayed implementation with unclear timeline. The change management overhead isn't addressed\u2014who trains my team? What's the learning curve? How much time does initial setup actually take versus 'day one' marketing speak? What happens after the free trial ends and they hit me with enterprise pricing?",
    "dealbreaker": true,
    "dealbreaker_reason": "I need concrete pricing to secure budget approval, and waitlist access means I can't plan implementation timeline or justify resource allocation without knowing total cost of ownership. I've been burned by 'free trial then surprise enterprise pricing' before.",
    "gut_reaction": "Interesting technical capability but too many unknowns for budget approval. Feels like they're hiding pricing because it's going to be expensive once they get me invested in the trial.",
    "unanswered_questions": "What does this actually cost after trial? What's the implementation timeline from waitlist to production use? How many staff hours for setup and training? What's my vendor lock-in risk? Can I export everything if we need to leave? What's your company stability\u2014are you funded and staying around?",
    "price_reaction": "Free trial waitlist tells me nothing. I need ballpark annual cost per user or per team to even start a budget conversation. Founding cohort perks sound nice but don't pay my bills or justify expenditure.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 46,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It's supposed to organize and map scattered Excel files, scripts, and analysis workflows to create an auditable trail for handoffs.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Grant reporting got easier because we could export a clean audit trail with method notes and decision history.",
    "what_feels_off": "The whole thing reads like AI copy\u2014'every change has a receipt' is cutesy, the social proof quotes are suspiciously perfect, and 'existing mess as-is' feels like it's trying too hard to be relatable. The nonprofit quote is the only one that speaks my language but I have no way to verify if it's real.",
    "objections": "I have no idea what this actually costs after the trial, no idea if it works with our grant reporting templates, no sense of whether this requires IT approval or cloud infrastructure I can't get approved, and absolutely no proof this isn't vaporware since it's just a waitlist. The 'founding cohort' language makes me think they don't have a real product yet.",
    "dealbreaker": true,
    "dealbreaker_reason": "No pricing information means I can't budget for it. I need to know if this is $50/month or $5,000/month before I waste time on a demo. Waitlists without pricing are a red flag that they'll price it out of nonprofit range once I'm interested.",
    "gut_reaction": "Sounds useful in theory but screams early-stage startup that will either pivot, get expensive, or disappear. I can't justify staff time testing something that might not exist in six months when grant cycles are annual.",
    "unanswered_questions": "What does this actually cost? Does it require cloud storage our compliance team won't approve? Can it handle our specific grant reporting templates? What happens to our data if you shut down? How long does setup actually take with our existing mess?",
    "price_reaction": "Free trial waitlist with no pricing anchor is a non-starter. I need budget approval before investing time, and 'founding cohort perks' suggests they'll jack up prices once they figure out what people will pay.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 47,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It organizes and maps connections between different types of analysis files and scripts to make inherited workflows easier to understand and audit.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "When someone new joins, they can navigate the logic history instead of reverse-engineering filenames and tribal knowledge.",
    "what_feels_off": "The social proof quotes are too perfectly aligned with the marketing message and feel fabricated. 'Stop reverse-engineering old files' is marketing speak that no one actually says. The 'Receipt' metaphor in the headline is confusing and gimmicky.",
    "objections": "No actual pricing shown so I can't budget for it. 'Waitlist' and 'guided cohort' sounds like this isn't even a real product yet. I need something my team can use now, not in some undefined future. No information about what happens after the free trial or what the real cost structure looks like. How do I justify budget without knowing if this is $500/month or $5000/month?",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't allocate budget or plan adoption for a product that's still in waitlist mode with no transparent pricing. My job requires knowing costs upfront to get approval, and 'founding cohort perks' tells me they're still figuring out their business model. I need proven solutions I can implement this quarter, not beta experiments.",
    "gut_reaction": "This sounds like it might solve a real problem we have with inherited analysis scripts, but it's not ready for purchase. Waitlist access with no pricing is a non-starter for budget planning.",
    "unanswered_questions": "What does this actually cost after trial? What's the timeline to general availability? How many staff hours are required for implementation? What's the ongoing maintenance burden? Can it handle our specific compliance requirements or is that custom work?",
    "price_reaction": "Free trial waitlist tells me nothing. I need to know if this is enterprise software requiring a procurement process or a SaaS tool I can expense. The lack of any pricing anchor makes me assume it's expensive and they're hiding it.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 48,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It creates a visual map and audit trail for messy multi-language data workflows so teams can understand and hand off inherited analysis projects.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "If it saves staff hours and survives turnover, I can justify the spend.",
    "what_feels_off": "The entire tone feels like AI-generated SaaS copy\u2014'Every change has a Receipt' is trying too hard to be clever. The social proof quotes are suspiciously on-message and generic. 'Existing mess as-is' feels forced-casual. No actual pricing means I can't assess budget fit at all.",
    "objections": "No real pricing information, just a waitlist. I don't have time to join waitlists and wait for 'setup calls'\u2014I need to know if this fits my grant budget now. The product sounds ambitious but unproven. How does it actually handle MATLAB and R dependencies? That's complex. The healthcare and nonprofit quotes feel manufactured. I need real case studies with measurable time savings, not vague 'got easier' claims.",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't justify joining a waitlist without knowing actual cost structure. Grant budgets require line-item justification months in advance. 'Free trial cohort' and 'founding perks' tells me this is early-stage and pricing is undefined, which means risk of cost changes or product failure. I need stable, proven tools with transparent pricing.",
    "gut_reaction": "Sounds useful in theory but way too vague on implementation and completely opaque on pricing. I can't take this to my finance team with just 'free trial waitlist'\u2014that's not budget planning, that's wishful thinking.",
    "unanswered_questions": "What will this actually cost after the trial? What's the per-user pricing model? How long until it's production-stable? What happens to my data if the startup folds? Does it actually work with institutional security requirements or just claim to? How much staff time does onboarding really take?",
    "price_reaction": "There is no price information, which is a red flag. 'Guided free trial cohort' and 'founding perks' means they haven't figured out their pricing model yet. I need actual numbers to evaluate ROI and budget fit.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 49,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It organizes and maps dependencies across mixed file types and scripts to create an auditable trail of analysis workflows.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Grant reporting got easier because we could export a clean audit trail with method notes and decision history.",
    "what_feels_off": "The 'Receipt' metaphor in the headline feels forced and unclear. The social proof quotes sound generic and AI-polished\u2014no real person talks like 'fixed handoff risk in one sprint.' The 'existing mess as-is' language tries too hard to be relatable. The whole thing reads like product copy written by someone who hasn't actually felt the pain.",
    "objections": "I need to see actual pricing before I join a waitlist\u2014'free trial cohort' tells me nothing about budget impact. No information about data residency or security certifications for grant compliance. The walkthrough is behind a click, not embedded. I don't know what happens after the trial ends or what the real cost structure looks like. I can't justify waitlist time without knowing if this will fit our NIH reporting requirements.",
    "dealbreaker": true,
    "dealbreaker_reason": "No transparent pricing means I can't assess budget feasibility before investing time in a trial. I need cost certainty upfront to even consider putting this in a grant proposal or annual budget request.",
    "gut_reaction": "This sounds like it could solve real problems we have with mixed R and Excel workflows, but the lack of pricing transparency and vague 'cohort' language makes me think it's going to be expensive SaaS that I'll have to defend to finance without concrete numbers.",
    "unanswered_questions": "What does this actually cost after trial? Does it meet NIH data management plan requirements? Can it handle PHI-adjacent work? What's the vendor's financial stability? How does pricing scale with team size or data volume? Is there an academic or nonprofit discount?",
    "price_reaction": "The waitlist model without pricing signals either they haven't figured out their model yet or it's going to be too expensive for academic budgets. I need a clear price sheet, not 'founding cohort perks.'",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 50,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It organizes messy analysis files across different tools and creates audit trails, but I'm unclear if it actually does the analysis or just documents what's already there.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "If it saves staff hours and survives turnover, I can justify the spend.",
    "what_feels_off": "The whole thing reads like AI wrote it\u2014lots of vague promises about 'mapping dependencies' and 'surfacing issues' without showing me concrete examples or proof it works. The testimonials are suspiciously perfect and lack any names or verifiable organizations. 'Regional Healthcare Network' could be anyone or no one.",
    "objections": "I have no idea what this actually costs after the free trial, no sense of implementation time beyond 'one week' marketing speak, and zero confidence this won't become shelfware when my team realizes it's just another layer of complexity. The waitlist model feels like they're not ready yet and I'm being asked to beta test their half-baked product.",
    "dealbreaker": true,
    "dealbreaker_reason": "No pricing transparency means I can't budget for this, and I'm not committing staff time to learn a tool when I don't know if we can afford to keep it after trial. I've been burned before by 'free trials' that turn into enterprise pricing I can't justify to grants.",
    "gut_reaction": "This sounds like a solution looking for a problem. My team's workflows aren't pretty but they work, and adding another tool to 'organize' them feels like more overhead, not less.",
    "unanswered_questions": "What does this cost annually? How much staff time does implementation actually take? What happens to our work if we stop paying\u2014do we lose access to our own documentation? Can this handle our specific compliance requirements for federal grants?",
    "price_reaction": "Hiding behind a waitlist with 'founding cohort perks' is a red flag. Just tell me what it costs so I can decide if it's worth my time to even explore.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 51,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It maps dependencies across mixed-language analysis files and logs changes for auditability.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The social proof quotes sound AI-generated and vague\u2014'exactly where our process was fragile' is consultant-speak, not how analysts talk. The 'Receipt' metaphor in the headline is trying too hard to be clever. 'Mess as-is' feels condescending.",
    "objections": "I don't understand how it actually maps cross-language dependencies\u2014that's technically complex and the page handwaves it. What does 'surfaces data issues' mean in practice? How does it parse R versus Python versus MATLAB logic? The proof section lists features but doesn't show me what the interface actually looks like or how deep the analysis goes.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This addresses a real pain point but I don't trust that it can deliver on the technical promise without seeing actual proof. The copy feels like it was written by someone who knows the problem but hasn't shown me they've solved it.",
    "unanswered_questions": "How does it parse and understand multiple languages' logic? What's the accuracy rate on dependency mapping? Can I export in formats my reviewers actually use? What happens to my proprietary code\u2014where does it go?",
    "price_reaction": "Waitlist-only pricing is fine for early stage but I need to know ballpark costs before investing time in a trial. Is this per-user, per-project, enterprise-only?",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 52,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across mixed-language analysis files and creates audit trails without forcing me to rewrite my existing scripts.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The 'Receipt' metaphor in the headline feels like forced branding, and 'your existing mess as-is' is trying too hard to sound casual and relatable.",
    "objections": "I need to see the actual interface parsing my R and MATLAB code before I trust it won't misinterpret dependencies, and 'waitlist' means I can't even test this now to verify the claims.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally someone who gets that I don't want automation taking over. The approval-based workflow and multi-language support hits my pain points, but I'm skeptical until I see it handle real legacy code complexity.",
    "unanswered_questions": "How does it actually parse MATLAB and R dependencies\u2014does it execute code or just do static analysis? What happens when it can't map something? Can I export the workflow map in a format I control?",
    "price_reaction": "Waitlist with no pricing visibility is annoying but understandable for early stage\u2014I just hope 'founding cohort' doesn't mean they'll jack up prices later once I'm dependent on it.",
    "transparency_trust": 4,
    "manual_friction_relief": "yes",
    "clarity_score": "partial"
  },
  {
    "person_id": 53,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It maps dependencies across different file types and scripts to create an auditable trail of analysis changes.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The copy promises to understand cross-language dependencies and data flows which is an extremely hard technical problem, but glosses over how it actually works. The 'Receipt' metaphor in the headline is forced and unclear. The testimonials feel generic - no real names, just vague titles. 'Workflow Map' and 'Data Health Report' are buzzwords without substance.",
    "objections": "I don't believe it can accurately map dependencies across Excel, Python, R, MATLAB, and SQL without massive manual configuration on my end. The technical claims are vague - how does it parse legacy code logic? What if my Python scripts have custom functions or nested references? This feels like it oversimplifies a genuinely complex problem. Also, no actual pricing means I can't evaluate if this is worth the inevitable learning curve and migration effort.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This reads like a product that promises to solve my exact pain points but I'm skeptical it can deliver without me doing most of the heavy lifting. The lack of technical depth makes me think it's either early-stage vaporware or will require so much setup that I might as well document things myself.",
    "unanswered_questions": "How does it actually parse different languages and determine dependencies? What happens when it gets the logic flow wrong? Can I edit the maps manually? What's the actual learning curve? When will I see real pricing and what will the enterprise tier cost?",
    "price_reaction": "Waitlist with no pricing is a red flag - tells me they haven't figured out their value proposition or the product isn't ready. I need to know if this is $50/month or $5000/month before I invest time in a trial.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 54,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It's a tool that maps dependencies across mixed-language analysis files and creates audit trails for workflow handoffs.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The social proof quotes are too perfectly worded and hit every talking point like a marketing checklist. 'Receipt' metaphor feels forced. The 'existing mess' language tries too hard to be relatable. Vague about how the dependency mapping actually works technically.",
    "objections": "I don't understand how it reads Python, R, MATLAB, and SQL to actually map dependencies\u2014that's incredibly complex and each language has different paradigms. What's the accuracy rate? How does it handle custom functions or libraries? The testimonials don't feel real. No pricing transparency beyond 'waitlist.' I've seen too many tools promise to organize legacy workflows and fail spectacularly.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This sounds useful in theory but I'm skeptical it can actually parse cross-language dependencies accurately without me spending weeks configuring it. The copy avoids the hard technical questions about how the parsing works.",
    "unanswered_questions": "What's the parsing accuracy for each language? How does it handle custom libraries or obscure R packages? What happens when it can't map something? How much manual configuration is actually required? What are the real limitations?",
    "price_reaction": "Waitlist model is fine but tells me nothing about whether this will be affordable long-term or require enterprise pricing once they have me hooked.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 55,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It's supposed to be a workflow documentation tool that maps dependencies across different file types and languages while tracking changes.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The whole thing reads like AI-generated copy trying to sound like it understands analyst pain points but stays frustratingly vague about technical specifics. 'Data Health Report' and 'Change Receipt Timeline' sound like feature names dreamed up in a product meeting, not things I'd actually use. The social proof quotes are too perfectly on-message to feel real.",
    "objections": "I have no idea how it actually works under the hood. What does 'map dependencies' mean technically? Does it parse code? Static analysis? How accurate is it with R scripts that use eval() or dynamically generated SQL? What happens when it gets the dependency graph wrong? The 'we organize the logic and data flow' is hand-waving where I need precision.",
    "dealbreaker": true,
    "dealbreaker_reason": "No technical documentation linked, no actual specifics about methods, and the whole pitch asks me to upload my proprietary analysis code to a black box system on a waitlist. I need to see validation reports, methodology docs, and understand exactly what happens to my code before I'd ever upload it. 'Trust us, join a waitlist' doesn't work for someone who needs explicit control.",
    "gut_reaction": "This feels like a solution looking for a problem, written by people who haven't actually maintained legacy analytical workflows. The pain points are real but the solution is too abstract and unproven.",
    "unanswered_questions": "How does the dependency mapping actually work? What analysis engine is used? Where is my code stored and processed? Can I run this locally? What's the false positive rate on 'data health' checks? How does it handle complex R packages or custom Python libraries? What's the actual output format of these audit trails?",
    "price_reaction": "Waitlist-only with no pricing transparency is a red flag. I can't evaluate ROI without knowing if this will cost $50/month or $5000/month once the free trial ends.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 56,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It maps dependencies across multiple file types and scripts to create an auditable workflow documentation system.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The hero headline with 'Receipt' feels try-hard and buzzwordy\u2014just say audit trail. The social proof quotes are too perfectly on-message and read like they were written by marketing. 'Your existing mess as-is' tries too hard to be relatable and comes off as pandering.",
    "objections": "I don't understand what 'organize the workflow' actually means technically\u2014does it refactor my code or just document it? The dependency mapping sounds useful but I need to know if it actually executes my scripts or just reads them statically. What happens to my data when I upload it? Is this cloud-based? The vagueness around 'suggestions' worries me\u2014what kinds of changes is it proposing?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This could solve real handoff problems I have, but the copy is too vague about what the tool actually does under the hood. I need to see the walkthrough before I'd even consider joining a waitlist.",
    "unanswered_questions": "Where does my data go when I upload? Does it execute my code or just parse it? What specifically are the 'automated suggestions'\u2014formatting, refactoring, or something else? How does it handle proprietary code and sensitive data? What's the actual pricing going to be after the free trial?",
    "price_reaction": "Free trial is fine but no indication of what I'll pay later, which makes me suspicious about lock-in. I need to know if this will be affordable for a single analyst or if it's enterprise-only pricing.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 57,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It's supposed to map dependencies across different file types and languages while keeping an audit trail of changes.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Only approved suggestions are applied. You remain the decision-maker.",
    "what_feels_off": "The whole thing reads like AI-generated copy trying too hard to sound technical - phrases like 'your existing mess' and 'tribal knowledge' feel forced. The social proof quotes are suspiciously perfect and generic. How exactly does it 'map dependencies' across Python, R, MATLAB, and SQL? That's a massive technical claim with zero explanation of the actual methodology.",
    "objections": "I don't understand how this actually works under the hood. What's the engine doing to parse all these different languages? How accurate is the dependency mapping? The 'auto-detected issues' list is vague - what's the false positive rate? And calling mixed-language workflows a 'mess' feels dismissive of legitimate complexity. Also, waitlist-only access means I can't even validate any of these claims.",
    "dealbreaker": true,
    "dealbreaker_reason": "No way to validate the core technical claims before committing to a waitlist. I've seen too many tools promise magic cross-language parsing and deliver garbage. Show me the actual methodology documentation first, not marketing copy about 'receipts' and 'trails'. I need to see how it handles edge cases in my actual workflow before I waste time on another sales call.",
    "gut_reaction": "This feels like someone watched too many SaaS landing page tutorials. The problem they're describing is real, but the solution is hand-wavy and the copy is trying way too hard to sound reassuring without showing actual technical depth.",
    "unanswered_questions": "What parsing engine are you using? How do you handle code that doesn't follow standard conventions? What's the accuracy rate on dependency detection? Can I see sample output before joining a waitlist? What happens to my proprietary code when I upload it? How does pricing actually work after the trial?",
    "price_reaction": "Waitlist-only with vague 'founding cohort perks' tells me nothing about actual cost structure or whether this will be affordable for a single analyst versus requiring enterprise budgets.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 58,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "agree",
    "clarity_response": "It maps dependencies across mixed-language analysis files and logs every change with an audit trail.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The social proof quotes are too perfect and conveniently address exactly the product's value props\u2014they read like they were written by marketing, not actual users.",
    "objections": "I need to see the actual product working, not join a waitlist. The '3-Minute Product Walkthrough' is promising but I want to test it myself with my own messy files before committing time to a setup call. Also unclear how it handles proprietary code and whether my data stays local or gets uploaded to their servers.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Finally, someone who gets that I don't want black-box automation. The emphasis on control and explicit approvals is right, but I'm skeptical until I see it handle real legacy chaos\u2014not a demo dataset.",
    "unanswered_questions": "Where does my data go when I upload? Can I run this locally or air-gapped? What's the actual pricing after the free trial? How does it parse legacy code that's intentionally obscure or poorly documented?",
    "price_reaction": "Waitlist with no pricing visibility is frustrating. I understand it's early but I need to know if this will be $50/month or $5000/month to justify even exploring it.",
    "transparency_trust": 3,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 59,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It organizes multiple file types and scripts into a visual workflow map with change tracking and audit documentation.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The testimonials feel too perfectly tailored to the messaging, and phrases like 'existing mess' and 'tribal knowledge' sound like they're trying too hard to relate. The whole tone feels like AI copy trying to sound relatable to analysts.",
    "objections": "I don't understand how it actually analyzes my methodology or logic\u2014it sounds like it just maps file connections which I can already do mentally or with documentation. What does 'surface data issues' actually mean technically? Is it running statistical checks or just finding blanks? I need to know the validation logic before trusting any 'suggestions.' Also, 'waitlist' means I can't even test it now to see if it's worth my time.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This sounds like another tool trying to automate judgment calls I shouldn't be automating. The promise is organization and audit trails, which I already maintain manually with proper documentation discipline.",
    "unanswered_questions": "What exactly are the automated suggestions based on? What statistical methods does the Data Health Report use? How does it handle domain-specific logic that requires methodological expertise? Can I customize what constitutes a 'data issue' or am I stuck with their black box definitions?",
    "price_reaction": "Free trial through waitlist is fine but tells me nothing about actual cost or whether my lab budget could even cover this long-term.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  },
  {
    "person_id": 60,
    "bucket": "ai_weary_old_school_analyst",
    "resonance": "neutral",
    "clarity_response": "It's supposed to map and document multi-language analysis workflows by showing dependencies and logging changes without rewriting existing code.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Automated suggestions never auto-apply. You approve every material change before it lands.",
    "what_feels_off": "The hero copy tries too hard to sound clever with the receipt metaphor, and phrases like 'existing mess' feel condescending. The social proof quotes are too perfectly aligned with different use cases\u2014feels staged. 'Defensible results' in the final CTA sounds like consultant-speak.",
    "objections": "I don't understand how it actually maps cross-language dependencies\u2014that's technically hard and they gloss over it. What does 'ingests' mean in practice? Does it parse my Python code? My R scripts? How accurate is that? The 'data health report' detecting 'suspicious outliers' is a red flag\u2014that requires domain knowledge, not automation. And who are these testimonial people? No names, just vague titles.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This sounds like it could solve a real problem I have with inherited workflows, but the copy doesn't show me enough of how it works under the hood. I need to see the actual parsing logic and accuracy rates before I trust it with my code.",
    "unanswered_questions": "How does it parse different languages accurately? What's the error rate on dependency mapping? Can I see the methodology documentation before signing up? What happens to my code and data\u2014where is it processed? How does it handle complex transformations or custom functions?",
    "price_reaction": "Free trial is fine, but no mention of what it costs after. That's a yellow flag\u2014means they're probably targeting enterprise pricing and don't want to scare people off early.",
    "transparency_trust": 2,
    "manual_friction_relief": "partial",
    "clarity_score": "partial"
  }
]