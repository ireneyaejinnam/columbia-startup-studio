[
  {
    "person_id": 1,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control system for data analysis files that tracks changes across different tools and languages.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "too_expensive",
    "strongest_line": "No more 'Final_v2_FINAL.' Just one project with a complete memory.",
    "what_feels_off": "The whole thing reads like it's for actual data teams with legacy systems and multiple analysts - all the testimonials are from organizations and research directors, not stressed grad students. The 'mixed-language legacy codebases' stuff is completely irrelevant to me. I don't even know what MATLAB is. This feels like enterprise software cosplaying as accessible.",
    "objections": "I have one messy Excel file with survey data, not twelve monthly reports or Python scripts from three different people. I don't need audit trails or institutional licenses - I need someone to just tell me which statistical test to run on my Likert scales. Also $29/month is groceries for a week and I have no idea if this would even help with my actual problem which is that I don't know statistics, not that I can't track file versions.",
    "dealbreaker": true,
    "dealbreaker_reason": "This is clearly built for people who already know what they're doing and just need better organization. I don't know what I'm doing at all. Tracking my bad analysis decisions in an audit log doesn't help me make better decisions in the first place. I need to learn how to actually analyze data, not document my confused attempts at it.",
    "gut_reaction": "This immediately feels like it's not for me - it's talking about teams and legacy systems and I'm just one person who can't figure out how to run a regression. The price is way too high for something that won't teach me statistics.",
    "unanswered_questions": "Will this actually help me analyze my survey data or just organize my mess better? Does it tell me WHAT to do or just track what I already did? Can it explain statistical concepts or is it just file management? Why would a grad student with one dataset need version control?"
  },
  {
    "person_id": 2,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control thing for data files and code that multiple people work on.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it.",
    "what_feels_off": "The whole thing feels like it's for people managing corporate reporting workflows, not actual research. All the testimonials are from 'teams' and 'organizations' \u2014 I'm one broke grad student with a thesis. The language is so enterprise-y it makes my eyes glaze over. 'Auditable history'? 'Methodological control'? I just need to run a chi-square test.",
    "objections": "I don't have teams or legacy codebases or twelve monthly reports. I have one Excel file with survey responses and a thesis advisor who keeps asking if my analysis is done yet. This is solving a problem I don't have. Also I'm not uploading my thesis data to some random company's servers \u2014 IRB would have questions.",
    "dealbreaker": true,
    "dealbreaker_reason": "$29/month is two weeks of groceries and I still don't understand if this actually helps me run statistical tests or just tracks changes to files I'm already working in. It sounds like fancy Git for data people with actual jobs, not a tool that will help me finish my thesis.",
    "gut_reaction": "This is clearly built for corporate analysts or research teams with budgets. I'm a grad student who can't afford DataCamp and needs to run basic stats, not manage a 'person-dependent workflow' across an organization.",
    "unanswered_questions": "Does this actually help me DO analysis or just track it? Will it tell me if my chi-square is right? Does it work with SPSS files? What happens to my data privacy? Why would I pay $29/month for this when I can't even afford to renew my Adobe subscription?",
    "price_reaction": "The individual tier is almost 2% of my monthly stipend for something that sounds like documentation software. The institutional tier is laughable \u2014 my department won't even pay for our conference travel."
  },
  {
    "person_id": 3,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control system for data analysis files that tracks changes and organizes messy projects.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I've run this regression four different ways and I don't know which one is right and I'm too scared to ask my advisor.",
    "what_feels_off": "The whole thing reads like enterprise software marketing. The testimonials sound fake or like they're from people with completely different problems than mine. Operations analysts with twelve monthly reports? I'm just trying to finish one thesis. The 'Receipt' metaphor in the headline is confusing and doesn't tell me anything. This feels like it was written for corporate data teams, not grad students.",
    "objections": "I don't need to track changes across a team\u2014I'm working alone. I need to know if I'm using the right statistical test, not a log of what I did wrong. This won't tell me if my regression model is correct or if I'm violating assumptions. It sounds like it just documents my mistakes in a fancy way. Also, $29/month when my funding runs out in August? That's groceries.",
    "dealbreaker": true,
    "dealbreaker_reason": "This doesn't solve my actual problem. I need help knowing which analysis is methodologically correct, not a history of the four wrong ways I already tried it. This is for people managing inherited workflows, not someone who doesn't understand stats well enough to defend their thesis.",
    "gut_reaction": "This looks expensive and over-engineered for what I need. It's solving a collaboration and documentation problem when my problem is that I don't know if my analysis is even valid.",
    "unanswered_questions": "Will this actually tell me if I'm using the wrong statistical test? Can it flag methodological errors or just track changes? Does it work with SPSS? Will it help me explain my methods to my committee or just show them a timeline of my confusion?"
  },
  {
    "person_id": 4,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and audit tool for data analysis files that works across multiple languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "Our system is messy, but we know how it's broken.",
    "what_feels_off": "The whole thing reads like it's for corporate analysts managing reporting pipelines, not researchers doing actual statistical analysis. The testimonials are all from business roles. The 'health check' and 'broken references' language makes it sound like it's about catching Excel errors, not helping me figure out how to specify a multilevel model correctly.",
    "objections": "This doesn't solve my problem at all. I don't need an audit trail of my mistakes, I need to stop making the mistakes in the first place. My issue isn't tracking what changed in my code, it's that I don't know what code to write. I need someone to teach me how HLMs work, not document my failed attempts. Also $29/month is a lot when I'm on a grad student stipend and this seems completely orthogonal to my actual pain point.",
    "dealbreaker": true,
    "dealbreaker_reason": "This is solving a documentation and reproducibility problem for people who already know what they're doing. I'm stuck because I don't know how to do the analysis my committee wants. Logging my errors more carefully doesn't help me learn multilevel modeling.",
    "gut_reaction": "This feels like it was built for data analysts at companies who need to cover their ass with audit trails. I'm trying to learn statistics well enough to graduate, not document my workflow for compliance.",
    "unanswered_questions": "Does this actually help me understand what my code is doing wrong or does it just track changes? Will it explain why my multilevel model won't converge? Can it suggest the right model specification for my nested data structure?",
    "price_reaction": "$29/month is groceries for a week. The institutional license at $2,500/year is laughable for a grad student to even think about asking their department for, especially for something that doesn't actually teach me the statistics I need."
  },
  {
    "person_id": 5,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It tracks changes in data analysis files and makes messy inherited projects easier to understand.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The whole thing feels like it's written for corporate data teams, not researchers. The testimonials are all from business people. The language is very polished and corporate\u2014'methodological control,' 'audit layer,' 'navigable history'\u2014it reads like a SaaS pitch deck. Also 'Every change has a Receipt' is trying way too hard to be clever and just made me confused.",
    "objections": "I'm already drowning in my existing tools and this looks like it would add another layer of complexity I don't have bandwidth for. I don't know if it actually works with SPSS which is what I'm using. The examples are all about Excel and corporate reports, not dissertation data. My committee wants me in R but this doesn't seem like it would help me actually learn R or fix my import problems. And I definitely can't afford $29/month when I'm on a PhD stipend with a toddler.",
    "dealbreaker": true,
    "dealbreaker_reason": "It doesn't address my actual problem which is learning to use R correctly for my dissertation, and $29/month is way too much when I'm barely making ends meet. This feels like a corporate tool being advertised to me through Instagram when what I need is something that will help me get through my defense, not manage business workflows.",
    "gut_reaction": "This looks expensive and corporate and I can't tell if it would even work for dissertation research. The Instagram ad targeting feels off\u2014like they're trying to sell enterprise software to broke grad students.",
    "unanswered_questions": "Does it work with SPSS files? Will my committee accept this for reproducibility requirements or do they still want native R code? Is there actually a student discount? What does 'academic licensing' mean\u2014is that just for institutions or can individual students get it? Would this help me learn R or just be another thing to figure out?",
    "price_reaction": "$29/month is groceries for my family. The institutional price at $2,500/year is laughable\u2014my department won't pay for this for one student. There's no student tier mentioned which tells me this isn't really for people like me despite showing up in my Instagram feed."
  },
  {
    "person_id": 6,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation system for data analysis projects that works across multiple file types and languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "Upload a zip file containing your Python scripts, R code, MATLAB files, Excel workbooks, CSVs, or any combination. CleanSheet parses the whole thing, maps dependencies between files, and organizes the logic into a connected visual flow \u2014 even when the original code spans multiple languages or was written by three different people over four years.",
    "what_feels_off": "The hero section is trying way too hard with the receipts metaphor and the whole thing reads like it's written for middle managers at corporations, not researchers. The social proof quotes are suspiciously vague and generic \u2014 no actual institutions named, just job titles that could be anyone. The 'ad-hoc analyses' language feels like consultant-speak.",
    "objections": "This is built for people managing inherited Excel hell at companies, not for academic research. I don't have twelve monthly reports or broken formulas \u2014 I have six interview transcripts I need to code thematically. This solves a handoff problem I don't have. My advisor is the only one who needs to see my work and he's not going to care about fancy audit trails, he wants substantive findings. Also zero mention of qualitative data analysis, which is actually my problem.",
    "dealbreaker": true,
    "dealbreaker_reason": "This doesn't address qualitative analysis at all. It's positioning itself as a tool for quantitative workflows and inherited codebases. I need help coding interviews, not auditing formulas. Wrong product for my actual pain point.",
    "gut_reaction": "This is Git for spreadsheets aimed at corporate analysts who inherited messy workflows. I'm a quant person stuck doing qual work and this doesn't help with that at all.",
    "unanswered_questions": "Does it actually do anything with text data or qualitative content? Can it help with thematic coding? Or is this purely for numerical analysis and scripts? Why would I pay $29/month for something that sounds like glorified version control when Git is free?",
    "price_reaction": "$29/month is steep for a grad student stipend when I can use Git for free and Google Docs has version history. The institutional tier at $2,500/year is laughable \u2014 my department would never approve that for a tool that just tracks changes."
  },
  {
    "person_id": 7,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and audit tool for data analysis files and code, but I don't really understand what problem it's solving for me.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "too_expensive",
    "strongest_line": "No more 'Final_v2_FINAL.' Just one project with a complete memory.",
    "what_feels_off": "The whole thing reads like it's for corporate data analysts maintaining dashboards and SQL pipelines, not researchers doing qualitative work. All the examples are about 'monthly reports' and 'broken formulas' and 'AI-generated code' - none of that is my life. The language is super tech-forward and assumes I'm working with code and scripts when I'm literally just trying to figure out if my thematic codes make sense.",
    "objections": "I don't have scripts or code or Excel formulas - I have interview transcripts and a messy Dedoose project I abandoned. This seems built for people inheriting complex quantitative workflows, not someone doing thematic analysis on interviews. I can't tell if it even works with qualitative data. The 'health check' and 'broken references' stuff means nothing to me because I'm not dealing with calculations.",
    "dealbreaker": true,
    "dealbreaker_reason": "This product is clearly not for qualitative researchers. I need help organizing themes and codes from interviews, not auditing Python scripts or tracking Excel formula changes. Also $29/month when I'm a broke grad student and can't even tell if this applies to my work at all.",
    "gut_reaction": "This is for corporate data people or quant researchers, not me. I got halfway through and realized none of the examples match what I'm struggling with - I don't have legacy code or broken formulas, I have 40 transcripts and no idea if my coding is rigorous.",
    "unanswered_questions": "Does this even work with qualitative data like interview transcripts? Can it help with thematic analysis or is it only for quantitative work? What does 'health check' mean for non-numerical data? Would my advisor even accept this as a legitimate analysis tool?",
    "price_reaction": "$29/month is a lot when I'm making nothing and already paid for Dedoose that I don't use. The institutional price is laughable - my department can barely afford printer paper. I need something free or under $10/month, and I need to know it actually helps with qual analysis first."
  },
  {
    "person_id": 8,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control system for data analysis files that tracks changes across different programming languages and spreadsheets.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "No more 'Final_v2_FINAL.' Just one project with a complete memory.",
    "what_feels_off": "The whole thing feels like it's written for software engineers or data teams at companies, not researchers. The 'Receipt' headline is confusing and corporate. The testimonials are all from analysts and directors, not people doing research. It keeps talking about 'when people leave' and 'legacy code' like I'm managing a team - I'm just one person trying to finish my dissertation.",
    "objections": "I don't have Python scripts or R code or a 'mixed-language legacy codebase' - I have SPSS output files and Excel sheets with survey data. I don't know if this even works with SPSS. The problem isn't version control, it's that I keep having to redo my entire analysis when I add new data because I don't understand how SPSS wants the file structured. This doesn't seem to solve that at all. Also $29/month for years of grad school adds up fast.",
    "dealbreaker": true,
    "dealbreaker_reason": "It doesn't mention SPSS anywhere and my advisor requires it. The product seems built for people who code in multiple languages, not for someone like me who's just trying to run basic statistical tests without breaking everything. It's solving a collaboration problem I don't have instead of the 'I don't understand how to structure my data file' problem I actually have.",
    "gut_reaction": "This sounds like overkill for what I need and I'm not even sure it works with the software I'm required to use. It feels like a tool for tech companies, not dissertations.",
    "unanswered_questions": "Does it work with SPSS? Will it help me understand why my data file structure keeps breaking my analysis? Can it actually teach me what I'm doing wrong or just track versions of the wrong thing? What does 'health check' even mean for survey data?",
    "price_reaction": "$29/month is almost $350 a year, which is a lot on a stipend. The institutional license is $2,500 which my university would never pay for one PhD student. I need something that helps me learn SPSS, not track changes I make in it."
  },
  {
    "person_id": 9,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation system for data analysis files that tracks changes across different programming languages and spreadsheets.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "Our system is messy, but we know how it's broken.",
    "what_feels_off": "The whole thing reads like it's written for actual data scientists with codebases and legacy scripts, not someone manually coding newspaper articles in Google Sheets. The social proof is all from people with 'monthly reports' and 'AI-generated code' \u2014 I don't have either of those problems. The language is corporate tech speak that assumes I'm managing teams and workflows.",
    "objections": "This is clearly built for people who already know how to code and have complex technical setups. I'm literally just trying to count themes in 300 newspaper articles in a Google Sheet. I don't have Python scripts or MATLAB files or 'mixed-language legacy codebases.' The cheapest option is $29/month which is real money on a PhD stipend for something that sounds like overkill. Also I have no idea if this would even work for qualitative coding schemes or if it's just for formulas and calculations.",
    "dealbreaker": true,
    "dealbreaker_reason": "This product is not for me at all. I'm not a data analyst maintaining reports or managing inherited code. I'm a humanities researcher who needs to systematically code historical sources, and nothing on this page speaks to that use case. It assumes technical competence I don't have and solves problems I don't recognize.",
    "gut_reaction": "This feels like enterprise software for actual programmers pretending to be accessible. The words 'audit trail' and 'reproducible methodology' caught my eye for about two seconds before I realized this is built for a completely different world than mine.",
    "unanswered_questions": "Does this even work for qualitative content analysis? Can it handle coding schemes that aren't formulas? Would it help me track how I'm categorizing newspaper articles or is this only for numerical data? Why would a historian need MATLAB support?",
    "price_reaction": "$29/month is groceries for a week. The 'institutional' tier at $2,500/year is laughable \u2014 my department can barely afford coffee for the seminar room. Even if this solved my problem, which it doesn't seem to, I'd need to see a free version for students or just keep suffering through my Google Sheet."
  },
  {
    "person_id": 10,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation system for data projects that works across different file types and programming languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "Our system is messy, but we know how it's broken.",
    "what_feels_off": "The whole thing reads like it's for corporate data teams managing reports, not for someone trying to run structural equation modeling for a dissertation. The testimonials are all from analysts maintaining dashboards and reports - that's literally what I used to do. I don't need audit trails for my committee, I need to figure out what statistical test to run. Also 'Every change has a Receipt' is trying way too hard to be clever and just sounds confusing.",
    "objections": "This doesn't solve my actual problem. I don't have a team or inherited scripts or twelve monthly reports anymore. I have SPSS output I don't understand and a committee asking for analyses I've never heard of. Tracking changes to my Excel files doesn't help me learn what structural equation modeling is or how to interpret the results. This feels like a solution for my old job, not my current nightmare.",
    "dealbreaker": true,
    "dealbreaker_reason": "It's solving a completely different problem than what I'm facing. I don't need version control or audit trails - I need to understand advanced statistical methods my committee is requiring. This is built for people managing operational data workflows, not for someone drowning in academic methodology requirements.",
    "gut_reaction": "This is for the job I left four years ago. I used to worry about documentation and handoffs; now I worry about whether I'm using the right econometric model and my advisor is going to tear apart my methods chapter.",
    "unanswered_questions": "Does this help me understand what statistical tests I should be using? Does it explain model fit indices or help me interpret regression diagnostics? Can it tell me if my data meets the assumptions for the analysis my committee wants? Because that's what I actually need.",
    "price_reaction": "$29/month is steep for something that won't help me graduate. The institutional license at $2,500/year is laughable - my department can barely afford SPSS licenses and this doesn't even do statistical analysis."
  },
  {
    "person_id": 11,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of audit trail system for people who work with spreadsheets and code files.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "Our system is messy, but we know how it's broken.",
    "what_feels_off": "This entire page is written for IT people and data analysts at companies, not researchers. The language is all about 'teams' and 'organizations' and 'inherited workflows' like I'm managing some corporate reporting system. I'm not managing twelve monthly reports. I have transcripts I need to code and survey data I need to analyze. The whole framing is wrong.",
    "objections": "I don't have scripts or code or broken formulas to track. I have Word documents with interview transcripts and an Excel file with survey responses. I need to actually DO the analysis, not audit changes to files. This sounds like it's solving a problem I don't have. Also, I work alone on my dissertation\u2014why would I need version control or to show other people 'exactly what happened'? My committee doesn't care about my file management.",
    "dealbreaker": true,
    "dealbreaker_reason": "This product isn't for dissertation work at all. It's for people inheriting messy corporate systems. I need help analyzing qualitative data and connecting it to my quantitative findings, not tracking formula changes in Excel. And $29/month is money I don't have for something that won't help me code my transcripts or run my stats.",
    "gut_reaction": "I read the whole thing waiting for it to say something about qualitative analysis or mixed methods and it never did. This is for corporate analysts, not PhD students.",
    "unanswered_questions": "Does this actually help me analyze data or just track changes to files? Can it code qualitative data? Does it connect themes from transcripts to survey results? What does 'health check' even mean for a dissertation dataset?",
    "price_reaction": "$29/month is almost a tank of gas. The institutional license at $2,500/year is laughable\u2014my department doesn't have that kind of budget and wouldn't spend it on this even if they did. I can't justify this when I'm not even sure what it would do for my actual work."
  },
  {
    "person_id": 12,
    "bucket": "thesis_dreader",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation tool for data analysis files across multiple languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it.",
    "what_feels_off": "The whole thing feels like it's written for IT managers, not actual analysts. 'Auditable history' and 'compliant audit exports' sound like compliance theater. The hero copy is completely abstract\u2014what is a 'Receipt' supposed to mean to me? Also 'Stop managing files. Start defending results' makes me feel attacked, like I'm doing something wrong instead of just trying to finish my dissertation.",
    "objections": "I don't have twelve monthly reports or a team\u2014I have one messy dissertation analysis that keeps breaking. This feels built for organizations with institutional problems, not a solo PhD student who just needs her pandas merge to work. The testimonials are all from teams and directors, nobody like me. Also, will this even work with Jupyter notebooks? That's not clear. And I don't need an audit trail, I need my code to stop throwing errors.",
    "dealbreaker": true,
    "dealbreaker_reason": "$29/month is a lot when I'm on a grad student stipend and this doesn't actually fix my code\u2014it just documents what's broken. I need something that helps me understand why my statistical analysis is failing, not a fancy changelog for files I'm the only person touching.",
    "gut_reaction": "This reads like enterprise software trying to solve organizational handoff problems. I'm one person trying to wrangle data for a dissertation, not maintain legacy systems.",
    "unanswered_questions": "Does this actually help me understand or fix broken Python code, or just track changes? Does it work with Jupyter notebooks? Will it explain statistical errors or just log them? What does 'visual flow' actually look like\u2014is it useful or just another diagram I have to learn?",
    "price_reaction": "Way too expensive for a grad student. The Individual tier is more than my Spotify and Netflix combined, and the academic 'Institutional' tier assumes I have departmental budget, which I don't. Where's the student discount?"
  },
  {
    "person_id": 13,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control system for data work that tracks changes across Excel, scripts, and multiple file types.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The hero headline is pretentious and confusing. 'Every change has a Receipt' sounds like marketing trying too hard to be clever. The copy keeps saying it works with everything but I have no mental model of what this actually looks like. Is it a desktop app? Browser? Does it replace Excel or sit next to it? The 'health check' feature sounds useful but then disappears and never comes back. Also, I don't have legacy codebases or mixed-language projects\u2014I have five Excel files that need the same pivot tables every month.",
    "objections": "This feels built for engineers inheriting messy code, not for someone like me doing program evaluation in Excel and Tableau. My problem isn't version control or audit trails, it's that I'm manually doing the same transformations five times a month. None of this tells me if it can automate my actual repetitive work or if it just watches me do it and writes it down. The testimonials are from people with way more complex setups than mine. And $29/month is real money when I'd still be doing all the manual work myself, just with better documentation nobody asked for.",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't tell if this solves my actual problem\u2014repetitive manual data cleaning and reporting. It sounds like it documents what I do, but I need something that does it for me or at least reduces the repetition. The price is too high for a documentation layer when my budget is zero and I'd need to justify it to my director.",
    "gut_reaction": "This sounds over-engineered for my needs. I don't have code dependencies or multiple languages\u2014I have the same Excel cleanup steps I repeat every month across five programs. I need automation, not audit trails.",
    "unanswered_questions": "Does this actually automate repetitive tasks or just document them? What does the interface look like? Can it learn my Excel cleanup steps and repeat them? Does it connect to Tableau? How would I explain the $348/year cost to my boss when our tool budget is basically nonexistent? Is there a nonprofit discount?",
    "price_reaction": "$29/month is nearly $350 a year for something I'm not sure solves my problem. Team pricing makes no sense for me. Institutional at $2,500 is laughable\u2014my entire nonprofit probably doesn't spend that on software. If this actually automated my monthly cleanup I might consider $10-15/month."
  },
  {
    "person_id": 14,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation layer for spreadsheets and data scripts, but I honestly can't tell if it's a desktop app, a cloud platform, or what.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole thing reads like AI copy trying to sound technical. 'Every change has a Receipt' is meaningless corporate poetry. The 'health check' and 'visual flow' features are described but I have zero idea what they actually look like or how they'd work with my mess of Google Sheets tabs and Claude exports. Also suspicious that all three testimonials are conveniently anonymous job titles.",
    "objections": "I don't understand the actual interface or workflow. Do I work in CleanSheet or do I keep working in Excel and Sheets like normal? How does it 'connect' to my files\u2014does it read them, copy them, require exports? The 'upload a zip file' thing makes it sound like a one-time static analysis tool, but then it talks about ongoing tracking. These seem contradictory. Also my clients would never let me upload their sensitive data to some random SaaS tool without a security audit.",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't upload client data to a third-party platform without their explicit approval, and most of my early-stage clients don't have the bandwidth to vet new tools. Even if they did, I don't see how this solves my actual problem\u2014making my analysis outputs look professional. This seems built for inherited legacy systems in large orgs, not for solo consultants duct-taping together decent-looking deliverables.",
    "gut_reaction": "This feels like it was built for data engineers at mid-size companies dealing with technical debt, not for me. The language is trying too hard to sound sophisticated and the use case doesn't match my workflow at all.",
    "unanswered_questions": "Where does my data actually live? Is this a SaaS platform or local software? How does it integrate with tools I'm already using versus requiring me to change my workflow? Can I white-label outputs for clients? What does the actual UI look like? Is there SOC 2 compliance or any security certifications?",
    "price_reaction": "$29/month for a solo tool I'm not sure I need is a tough sell when I'm already paying for subscriptions. The Team tier at $149 makes no sense for me since I work alone. The jump to $2,500/year for Institutional is absurd\u2014I'm nowhere near that scale and the features listed don't justify it."
  },
  {
    "person_id": 15,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation layer for data analysis files across multiple tools and languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The whole thing feels like it was written for data engineers, not researchers. The 'Receipt' metaphor in the headline is confusing and doesn't land. Step 2 is nearly identical to Step 1 but longer. The objection handling feels like you're anticipating problems you haven't proven you solve. Also, 'Stop managing files. Start defending results' sounds like MBA-speak, not how anyone in my world actually talks.",
    "objections": "I work with qualitative data and mixed-methods research\u2014interview transcripts, survey results, synthesis documents. This reads like it's built for quantitative analysts working with code and spreadsheets. I don't see myself or my workflow anywhere in this copy. Also, you say it 'makes sense of' uploaded files but don't explain how. The ChatGPT transcript thing burned me because I couldn't verify what it did\u2014this sounds like the same black box problem dressed up differently.",
    "dealbreaker": true,
    "dealbreaker_reason": "This isn't built for my use case. I need tools that handle unstructured qualitative data, not Excel formulas and Python scripts. Even if it could theoretically work, nothing in this copy speaks to research synthesis, thematic analysis, or stakeholder reporting\u2014which is 90% of my job.",
    "gut_reaction": "This reads like Git for spreadsheets, which solves a real problem for analysts who inherit broken Excel models, but that's not me. The copy keeps talking about code and scripts and I barely touch those.",
    "unanswered_questions": "Does this work with qualitative data at all? What does 'makes sense of it' actually mean technically? Is this doing AI interpretation or just version tracking? How does the visual flow thing work with non-code workflows? What does 'compliant audit exports' mean for academic research specifically?",
    "price_reaction": "The individual tier is reasonable. Institutional pricing at $2,500/year for unlimited users is probably a good deal for large research teams, but it's not relevant to me as a solo researcher. I'd need to see this work for my actual files before I'd spend $29/month on it."
  },
  {
    "person_id": 16,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's supposed to be a version control and documentation layer that sits on top of your existing Excel, scripts, and data files to track changes and make inherited workflows more understandable.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The whole thing reads like it was written by someone who doesn't actually do this work. 'Upload a zip file and we'll make sense of it' \u2014 of what, my eight years of Qualtrics exports, Excel macros, and Tableau extracts? How? Step 2 contradicts Step 1, and the 'visual flow' language is extremely vague. Also, 'Stop managing files. Start defending results' sounds like a consultant wrote it, not someone who's actually been in a grant review meeting.",
    "objections": "This doesn't integrate with Qualtrics or Tableau, which is where my actual workflow lives. It says it 'works with Excel' but doesn't explain if that means I have to upload files manually every time or if it syncs. The 'health check' and 'automated suggestions' make me nervous \u2014 I can't have something flagging my methodology as 'broken' right before a board presentation. And I have zero confidence this won't get blocked by our IT policy just like Power BI did.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot risk my quarterly reporting cycle on a tool that doesn't explicitly integrate with my actual stack and will probably get blocked by IT anyway. The $2,500/year institutional tier is more than my entire software budget and I'd still need to get IT approval, which took six months for the last thing I tried. This solves a problem I have but creates three new ones.",
    "gut_reaction": "This feels like it was built for software engineers doing data science, not for nonprofit researchers doing compliance reporting with inherited Excel files. The use case sounds right but the solution doesn't fit my actual constraints.",
    "unanswered_questions": "Does this require cloud upload of sensitive grant data? Will it work behind our firewall? Does it integrate with Qualtrics and Tableau or do I manually export everything? What happens when IT blocks it? Can I export the audit trail in a format my funders will actually accept? Who owns the data?",
    "price_reaction": "The institutional tier is the only one that would matter to my organization and it's cost-prohibitive for a small nonprofit. Even if I could justify it, I'd need IT approval, a security review, and probably board sign-off. The individual tier is useless because my team needs shared access, and the team tier caps at 10 users but doesn't clarify if that includes external evaluators or just internal staff."
  },
  {
    "person_id": 17,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's version control and dependency mapping for analysis files across different tools and languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The hero headline is trying too hard to be poetic and doesn't tell me what the product does. 'Receipt' in quotes feels gimmicky. The whole 'Stop managing files. Start defending results' line is generic inspirational copy that could apply to twenty different products. Also, if this works with SPSS and NVivo you buried the lede \u2014 you list 'and more' but those are the specific tools I need coordination on.",
    "objections": "I have no idea if this actually integrates with SPSS or NVivo, which is my entire problem. You say 'Excel, Python, R, MATLAB, SQL' but those aren't the bottleneck tools for government contract work. I need to know if your dependency mapping understands SPSS syntax files and NVivo coding schemes, or if you're just doing file-level version control, which Git already does. Also, how does this help me when one employee doesn't know SPSS? Does it translate outputs? Train people? Or just show me what's broken, which I already know?",
    "dealbreaker": true,
    "dealbreaker_reason": "Doesn't explicitly support the proprietary tools my clients require (SPSS, NVivo). If it did, that would be the headline. The fact that it's not mentioned means I'd be paying to solve a problem I don't have while my actual coordination nightmare continues.",
    "gut_reaction": "This is version control for data work, which I understand the value of, but it doesn't solve my operational problem of having two people who can't cover for each other because they know different required software. I need interoperability or training support, not audit trails.",
    "unanswered_questions": "Does this work with SPSS and NVivo? Can it translate between tool outputs so my team can actually hand off work? What does 'health check' mean for a mixed SPSS/Excel workflow? Can reviewers from federal agencies actually use the audit trail format, or is it just internal documentation? What happens to my data \u2014 is it uploaded to your servers or processed locally?",
    "price_reaction": "Institutional pricing at $2,500/year for unlimited users is actually reasonable for government contract budgets, but I can't justify it if the tool doesn't support the software my contracts require. The individual tier is pointless for me since my problem is team coordination, not personal organization."
  },
  {
    "person_id": 18,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It tracks changes to spreadsheets and code files so you can see version history and dependencies.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The hero headline is pretentious and unclear. The whole thing feels like it's trying too hard to sound important with words like 'auditable history' and 'methodological control' when what I need is something simple. The peer quotes are too perfectly on-message and don't sound like real people talking. The 'Stop managing files. Start defending results' line is eye-roll territory.",
    "objections": "This sounds like version control for spreadsheets, which I don't really need because my Excel files aren't breaking\u2014my problem is that the data is trapped in Salesforce and I have to manually export and reshape it every month. This doesn't touch my actual workflow at all. Also I work alone on my reports, so the collaboration features don't matter to me. And I'm skeptical it can actually 'make sense of' a messy system\u2014that sounds like marketing speak.",
    "dealbreaker": true,
    "dealbreaker_reason": "This solves a problem I don't have. My spreadsheets work fine\u2014it's getting clean data OUT of Salesforce that kills my time. I need help with data extraction and transformation, not version history. And $29/month for a solo tool when I already have a working Excel process feels like a waste of limited professional development budget.",
    "gut_reaction": "This is trying to sell me git for Excel when what I actually need is someone to teach me how to build proper Salesforce reports. Totally missing my pain point.",
    "unanswered_questions": "Does this actually connect to Salesforce or other databases where my source data lives? Can it automate my monthly export-and-reshape routine? What does 'health check' even mean in practice\u2014will it tell me my formulas are fine when I already know they are?",
    "price_reaction": "The institutional tier at $2,500/year is laughable for my org\u2014we barely have budget for our existing software licenses. Even $29/month is hard to justify when it doesn't solve my core problem of wrangling Salesforce data. The team tier makes no sense for my use case since I'm the only one doing this analysis."
  },
  {
    "person_id": 19,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation layer for Excel files and code that tracks changes and shows dependencies.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The hero copy is trying way too hard to be poetic\u2014'Every change has a Receipt' doesn't land. The 'mixed-language legacy codebases' promise sounds like vaporware. If this actually parsed dependencies across Python, R, MATLAB, and Excel accurately, that would be the lead, not buried in step 2. Also, the social proof quotes are suspiciously on-message and feel AI-generated themselves.",
    "objections": "I don't actually have a problem with version control\u2014my clients care about the insights, not the audit trail of my formulas. This feels like it's solving for compliance teams or academics, not consultants who need to deliver recommendations. The 'health check' and 'broken references' thing might be useful but it's not clear if that's the main product or just a feature. Also, I have no idea how this would integrate into my actual workflow when I'm working in a client's secure environment.",
    "dealbreaker": true,
    "dealbreaker_reason": "My work happens in client systems with strict data governance. I can't just 'upload a zip file' of their data to some third-party tool. This entire product assumes I own and control my data infrastructure, which I don't. It's built for internal analysts, not consultants.",
    "gut_reaction": "This reads like someone built Git for Excel users but doesn't understand that most of us don't need Git for Excel. The problem isn't tracking my changes\u2014it's synthesizing insights my clients will pay for.",
    "unanswered_questions": "Can this work on-premise or in a client's secure environment? Does it actually help me do thematic analysis or is it just documentation? What does 'health check' actually catch that Excel's error checking doesn't? How does this compare to just using Git if I'm already working in code?",
    "price_reaction": "$29/month is steep for what sounds like a documentation tool I don't need. The institutional tier at $2,500/year makes sense for universities but I'm not the buyer there. The team tier doesn't apply because I work solo on client engagements. No pricing tier fits a consultant who works across multiple client environments."
  },
  {
    "person_id": 20,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation layer for mixed data analysis files, but I'm not clear if it's a collaborative workspace or just a logging tool.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is pretentious and confusing\u2014'Every change has a Receipt' doesn't tell me anything. The whole thing reads like it was written by someone who's never managed a team through an actual tool migration. Also, Step 1 and Step 2 seem to describe the same thing twice, which makes me wonder if the product itself is half-baked.",
    "objections": "This doesn't solve my actual problem, which is getting three people to work the same way. They'll just keep using their own tools and ignore this. Also, 'upload a zip file' sounds like a one-time forensic tool, not daily workflow software. How does this integrate with SurveyMonkey or our grant reporting templates? And the testimonials are too perfect\u2014no one talks like that.",
    "dealbreaker": true,
    "dealbreaker_reason": "I need something that forces standardization and makes collaboration non-optional. This sounds like it sits on top of everyone's existing chaos and just documents it better. That doesn't fix the root problem\u2014it just gives me fancier documentation of three different workflows. Also, at $2,500/year for institutional when my team is only 3 people, I'd pay $149/month for Team tier, but then I'm locked at 10 users when I have 18 grantees I might want to give access to.",
    "gut_reaction": "This feels like it was built for solo data scientists who inherited messy code, not for managers trying to herd cats into using the same platform. The pitch assumes my problem is documentation when my problem is alignment.",
    "unanswered_questions": "Does this replace our existing tools or just add another layer? How do my evaluators actually use this day-to-day\u2014do they work IN CleanSheet or does it just watch what they do elsewhere? What happens to our existing SurveyMonkey workflows? Can grantees access read-only views? How does this handle our IRB compliance requirements for health data?",
    "price_reaction": "Team tier caps at 10 users but I have 18 grantees plus my 3-person team. Institutional is $2,500/year which is actually cheaper than Team tier annually, but 'unlimited users' for a small foundation feels like paying for capacity I don't need. The pricing structure doesn't match my org size\u2014we're too big for Team, too small for Institutional pricing to feel proportional."
  },
  {
    "person_id": 21,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation layer for data analysis files across multiple tools, but I'm not clear if it actually runs the code or just logs what I do manually.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The whole thing reads like it was written by someone who doesn't actually do this work. 'Upload a zip file and we'll make sense of it' \u2014 make sense of it how? Parse it into what? The 'health check' in Step 1 sounds like vaporware. And the objection handling is patronizing, especially 'our system is messy, but we know how it's broken' \u2014 that's not my objection at all.",
    "objections": "I have no idea what the actual interface looks like or how it integrates with my existing workflow. Does it replace Excel? Does it sit alongside it? Do I work in CleanSheet or in my normal tools? The 'visual flow' and 'connected logic' language is too abstract. I need to see a screenshot. Also, 'automated suggestions' appears in objection handling but was never mentioned in the features \u2014 what suggestions? This feels half-baked.",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't tell if this product actually exists or is just a landing page collecting emails. There's a waitlist button next to the trial button, which suggests it's not ready. I'm not going to be an unpaid beta tester for something this vague about what it actually does technically.",
    "gut_reaction": "This is trying to solve a problem I have, but it's written in such generic 'workflow automation' language that I don't trust they understand the actual mechanics of policy analysis work. Feels like a concept, not a product.",
    "unanswered_questions": "Does it actually execute my Python and R code or just document it? How does it handle sensitive data \u2014 can I run this locally? What does the audit trail look like in practice \u2014 is it just Git for non-coders? Can I export to formats my reviewers actually use? Does it work with SPSS at all, since that's not mentioned once?",
    "price_reaction": "The individual price is reasonable if it works, but the institutional tier at $2,500/year for unlimited users is suspiciously cheap for something claiming to handle compliance and audit requirements \u2014 that makes me think it doesn't actually meet real institutional standards."
  },
  {
    "person_id": 22,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's version control and documentation for data analysis files across multiple tools and languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me.",
    "what_feels_off": "The hero headline is pretentious and unclear\u2014'Receipt' with a capital R means nothing to me. The whole thing reads like it's solving for data engineers inheriting legacy systems, not for solo consultants doing primary research. The 'mixed-language legacy codebases' positioning is totally off for my work\u2014I'm running surveys and doing thematic analysis, not debugging MATLAB scripts from 2019.",
    "objections": "This isn't built for UX research workflows at all. I need survey tools, participant recruitment, qualitative coding, and synthesis\u2014not audit trails for Excel macros. The testimonials are all analysts maintaining inherited reports, which isn't my problem. I'm creating new research from scratch on every project. Also at $29/month it's another SaaS bill when I'm already bleeding money on tools, and I don't see how this replaces any of them.",
    "dealbreaker": true,
    "dealbreaker_reason": "Wrong use case entirely. I'm not inheriting broken spreadsheets, I'm conducting original research. This solves for technical debt in data pipelines, not for research operations. Doesn't integrate with any of the tools I actually need\u2014Dovetail, UserTesting, Optimal Workshop, Qualtrics.",
    "gut_reaction": "This is positioned for data analysts dealing with legacy technical debt, not researchers. The opening made me work too hard to understand what it even does, and once I figured it out, I realized it's not for me at all.",
    "unanswered_questions": "Does this integrate with actual research platforms? Can it handle qualitative data or just quantitative? What about participant data privacy and GDPR compliance? Does it replace anything I'm currently paying for or is it just another cost on top?",
    "price_reaction": "$29/month is low for enterprise data teams but high for solo consultants who need research-specific tools instead. I can't justify it when it doesn't solve my actual problem\u2014tool costs for running studies, not documenting spreadsheet changes."
  },
  {
    "person_id": 23,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation layer for mixed data analysis files that creates an audit trail.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "Our system is messy, but we know how it's broken.",
    "what_feels_off": "The whole thing reads like it was written by someone who's never actually dealt with field data collection in low-connectivity environments. The social proof quotes are generic and suspiciously on-message. 'Operations Analyst, Regional Healthcare System' could be anyone or no one. Also, 'Upload a zip file and we'll make sense of it' is exactly the kind of promise I've heard twelve times before that falls apart when you actually have ODK exports with GPS coordinates, branching logic, and repeat groups mixed with manual Excel adjustments from country offices.",
    "objections": "This solves a different problem than I have. My issue isn't version control or audit trails on analysis files\u2014it's getting clean data OUT of the field in the first place and standardizing collection across teams with spotty internet and varying technical capacity. This looks built for people doing analysis in stable office environments, not M&E in international development. Also, no mention of how it handles actual data collection tools, mobile data, or offline sync. And I don't see how this helps when my South Sudan team is using a three-year-old version of KoBoToolbox because that's what works on their tablets.",
    "dealbreaker": true,
    "dealbreaker_reason": "Doesn't address field data collection, connectivity issues, or the actual coordination problems across distributed teams in low-resource settings. This is a tool for analysts who inherit messy files, not for M&E directors managing data pipelines from village-level surveys to donor reports.",
    "gut_reaction": "This is Git for spreadsheets marketed to people who don't know Git exists. Might work for a corporate analyst, but it's solving the wrong 20% of my problem and ignoring the 80% that actually breaks my workflow.",
    "unanswered_questions": "Does it work offline? How does it integrate with actual data collection platforms like ODK, KoBoToolbox, or CommCare? Can field teams with limited bandwidth use this? What happens when someone uploads a 500MB dataset with Arabic and French mixed metadata? How does it handle geospatial data or multimedia attachments from mobile surveys?",
    "price_reaction": "The institutional tier at $2,500/year for unlimited users actually isn't outrageous for an NGO budget line, but I'd never get approval without a pilot that proves it solves our actual coordination problem, which I don't think it does. The individual and team tiers are irrelevant\u2014my team can't each use different tools."
  },
  {
    "person_id": 24,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "It's supposedly a version control and documentation layer for spreadsheets and scripts that tracks changes across multiple file types.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The whole thing reads like a solution looking for a problem. 'Every change has a Receipt' is awkward marketing speak. The 'health check' and 'visual flow' descriptions are vague \u2014 I have no idea what I'm actually looking at. The objection handling feels patronizing, especially 'That institutional knowledge is the risk' \u2014 you're telling me my expertise is a liability? And the mixed-language parsing claim is a huge technical promise with zero proof.",
    "objections": "I don't understand what the interface actually looks like or how it integrates with my existing Excel workflow. Does it open Excel files natively or do I work in some web interface? The dependency mapping sounds nice in theory but I've seen these tools completely misread complex formulas. No mention of whether this works with my VBA macros or Power Query connections. And frankly, Git exists for version control \u2014 why do I need this proprietary black box?",
    "dealbreaker": true,
    "dealbreaker_reason": "I'm not uploading my clients' proprietary financial models to some cloud service. The security and data governance piece is completely unaddressed. Plus, this feels like it's trying to replace my workflow understanding with automated scanning, which means when it gets something wrong, I'm worse off than before.",
    "gut_reaction": "This feels like someone built Git for non-technical people but made it sound more complicated in the process. I don't trust the 'we'll make sense of it' promise \u2014 my spreadsheets are complex for good reasons.",
    "unanswered_questions": "Where does my data go? Can I run this locally? What happens when your AI misreads a complex nested IF statement or a circular reference I built intentionally? Does this actually open Excel or just parse it? What about macros, pivot tables, external data connections?",
    "price_reaction": "The Individual tier is reasonable for what it claims, but the Institutional jump to $2,500 feels arbitrary. The Team tier at $149 for 10 users is oddly positioned \u2014 most consultancies either have solo practitioners or full teams. I'd want to try this for free for at least 30 days on real client work before committing."
  },
  {
    "person_id": 25,
    "bucket": "ai_loop_prisoner",
    "resonance": "disagree",
    "clarity_response": "A version control and audit tool for spreadsheets and mixed-language data analysis projects.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole thing reads like it was written by AI trying to sound like it understands messy research workflows but doesn't actually solve my problem. 'Every change has a Receipt' is meaningless corporate speak. The testimonials feel manufactured\u2014no one talks like that. And the product seems to be solving the problem of inheriting OTHER people's messy code, not fixing the code ChatGPT is giving me right now that doesn't work.",
    "objections": "This doesn't help me when I'm stuck debugging ChatGPT's broken R code at 9pm. I don't need an audit trail of my five iterations of bad code\u2014I need code that actually runs the first time. Also unclear if this even works with the back-and-forth loop I'm in with ChatGPT or if I'd have to completely change my workflow. The 'health check' sounds nice but I know my data is fine, it's the generated code that's wrong.",
    "dealbreaker": true,
    "dealbreaker_reason": "$29/month is real money when I'm saving for med school applications, and this seems to document my process rather than fix the actual problem\u2014ChatGPT giving me confident code that breaks. I need something that validates or fixes AI-generated code in real time, not a fancy filing system for my debugging hell.",
    "gut_reaction": "This feels like it's for institutional legacy system problems, not for someone trying to get analysis done with AI tools that half-work. Reads very enterprise-y and vague about what it actually does.",
    "unanswered_questions": "Does this integrate with ChatGPT at all? Can it validate AI-generated code before I run it? What does 'made the logic readable' actually mean\u2014does it explain the code or just organize it? How does uploading a zip file help me when I'm iterating with ChatGPT in a browser?",
    "price_reaction": "$29/month is almost what I pay for groceries. For a grad school applicant this needs to save me multiple hours every week to be worth it, and I'm not convinced it does anything except document the mess I'm already creating. The institutional pricing makes sense for labs but I'm not the decision maker there."
  },
  {
    "person_id": 26,
    "bucket": "ai_loop_prisoner",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation system for data analysis files across multiple languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it.",
    "what_feels_off": "The whole thing feels written for corporate analysts managing reports, not researchers running statistical models. The headline about 'receipts' is trying too hard to be clever. The social proof quotes sound manufactured \u2014 no academic talks like 'paid for itself in reporting time.' And it keeps saying it handles mixed-effects models but never actually shows me HOW.",
    "objections": "This doesn't solve my actual problem. I don't need version control for spreadsheets \u2014 I need something that helps me specify and debug statistical models. It says it works with Python and R but doesn't explain what that means for my lme4 code that won't converge. Is it just going to show me a pretty flowchart of my broken code? I already know the code is broken. I need it to actually help me fix the model specification.",
    "dealbreaker": true,
    "dealbreaker_reason": "This is built for people managing inherited Excel reports, not for researchers doing statistical analysis. It's solving file management and audit trails when my problem is understanding why my random effects structure is wrong. $29/month to track changes in files I can already track with git is not worth it when I'm on a PhD stipend.",
    "gut_reaction": "This looks like it's for corporate data analysts who inherited messy dashboards. I write code to run stats, not manage monthly reports. Nothing here tells me it understands statistical modeling.",
    "unanswered_questions": "Does this actually help with model specification? Can it diagnose convergence issues? Does it understand statistical syntax or just track file changes? What does 'works with Python and R' actually mean \u2014 does it parse statsmodels and lme4 syntax specifically?"
  },
  {
    "person_id": 27,
    "bucket": "ai_loop_prisoner",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation tool for mixed data work and code, but I don't understand how it actually works under the hood.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole thing feels like it was written by a product manager who doesn't actually do hands-on data work. 'Navigable history' and 'visual flow' are vague buzzwords. How does it make AI code readable? Does it just document it or does it actually explain what's happening? The hero copy is trying too hard to sound profound with the receipt metaphor.",
    "objections": "I have no idea if this actually solves my problem with ChatGPT giving me code that breaks. Does it debug? Does it explain logic? Or does it just track changes like Git already does? The 'health check' and 'parsing dependencies' sounds like magic hand-waving. Also $29/month is real money when I'm not even sure this does more than Git plus some documentation I could write myself.",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't tell if this helps me understand and fix AI-generated code or if it just documents the mess I'm already making. If it's just fancy version control, I'm paying $29/month for something Git does for free. And if my boss sees I'm using a tool to 'make AI code readable' that's basically admitting I don't understand what I'm shipping.",
    "gut_reaction": "This sounds like it's for people who inherited other people's messy work, not for someone like me who's actively producing questionable code every day. I need something that helps me learn and fix what ChatGPT spits out, not just document my confusion.",
    "unanswered_questions": "Does this actually help me understand code or just track it? Can it catch when ChatGPT code will break before I run it? Does it integrate with ChatGPT or do I still have to copy-paste? What does 'make the logic readable' actually mean technically? Will my company be able to see I'm using this and ask why I need help understanding my own work?",
    "price_reaction": "$29/month is almost 10% of what I'm taking home after taxes and rent. The testimonials are from people with fancier titles than mine at bigger organizations. This feels like it's priced for people with actual budgets, not bootcamp grads trying to fake it."
  },
  {
    "person_id": 28,
    "bucket": "ai_loop_prisoner",
    "resonance": "disagree",
    "clarity_response": "It's version control and dependency mapping for data analysis files across different tools and languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it.",
    "what_feels_off": "The whole thing feels like it's solving a different problem than mine. The testimonials are about inherited workflows and multi-person handoffs, but I'm a solo freelancer pasting survey data into ChatGPT. The 'auditable history' pitch assumes I have a methodology worth documenting in the first place. Also 'CleanSheet' sounds generic and the tagline 'Every change has a Receipt' is trying too hard to be clever.",
    "objections": "This doesn't actually help me know when ChatGPT is wrong about statistical significance or misinterpreting my survey data. It tracks changes to files, but my problem is that I don't know if the analysis itself is correct. I'm not inheriting messy codebases from other people\u2014I'm just bad at quant analysis and using AI as a crutch. This would just give me a documented history of potentially wrong interpretations.",
    "dealbreaker": true,
    "dealbreaker_reason": "It doesn't solve my actual problem. I need to validate whether my statistical conclusions are correct, not track versions of spreadsheets. This is built for teams with legacy code debt, not solo researchers who need to upskill on quantitative methods.",
    "gut_reaction": "This reads like it's for data engineers or ops analysts managing inherited messes. I'm a freelancer who needs to know if my ChatGPT-assisted stats are legit, and this doesn't address that at all.",
    "unanswered_questions": "Does this actually validate statistical analysis or just track file versions? Can it tell me if a finding is actually significant or if I'm misinterpreting correlation? Does it integrate with Qualtrics exports specifically?",
    "price_reaction": "$29/month isn't outrageous but it's not solving my problem so it's $29 wasted. The team tier makes no sense for me. If this actually helped me verify my quant work I'd pay it, but it won't."
  },
  {
    "person_id": 29,
    "bucket": "ai_loop_prisoner",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation system for spreadsheets and code, but I'm not sure if it actually helps me do the regression correctly.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it.",
    "what_feels_off": "The whole thing is about tracking changes and documentation, but my problem isn't that I can't see what I did\u2014it's that I don't know if what I did is statistically correct. This feels like it solves a different problem than the one I have. The testimonials sound very polished and corporate, not like real stressed people talking.",
    "objections": "This doesn't help me understand regression or know if my model is right. It just tracks my mistakes better. I need someone to tell me if my residuals are actually a problem or if I'm freaking out over nothing. Also $29/month is another subscription when I'm already paying for ChatGPT Plus and I'm not even sure this addresses my actual issue.",
    "dealbreaker": true,
    "dealbreaker_reason": "It doesn't solve my core problem\u2014I need to understand if my analysis is methodologically sound, not just have a record of all the wrong things I tried. This is documentation software for people who already know what they're doing.",
    "gut_reaction": "This reads like it's for IT people or data teams who inherit messy projects. I'm not inheriting anything\u2014I'm the one creating the mess because I don't understand the statistics well enough. Wrong tool.",
    "unanswered_questions": "Will this actually tell me if my regression model is set up wrong? Does it have any statistical validation features? Can it explain why my residuals look weird or just show me a history of when they started looking weird?",
    "price_reaction": "$29/month feels steep when I don't even know if it addresses my problem. The institutional pricing is irrelevant to me. I'd need to be really confident this helps before adding another monthly charge."
  },
  {
    "person_id": 30,
    "bucket": "ai_loop_prisoner",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation layer for data work, but I honestly can't tell if it's actually running my code or just tracking changes to files.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The hero section is trying way too hard to be poetic with 'Receipt' and 'trail' \u2014 just tell me what it does. The 'mixed languages, legacy codebases' promise sounds like vaporware. How is it actually parsing Python, R, MATLAB, and Excel together? That's an insane technical claim with zero explanation of how it works or what the limits are.",
    "objections": "I don't understand what this actually does with my statistical work. Does it check my A/B test calculations? Does it validate confidence intervals? Or is it just Git for spreadsheets? The testimonial about 'made the logic readable' is the only hint it might help with my actual problem, but there's zero detail about HOW it makes code readable or what that even means. I need something that catches stats errors, not just tracks when I changed a formula.",
    "dealbreaker": true,
    "dealbreaker_reason": "It doesn't address my core need at all. I'm getting burned by ChatGPT giving me wrong statistical interpretations. This seems to be about version control and documentation, not about validating whether my statistical methods are correct. I can't tell if it would have caught those confidence interval errors or not \u2014 and that's the only reason I'd pay for something right now.",
    "gut_reaction": "This feels like it's solving a different problem than mine. I need something that checks my stats work for correctness, not something that documents my workflow. The copy makes big claims about parsing multi-language codebases but gives me no reason to believe it actually works.",
    "unanswered_questions": "Does it actually validate statistical methods or just track changes? What does 'made the logic readable' mean technically? Can it catch incorrect confidence interval calculations? Does it integrate with Mixpanel or Amplitude? What are the actual limits of the code parsing \u2014 will it choke on my messy Python scripts?",
    "price_reaction": "$29/month is reasonable if it actually solved my problem, but I have no confidence it does. The institutional tier seems weirdly expensive compared to the individual tier \u2014 that's an 86x jump for unlimited users, which doesn't make sense unless this is really meant for academia."
  },
  {
    "person_id": 31,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It's a version control and dependency mapping system for data analysis files across multiple languages that automatically logs every change you make.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The 'Receipt' headline is trying too hard to be clever and doesn't actually communicate anything useful\u2014I almost bounced immediately. Also not clear if this actually solves my regeneration problem where ChatGPT gives me different code that produces different results, or if it just documents what happened after the fact.",
    "objections": "Does this actually help me when ChatGPT regenerates analysis code differently, or does it just show me a log of the two different outputs? I need to know if the AI-generated code is statistically sound, not just track that I changed it. The health check sounds promising but what does 'inconsistencies' mean for Python code doing regression analysis? Also unclear if this works with Jupyter notebooks or just .py files.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is definitely speaking to my pain but I'm not convinced it solves the actual problem\u2014I need to know if my AI-generated statistical code is correct, not just have a better changelog. That said, being able to see what changed between versions when an editor asks for revisions would save me hours of paranoid diff-checking.",
    "unanswered_questions": "Does it work with Jupyter notebooks? Can it actually validate statistical analysis or just track changes? When it 'parses' Python code, does it understand pandas/numpy/statsmodels logic or just file dependencies? What does the 'visual flow' actually show me\u2014is it useful or just decorative? If I upload two versions of analysis code that produce different results, will it tell me why or just that they're different?",
    "price_reaction": "$29/month is reasonable if it actually prevents me from publishing bad numbers, but that's almost 10% of a typical story payment so it needs to save me real time or reduce real risk. The trial is good\u2014I'd need to test it on an actual revision scenario where an editor asks me to rerun something."
  },
  {
    "person_id": 32,
    "bucket": "ai_loop_prisoner",
    "resonance": "disagree",
    "clarity_response": "It's version control and documentation for data analysis projects across multiple file types and programming languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it.",
    "what_feels_off": "The whole thing feels like it was written by AI trying to sound authoritative - especially 'Stop managing files. Start defending results.' That's the kind of dramatic pivot phrase that sounds good but means nothing. Also, the testimonials are too perfectly on-message. And the product claims to parse 'mixed languages, legacy files, all of it' which is a massive technical claim with zero proof.",
    "objections": "I don't work with Python or R or mixed codebases - I work with SPSS and Excel. This seems built for people with way more technical complexity than I have. The health check sounds nice but I'm not sure what 'inconsistencies' and 'formatting gaps' mean for my actual work. Does it understand SPSS syntax files? Probably not. And $29/month is another subscription when I'm already unsure if Claude is giving me the right answers - why would I trust another tool to tell me what's wrong with my data?",
    "dealbreaker": true,
    "dealbreaker_reason": "It doesn't mention SPSS at all, and I'm not confident it would actually understand the statistical output I'm struggling with. This feels like it's for software developers or data scientists with complex codebases, not analysts like me who need help interpreting regression coefficients and model fit statistics. I need something that helps me understand if my analysis is correct, not just tracks versions of files.",
    "gut_reaction": "This sounds impressive but not for me. It's solving a problem I don't really have - I'm not maintaining legacy code written by multiple people. I just need to know if my logistic regression is right.",
    "unanswered_questions": "Does it work with SPSS? Does it actually help me understand whether my statistical interpretations are correct, or does it just track changes? What does 'health check' mean for statistical analysis versus code? How would this be better than just asking Claude to check my work?",
    "price_reaction": "$29/month is almost what I pay for streaming services, and those I actually use daily. This feels like a big commitment for something I'm not sure would help with my actual problem, which is understanding statistical output, not version control."
  },
  {
    "person_id": 33,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It's version control and dependency mapping for mixed data workflows including Excel, R, Python, and other scripting languages.",
    "intent": "agree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The testimonials feel slightly too on-the-nose, like they were written to address specific personas rather than being real quotes. The healthcare analyst quote is fine but the startup one reads like it was engineered for people like me. Also 'Stop managing files. Start defending results' sounds like copywriting, not how I'd actually think about my problem.",
    "objections": "I don't understand how this actually makes ChatGPT-generated R code more understandable to me. Does it explain what the code does? Does it teach me R? Or does it just make a pretty diagram of dependencies? If it's the latter, I still can't sign off on code I don't understand. Also, will my clients accept 'I used CleanSheet to validate this' as proof I actually know what I'm doing? This feels like it might be another layer of tools between me and actually learning R.",
    "dealbreaker": true,
    "dealbreaker_reason": "This doesn't solve my actual problem. I need to understand R well enough to write and defend my own code, not add another documentation layer on top of AI-generated code I still don't understand. If a client or peer reviewer asks me a technical question about my methodology, CleanSheet's audit trail won't help me answer it. I'll still be exposed as someone who can't actually code in R.",
    "gut_reaction": "This hit uncomfortably close to home with the AI-generated code validation quote, but when I actually think about it, this is just a better way to organize and track the mess I'm already in. It doesn't get me out of the mess.",
    "unanswered_questions": "Does this actually help me understand code or just document it? Can it explain R functions to me in plain language? Will using this make me more competent or just better at hiding my incompetence? What happens when a client asks me to modify the R code live in a meeting?",
    "price_reaction": "$29/month is reasonable but I'm already paying for ChatGPT Plus and feeling guilty about that. Adding another subscription to prop up skills I should just learn properly feels like throwing good money after bad. The institutional tier is interesting if I could get a school district to buy it, but I'd need to be confident it actually solves my problem first."
  },
  {
    "person_id": 34,
    "bucket": "ai_loop_prisoner",
    "resonance": "disagree",
    "clarity_response": "It's a version control and dependency mapping tool for mixed data analysis files that creates an audit trail.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole pitch is about documentation and audit trails, but my actual problem is that I'm running statistical methods I don't fully understand. This doesn't help me know if my propensity score matching is methodologically sound\u2014it just gives me a prettier record of doing it wrong. The social proof testimonials feel manufactured, especially the one that's supposed to resonate with me. Also 'Receipt' in quotes in the headline is trying way too hard to be clever.",
    "objections": "This solves a different problem than the one that almost got me in trouble. I need to know if my statistical methods are correct, not just track what I did. My issue wasn't that I couldn't reproduce my ChatGPT analysis\u2014it's that the analysis itself was wrong. A changelog doesn't catch methodological errors. The 'health check' mentions broken references and formatting but says nothing about statistical validity. I'd still need that biostatistician to review my work.",
    "dealbreaker": true,
    "dealbreaker_reason": "It doesn't address methodological correctness at all\u2014just documentation and reproducibility. I can already reproduce my flawed analyses. What I need is confidence that the statistical approach itself is right, and this product explicitly says it doesn't make decisions about my data, which means it's not checking my methods either.",
    "gut_reaction": "This reads like it was built for people inheriting Excel hell from departed coworkers, not for people like me who are in over their heads methodologically. The audit trail is nice but secondary to my actual fear of publishing incorrect research.",
    "unanswered_questions": "Does it validate statistical methods or just track changes? Can it flag when I'm using the wrong test or violating assumptions? Does it understand the difference between technically correct code and methodologically appropriate analysis? What happens when I upload R code that runs perfectly but uses the wrong model for my data structure?",
    "price_reaction": "$29/month is reasonable but I wouldn't pay it for something that doesn't solve my core problem. The institutional tier at $2,500/year suggests they're targeting research teams who need audit compliance, but those teams probably already have statisticians who catch the errors this tool wouldn't."
  },
  {
    "person_id": 35,
    "bucket": "ai_loop_prisoner",
    "resonance": "disagree",
    "clarity_response": "It's supposed to track changes and create documentation for messy data projects across different file types and languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole thing feels like it was written by someone who's never actually inherited a broken R script from a consultant who left three years ago. 'Upload a zip file and we'll make sense of it' \u2014 really? My stuff barely makes sense to ME. Also 'Drop in your code and data' appears twice as different steps which is confusing. The receipts/trail metaphor in the headline is trying too hard and doesn't land.",
    "objections": "I don't understand what this actually does that Git doesn't do, or what the 'visual flow' looks like, or how it's going to parse my messy R code that I can barely parse myself. The testimonials sound fake \u2014 too perfectly aligned with the product's talking points. And if I'm being honest, this sounds like another thing that promises to solve my workflow but actually just adds another layer I have to manage.",
    "dealbreaker": true,
    "dealbreaker_reason": "$29/month is too much when I don't even know if this will work with my actual setup. I need to see it work on MY files before I pay anything. Free trial sounds good until I realize I have to invest time learning yet another system that might not solve my actual problem, which is that I don't understand the statistics well enough, not that I need better version control.",
    "gut_reaction": "This is selling me documentation and audit trails when my real problem is that I'm stuck in a loop with ChatGPT because I don't actually understand what I'm doing. Better tracking of my confusion doesn't fix the confusion.",
    "unanswered_questions": "What does the interface actually look like? How is this different from Git? Will it actually help me understand the CODE or just track it? Does it explain what my inherited scripts are doing or just show me when they changed? Can it handle the fact that half my analysis lives in Excel and half in R scripts that call each other?",
    "price_reaction": "The institutional tier is way too expensive for my nonprofit unless I can prove ROI, which I can't without trying it first. But $29/month adds up when I'm not sure this solves my core problem. The team tier makes no sense \u2014 I have one other person on my team, why would we pay $149?"
  },
  {
    "person_id": 36,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It tracks and documents changes across messy multi-language data projects so you can audit what happened and hand them off without institutional knowledge loss.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "That institutional knowledge is the risk.",
    "what_feels_off": "The 'Receipt' headline is trying way too hard to be clever and just confuses things. Also 'Drop in your code and data' appears twice as different steps which makes me think you don't actually know how this works. The social proof quotes sound plausible but the third one is suspiciously on-the-nose for my exact situation.",
    "objections": "I have no idea if this actually works with our specific nightmare scenario of R scripts calling Stata .do files that reference Excel lookups. 'Mixed languages' is vague marketing speak. Also, my team already lives in GitHub for some projects and I don't know if this replaces that, duplicates it, or integrates with it. And honestly, the ChatGPT problem isn't really about auditability \u2014 it's about whether the code is correct in the first place.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is closer to understanding my actual problem than most tools I've seen, but I'm skeptical it can handle the Frankensteinian reality of our codebase. The institutional tier price is reasonable if it works, but I'd need to see it handle a real example first.",
    "unanswered_questions": "Does this integrate with Git or replace it? What does 'visual flow' actually look like with 50+ interconnected files? Can it really parse Stata? What happens to our data security \u2014 does anything get uploaded to your servers? How does this help with the AI-generated code validation problem beyond just logging what we did?",
    "price_reaction": "Individual tier is irrelevant to me. Team tier seems low for 10 people but we'd probably need institutional anyway. $2,500/year for unlimited users is actually very reasonable for a research org if this does what it claims, but 'academic licensing' is too vague \u2014 does that mean discounted or just 'we'll let universities buy this'?"
  },
  {
    "person_id": 37,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for data analysis files across multiple languages and tools.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "Tell me how my researchers' data is stored, who has access to it, and whether I'll have to fight with IT to get this approved \u2014 then we can talk pricing.",
    "what_feels_off": "The entire page reads like it was written for data analysts doing their own ad-hoc work, not research operations managing qualitative user research. The testimonials are all about Excel formulas and scripts\u2014I use Dovetail and Qualtrics, not MATLAB. The 'social proof' section has zero mention of research participants, PII, or consent management. This feels like a product pivot that hasn't found its actual audience yet.",
    "objections": "Where is the data stored? What are the security certifications? Is this SOC 2 compliant? COPPA compliant? Can I export everything if we cancel? Who at CleanSheet can access our uploaded files? What happens to participant data if we're uploading research transcripts or survey results? The compliance section is completely missing and that's the first conversation I have to have with IT.",
    "dealbreaker": true,
    "dealbreaker_reason": "Zero mention of data security, compliance certifications, or where data is hosted. I can't even bring this to IT without that information, and I'm not uploading research data containing participant information to a tool that doesn't explicitly address COPPA, FERPA, or basic SOC 2 compliance. The institutional tier mentions 'compliant audit exports' but doesn't say compliant with what.",
    "gut_reaction": "This is solving a problem I don't have. My team doesn't inherit broken Python scripts\u2014we conduct user interviews and analyze qualitative data. The lack of any security or compliance information is disqualifying before I even look at price.",
    "unanswered_questions": "Where is data stored geographically? What certifications do you have? Can you sign a DPA? Do you have a subprocessor list? How is data encrypted at rest and in transit? What's your data retention policy? Can we self-host? Who on your team has access to customer data? What happens to our data if you get acquired or shut down?",
    "price_reaction": "The institutional tier at $2,500/year for unlimited users actually seems reasonable compared to other tools I've evaluated, but I can't even consider it without knowing if it meets our compliance requirements. The jump from $149/month for 10 users to $2,500/year for unlimited feels arbitrary\u2014what if I have 12 users? Also, my team is 4 people, so I'd be paying $149/month for capacity I don't need, and individual licenses don't make sense for collaborative research work."
  },
  {
    "person_id": 38,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for mixed data analysis projects involving spreadsheets and scripts.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The hero headline is trying too hard to be clever and just confuses the value prop. 'Every change has a Receipt' doesn't tell me what you do. The whole thing reads like it was written by a content marketer who's never actually untangled a legacy codebase. You say you parse Python, R, MATLAB, and SQL but don't explain HOW \u2014 static analysis? AST parsing? What about environment dependencies? And 'CleanSheet scans for inconsistencies' is hand-wavy marketing speak.",
    "objections": "Zero technical detail about how this actually works. What model are you using for the code parsing? Where does my data go \u2014 is it uploaded to your servers or processed locally? Can I self-host? What's the data residency policy? You mention 'compliant audit exports' but compliant with what standard? And the biggest one: how do you handle execution environments? If I upload a Python script with 47 dependencies, does your tool actually RUN it or just parse it? Because if it's the latter, you're just a fancy file browser.",
    "dealbreaker": true,
    "dealbreaker_reason": "No architecture details, no API documentation mentioned, no data residency policy, no explanation of what happens to my proprietary code and data when I upload it. I can't put this in front of procurement without answers to basic infosec and technical architecture questions. This reads like a landing page that wants my credit card before telling me what's under the hood.",
    "gut_reaction": "This sounds like Git meets Jupyter meets some kind of static analysis tool, but I have no idea what the actual technical implementation is. Way too much marketing fluff, not enough substance for someone who needs to evaluate whether this actually solves the problem or just creates a new dependency.",
    "unanswered_questions": "What's the technical architecture? Where does data go? Can I self-host or get an on-prem version? What models/engines power the code parsing? How do you handle execution vs. static analysis? What about dependency management? API access? Integration with our existing dbt/Snowflake stack? Is this SOC 2 compliant? HIPAA? What's your SLA?",
    "price_reaction": "Pricing is actually reasonable if the product does what it claims, but Institutional at $2500/year for unlimited users seems artificially low for an enterprise tool handling sensitive healthcare data \u2014 makes me wonder about the business model or whether you're planning to jack up prices later. I'd expect to pay more for something mission-critical with proper compliance."
  },
  {
    "person_id": 39,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for spreadsheets and analytical code that tracks changes and maps dependencies.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "too_expensive",
    "strongest_line": "Our system is messy, but we know how it's broken.",
    "what_feels_off": "The whole thing reads like AI-generated copy trying to sound human. 'Every change has a Receipt' with a capital R is trying too hard. The social proof quotes are suspiciously on-message and feel manufactured. No one actually talks like that. And the healthcare analyst quote about 'twelve monthly reports' - that's oddly specific in a way that feels fake.",
    "objections": "There is absolutely nothing on this page about FERPA compliance, data governance, or where my data actually goes. Does this upload my student data to your servers? Your cloud? Is it encrypted at rest and in transit? Who has access? What are your BAA terms? The institutional tier mentions 'compliant audit exports' but compliant with what? SOX? FERPA? GDPR? This is the first question any IR office would ask and it's completely absent. Also, I have no idea what 'upload a zip file and we'll make sense of it' actually means technically.",
    "dealbreaker": true,
    "dealbreaker_reason": "Zero mention of data privacy, security, or compliance frameworks. I cannot upload files containing student data to a third-party system without understanding exactly where that data lives, who can access it, and whether you're FERPA-compliant. This is non-negotiable for higher ed institutional research. The fact that it's not addressed at all tells me you don't understand the regulatory environment your institutional tier is trying to sell into.",
    "gut_reaction": "This feels like a solution looking for a problem in the higher ed space. The copy is generic and sounds AI-written. More critically, it completely ignores the compliance requirements that govern everything I do with student data.",
    "unanswered_questions": "Where does my data go when I upload it? Is this on-premise, cloud, or hybrid? What are your security certifications? Are you FERPA-compliant? Do you sign BAAs? How does this integrate with our existing SAS environment? What does 'health check' actually scan for - is that a static analysis or does it execute code? Who owns the IP on uploaded files?",
    "price_reaction": "$2,500/year for institutional sounds cheap, which makes me more suspicious. Real enterprise software for higher ed with proper security and compliance costs way more. Either this is too good to be true or it's not actually handling the compliance piece at all and I'd be paying for something I can't legally use."
  },
  {
    "person_id": 40,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "It's version control and documentation for data analysis files across different tools and programming languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I don't need it to do everything, I need it to actually work for the range of people on my team and not just the ones who already know stats.",
    "what_feels_off": "The hero headline is unnecessarily clever and doesn't tell me what the product actually is. The 'Receipt' metaphor doesn't land. The social proof quotes feel AI-generated\u2014too perfectly on-message and vague about actual organizational context. The 'Research Director, Public Health Nonprofit' quote is suspiciously aligned to my exact use case.",
    "objections": "There's zero information about data sovereignty, security, or compliance\u2014which is exactly what killed my last two vendor evaluations. The institutional pricing seems low for what they're promising, which makes me wonder about sustainability or hidden costs. I have no idea if this works with our Qualtrics/NVivo/Stata stack or just code-based workflows. Does this require our data to live on their servers? That's a non-starter for foundation grantmaking data. Also, 'we'll make sense of it' when I upload a zip file is not reassuring\u2014I need to know HOW.",
    "dealbreaker": true,
    "dealbreaker_reason": "No mention of data residency, security certifications, or compliance frameworks. After being burned twice on contract-stage data sovereignty issues, I won't even start a trial without knowing where data lives and who has access. This is a $2,500/year institutional purchase that doesn't address the most basic procurement requirements.",
    "gut_reaction": "This solves a real problem but the copy is written for individual analysts, not institutional buyers. I need to know about security, compliance, and vendor stability before I care about features.",
    "unanswered_questions": "Where does data live? What security certifications do you have? How does this work with qualitative data tools like NVivo or survey platforms like Qualtrics? Can grantees with minimal technical skills actually use this or is it just for my technical staff? What happens if your company goes under\u2014can we export everything? Do you have existing foundation clients I can reference?",
    "price_reaction": "Institutional at $2,500/year for unlimited users is surprisingly affordable, which actually raises red flags about whether this company will be around in two years when we need support. The individual and team tiers are irrelevant to me\u2014I need enterprise-level commitments and SLAs, not consumer pricing."
  },
  {
    "person_id": 41,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation layer for mixed data analysis files, but I can't tell if it's a repository, a workflow tool, or just fancy file tracking.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is trying too hard to sound profound and means nothing. 'Every change has a Receipt' with a random capital R? That's not how I talk about my work. The whole thing reads like it was written by someone who's never actually dealt with donor reporting requirements or IRB documentation. Step 2 promises to magically parse 'Python scripts, R code, MATLAB files' and map dependencies across languages\u2014I've seen this promise before and it falls apart the moment you have actual messy legacy code. Also, those testimonials are so generic they could be about any tool.",
    "objections": "I need to know how this integrates with our actual compliance requirements\u2014can it export audit trails in formats our federal partners accept? Does it handle PII and PHI appropriately? What happens to our data\u2014is it stored on their servers? The 'health check' feature sounds like it might flag things that aren't actually problems in a research context. And critically: can I actually export my data and history if this company goes under or we need to switch tools in three years? I've been burned by vendor lock-in before.",
    "dealbreaker": true,
    "dealbreaker_reason": "No mention of data security, compliance standards, or what happens to our proprietary methodologies when we upload them. I report to a board and manage federal grant money\u2014I can't even demo a tool that doesn't lead with SOC 2, data residency, and export capabilities. The institutional tier is cheaper than our current stack, which makes me more suspicious, not less.",
    "gut_reaction": "This sounds like someone built version control for spreadsheets and is trying to sell it to researchers without understanding what research compliance actually requires. I've sat through three demos in the last month that promised to solve handoff problems and none of them could handle our actual documentation needs.",
    "unanswered_questions": "Where is our data stored? What are the actual compliance certifications? Can we get BAAs for HIPAA if needed? What formats do audit exports come in? How does this interface with Qualtrics data or SPSS syntax files specifically? What happens if we need to leave\u2014can we get our full project history in a usable format? Who are the actual institutional clients and can I talk to one?",
    "price_reaction": "The institutional price is suspiciously low compared to our current spend, which suggests either they're underpricing to get traction or the product doesn't do what I actually need. If it genuinely solved our reproducibility and audit trail problems across SPSS, Qualtrics, and Dedoose, I'd expect to pay more. The individual and team tiers are irrelevant to me\u2014I need the institutional, but I need proof it works in a regulated research environment first."
  },
  {
    "person_id": 42,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It's some kind of version control and documentation layer that sits on top of existing data files and scripts across multiple tools.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Every automated suggestion requires your sign-off before anything changes. You keep full methodological control \u2014 CleanSheet handles the documentation and logging so you don't have to.",
    "what_feels_off": "The whole thing is vague about what actually happens to my data \u2014 where does it go when I upload it, how is it stored, who has access, is it encrypted at rest and in transit? The 'compliant audit exports' line is meaningless without saying compliant with what. The social proof quote from a 'Research Director, Public Health Nonprofit' sounds made up and doesn't address federal contract requirements at all. 'Made sense of it' and 'visual flow' are marketing fluff \u2014 I need technical specs.",
    "objections": "Zero information about data security, no mention of FISMA compliance or FedRAMP certification, no detail on how this integrates with our existing secure environments, no information about where data is processed or stored, no SOC 2 attestation mentioned, and absolutely nothing about how this would hold up in a federal audit beyond generic claims. The institutional tier mentions 'compliant audit exports' but compliant with what standard? Also no case studies from anyone doing CDC or NIH work.",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't upload federal contract data to a third-party tool without documented security compliance, data processing agreements, and clear audit trails that meet federal standards. This page doesn't even acknowledge those requirements exist. I'd need to see FedRAMP authorization or at minimum a detailed security whitepaper before I could even have a conversation with procurement.",
    "gut_reaction": "This reads like a solution for startups with messy spreadsheets, not for organizations handling sensitive federal contract work. The compliance angle is mentioned but never actually addressed with specifics.",
    "unanswered_questions": "Where is data stored and processed? What security certifications do you have? How does this meet federal data handling requirements? What's the data processing agreement look like? Can this run in our secure environment or does everything go to your servers? How do your audit logs map to federal documentation standards? What happens to our data if we cancel?",
    "price_reaction": "Institutional pricing at $2,500/year for unlimited users is actually reasonable if the product did what we need, but that's irrelevant when the fundamental compliance questions aren't answered. I'd pay five times that for a tool that could actually handle federal contract requirements with proper documentation."
  },
  {
    "person_id": 43,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for data analysis files that tracks changes across Excel, code, and spreadsheets.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is trying too hard to be clever and I have no idea what a 'Receipt' means in this context. The copy assumes I have a technical team working in Python, R, and MATLAB when my actual problem is non-technical program managers who need simple dashboards. The testimonials don't match my use case at all\u2014no nonprofits doing participant tracking or outcome reporting, just generic 'data analysts.' The whole thing feels like it's solving a software engineering problem, not a program evaluation problem.",
    "objections": "This doesn't integrate with Salesforce where our actual participant data lives. My evaluation coordinator isn't writing code in multiple languages\u2014she's pulling reports and building dashboards. My program managers don't need to see 'code logic flows,' they need to understand if their job placement rates went up or down. I have no idea if this would help them stop calling her with questions. The 'health check' for Excel files might be useful but I can't tell if that's the main product or just step one. Also, who are the ten users on a Team plan? I have twelve program managers who need read access, not ten people doing analysis.",
    "dealbreaker": true,
    "dealbreaker_reason": "No Salesforce integration mentioned and the entire product seems built for technical teams doing code-based analysis, not nonprofit program staff doing outcome reporting from a CRM. I'd be paying $2,500/year for something my evaluation coordinator might use but my program managers still wouldn't understand or trust.",
    "gut_reaction": "This feels like it was built for tech companies with data science teams, not nonprofits doing grant reporting. The language about 'legacy codebases' and 'mixed-language scripts' tells me this isn't for us.",
    "unanswered_questions": "Does this connect to Salesforce? Can non-technical staff actually create their own simple reports without calling the evaluation coordinator? What does 'unlimited users' mean for read-only access versus editing? Is there any nonprofit-specific outcome reporting or is this just generic data documentation? How does this compare to just getting better at Tableau, which we already paid for?",
    "price_reaction": "$2,500/year for unlimited users sounds reasonable on paper but only if my whole team would actually use it, which seems unlikely based on this copy. The Team plan at $149/month for only 10 users doesn't work when I have 12 program managers plus the evaluation coordinator. The pricing structure tells me they don't understand how nonprofit program teams are structured."
  },
  {
    "person_id": 44,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It logs changes to spreadsheets and scripts so you can see version history and what broke when someone leaves.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The whole tone is trying too hard to sound profound with the receipt metaphor and permanent memory language. The health check and automated suggestions sound like black box magic that will absolutely generate support tickets. The testimonials are suspiciously vague about what actually broke or how long setup really took.",
    "objections": "This sounds like it will create more problems than it solves. My department chairs need read-only dashboards with clean data, not another place to upload their messy Excel files. I already have version control through our file server and SharePoint. The institutional tier is $2,500/year for unlimited users but gives me zero confidence about data governance, security compliance, or how this integrates with our existing SAS and Tableau infrastructure. Will this even work with our SSO? What about FERPA compliance? And frankly, if I tell a department chair to upload their files to some cloud service for a health check, I'm the one fielding the IT security questions.",
    "dealbreaker": true,
    "dealbreaker_reason": "No mention of data security, compliance standards, or how this actually reduces my support burden instead of becoming another system I have to maintain and troubleshoot. This feels like a solution looking for a problem I don't have.",
    "gut_reaction": "This reads like someone built Git for Excel users but wrapped it in corporate therapy language. I need tools that keep non-technical users OUT of the weeds, not tools that promise to make their weeds more organized.",
    "unanswered_questions": "How does this actually integrate with institutional systems? What are the security and compliance certifications? Does this require cloud upload of sensitive student data? How does this reduce my support tickets instead of creating new ones when people can't figure out the visual flow maps? What happens when someone uploads a 50MB Excel file with PII in it?",
    "price_reaction": "$2,500/year sounds cheap until I realize I'd need to get IT Security involved, train users, monitor adoption, and probably still answer questions about why their broken Excel file is still broken. The individual tier is irrelevant to me, and team tier doesn't scale. For institutional, I need to see ROI in reduced ticket volume, and nothing here proves that."
  },
  {
    "person_id": 45,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It tracks changes to spreadsheets and code files so you can see who did what and go back to old versions.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Our system is messy, but we know how it's broken.",
    "what_feels_off": "This reads like it was written for tech startups, not organizations like mine. The testimonials are suspiciously vague\u2014no actual names, just job titles. The whole 'mixed-language legacy codebases' angle doesn't apply to me at all\u2014we use Excel and Word, not Python and MATLAB. It feels like they're trying to sound enterprise-ready by listing every programming language but missing the actual pain point for non-technical teams.",
    "objections": "I still don't know if this actually integrates with our Salesforce instance or if it's just another standalone tool that creates a new silo. There's no mention of security certifications, compliance standards, or whether it meets state data handling requirements. The institutional tier says 'compliant audit exports' but compliant with what? I need to know if this will pass our IT security review, our legal review, and our data governance review. Also, who owns the data once it's uploaded? That's going to be Legal's first question.",
    "dealbreaker": true,
    "dealbreaker_reason": "No information about security, compliance, or procurement requirements. I can't even start the conversation with IT and Legal without knowing if this meets FISMA, SOC 2, or our state's data handling policies. The 'upload a zip file' language will immediately trigger security red flags. I've been burned before bringing tools to committee without this documentation\u2014it's a non-starter.",
    "gut_reaction": "This sounds like version control for spreadsheets, which might be useful, but I have no confidence it will survive our approval process. It's written for a different audience\u2014people who work in code, not grant officers managing Excel reports.",
    "unanswered_questions": "Does it integrate with Salesforce? What are the specific security and compliance certifications? Can it be hosted on-premise or in our approved cloud environment? What's the vendor's track record with government and nonprofit clients? How does procurement actually work for institutional licenses? Is there a formal RFP process or existing contract vehicle I can use?",
    "price_reaction": "The institutional price at $2,500/year is actually reasonable for unlimited users, but price isn't my problem\u2014it's getting through the approval gauntlet. The individual tier is useless to me since I can't expense personal software for work data. I need institutional buy-in, not a personal trial."
  },
  {
    "person_id": 46,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for spreadsheets and mixed-language data analysis code that creates audit trails.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is trying too hard to be clever with the receipt metaphor. The testimonials feel generic and conveniently cover exactly the objections you're trying to handle. Step 2 promises to magically parse and map dependencies across multiple languages which sounds technically implausible without major caveats. The whole thing reads like it was written by someone who hasn't actually dealt with procurement processes.",
    "objections": "No information about security protocols, data residency, FERPA compliance, or SOC 2 certification. No case studies from actual higher ed institutions. The institutional pricing seems arbitrary and there's no clarity on implementation support, training resources, or what happens when the tool misidentifies dependencies in complex legacy code. I need to know error rates, false positive rates on the health checks, and what the learning curve actually looks like for non-technical staff who inherit these projects. Also, how does this integrate with our existing IT infrastructure and who owns the data once it's uploaded?",
    "dealbreaker": true,
    "dealbreaker_reason": "No mention of compliance frameworks, security certifications, or data governance that would pass our IT and legal review. I can't even begin a pilot without that documentation. Also, $2,500/year for unlimited users is either unsustainable pricing that will change or the product isn't robust enough to handle institutional scale.",
    "gut_reaction": "This sounds like a solution looking for a problem, built by someone who hasn't navigated institutional procurement. The magic parsing claims are oversold and the compliance gaps are disqualifying.",
    "unanswered_questions": "What are the actual security certifications? Where is data stored? What's the SLA? Is there an on-premise option? What does implementation actually look like at scale? How accurate is the dependency mapping? What's your track record with higher ed? Do you have references I can actually call? What happens to our data if you go under?",
    "price_reaction": "The institutional tier is suspiciously cheap for unlimited users which makes me question sustainability and support quality. I've managed enterprise contracts and this pricing doesn't reflect the actual cost of supporting institutions properly. Either you're underpricing to get market share and will raise rates dramatically, or support will be inadequate when we need it."
  },
  {
    "person_id": 47,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It tracks changes in spreadsheets and code files so you can see what was edited and go back to previous versions.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "That institutional knowledge is the risk. When the person who built the workflow moves on, the project should stay readable.",
    "what_feels_off": "The hero headline is pretentious and unclear. The whole thing reads like it's for data engineers, not for my team who uses Excel and Qualtrics to make funder reports. The testimonials sound like they're from technical people who code, not nonprofit program staff. The 'mixed-language legacy codebases' stuff is completely irrelevant to us\u2014we don't have Python scripts sitting around.",
    "objections": "My staff doesn't write code and doesn't maintain complex workflows with broken references. They make reports in Word from Excel tables. This sounds like version control for programmers, not a solution for making our data tell a story to the board. I don't see anything about dashboards or visualizations\u2014it's all about audit trails and tracking changes. That's not my problem. My problem is static reports that bore people.",
    "dealbreaker": true,
    "dealbreaker_reason": "This doesn't solve the problem I have. I need my team to produce engaging dashboards for the board, not track formula changes in spreadsheets. The institutional tier is $2,500/year for a feature set we don't need, and nothing on this page suggests it would help us communicate impact better.",
    "gut_reaction": "This feels like a developer tool dressed up with nonprofit testimonials. It's solving a technical debt problem I don't have instead of the storytelling and visualization problem I do have.",
    "unanswered_questions": "Does this actually create dashboards or visualizations? Can it connect to Qualtrics? What does the output look like\u2014is it still a static file or something interactive? How does this help communicate impact to non-technical board members?",
    "price_reaction": "The institutional price seems steep for what amounts to fancy version control. If it actually helped us produce better board reports I could justify it, but I don't see that here. The team tier at $149/month might be palatable if I understood what we'd actually get out of it."
  },
  {
    "person_id": 48,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for mixed data analysis files that tracks changes and dependencies.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Our system is messy, but we know how it's broken.",
    "what_feels_off": "The healthcare testimonial is suspiciously vague and doesn't mention HIPAA once. The whole page talks about uploading files to 'CleanSheet' but never addresses where my data actually goes or how it's secured. That's a red flag for any healthcare data tool.",
    "objections": "Zero mention of HIPAA compliance, BAA, data residency, or security certifications. The 'compliant audit exports' line in institutional pricing is doing a lot of work without explaining what compliance means. Where does the data live? Is it cloud-based? On-premise option? My legal team would reject this in the first conversation.",
    "dealbreaker": true,
    "dealbreaker_reason": "No HIPAA compliance information whatsoever. I can't upload patient data or clinical analysis files to a tool that doesn't explicitly address healthcare data security and provide a BAA. This has happened twice before with AI tools and I'm not going through another six-month evaluation only to get shut down by legal.",
    "gut_reaction": "Interesting concept but completely ignores the compliance reality of healthcare data work. The fact that there's a healthcare testimonial but zero security or compliance details tells me they haven't actually worked through the regulatory requirements.",
    "unanswered_questions": "Where is data stored? Cloud or on-premise deployment options? HIPAA compliance and BAA availability? What certifications do you have? Can data stay within our network perimeter? What's your security architecture?",
    "price_reaction": "Institutional pricing at $2,500/year for unlimited users is actually reasonable if the tool worked, but 'compliant audit exports' isn't the same as HIPAA-compliant infrastructure. That wording makes me think they're not actually compliant but trying to sound like they could be."
  },
  {
    "person_id": 49,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for multi-language data analysis projects that tracks changes and maps dependencies.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The health check that 'flags the patterns that actually matter' is completely vague\u2014what patterns? What's the criteria? The whole thing reads like it's solving version control, which already exists. Also 'maps dependencies between files' and 'organizes the logic into a connected visual flow'\u2014how? What's the underlying method? This is exactly the kind of non-answer that made me leave that demo.",
    "objections": "I have no idea what algorithm it uses to parse mixed-language code, how it handles statistical validity when flagging 'inconsistencies,' or what 'broken references' means in the context of an R script. Does it understand the statistical logic or just the syntax? Can it differentiate between a legitimate analytical choice and an error? The objection handling says 'every automated suggestion requires your sign-off'\u2014so it does make suggestions about my data. What suggestions? Based on what rules?",
    "dealbreaker": true,
    "dealbreaker_reason": "This is a black box. I need reproducible research tools that I can interrogate and validate. If I can't see the methodology behind the 'health check' or understand how it's parsing my analysis logic, I can't trust it in my workflow. Version control I already have with Git. What I don't have is transparency about what this actually does under the hood.",
    "gut_reaction": "This feels like Git meets linter meets something proprietary that I can't validate. The copy is polished but dodges every technical question that would actually matter to someone doing rigorous analysis work.",
    "unanswered_questions": "What is the actual parsing algorithm? How does it handle statistical methods\u2014does it recognize them or just treat them as code? What validation has been done on the dependency mapping? Can I export the full lineage in a format I can audit independently? What does 'health check' actually check for, methodologically? Is this just syntax checking or does it understand analytical logic?",
    "price_reaction": "The institutional tier at $2,500/year for unlimited users is actually reasonable for academic settings if it worked, but I'd need to see published validation studies and open documentation of the methods before I'd recommend it to a grant committee."
  },
  {
    "person_id": 50,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for mixed data analysis workflows that logs changes and maps dependencies across files.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "too_expensive",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole pitch feels like it's solving project management problems, not methodological ones. The 'health check' that scans for 'inconsistencies' and 'patterns that actually matter' is incredibly vague\u2014what patterns? How does it know what matters in my specific analysis context? This sounds like generic linting. The claim about parsing multi-language codebases and creating 'connected visual flows' is technically hard and the copy doesn't explain how it actually works or what the limitations are. Also, 'auditable history' already exists\u2014it's called Git.",
    "objections": "I already use version control for my code. The value prop here seems to be for people who don't know how to use Git or write reproducible code, not for someone doing serious methodological work. The 'health check' feature sounds like it would flag things that aren't actually problems in my analysis design and miss subtle methodological issues. No evidence this understands statistical code semantically\u2014just that it can parse syntax. The testimonials are from analysts and operations people, not methodologists. Where are the researchers?",
    "dealbreaker": true,
    "dealbreaker_reason": "This is a project management and documentation tool marketed as if it solves methodological rigor problems. It doesn't understand statistical methodology\u2014it just tracks changes and visualizes file dependencies. I need tools that understand mixed models, causal inference, measurement error. This is solving the wrong problem for my work, and $29/month for what Git already does better is not worth it.",
    "gut_reaction": "This is Git with a GUI and some file parsing, marketed to people who don't know version control exists. The multi-language dependency mapping might be interesting but I'd need to see exactly how it handles something like sourcing R functions from Python via reticulate before I believe it works.",
    "unanswered_questions": "How does the 'health check' actually work? What does it flag and why? Does it understand statistical code semantically or just syntactically? Can it distinguish between intentional design choices and actual errors? What are the limitations of the dependency mapping\u2014does it handle dynamic file paths, conditional sourcing, parameterized inputs? How does this integrate with existing version control if I'm already using Git? What does 'compliant audit exports' actually mean for academic publishing?",
    "price_reaction": "Twenty-nine dollars a month for an individual license is steep for what appears to be a documentation wrapper. I can get unlimited private repos on GitHub for free and write my own analysis documentation. The institutional tier at $2,500/year is competing with site licenses for actual statistical software, which seems optimistic for a tool that doesn't do any analysis itself."
  },
  {
    "person_id": 51,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "A version control and documentation layer for mixed analysis files that claims to track changes and map dependencies across different tools.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Every automated suggestion requires your sign-off before anything changes. You keep full methodological control \u2014 CleanSheet handles the documentation and logging so you don't have to.",
    "what_feels_off": "The health check and dependency mapping claims feel like magic promises without technical detail. How does it parse R code versus Python versus Excel formulas and map dependencies between them? That's non-trivial and the copy glosses over it like it's solved. The social proof quotes are generic and don't address my actual workflow with survival analyses or mixed models. The AI validation quote is close but vague.",
    "objections": "I need to know HOW it tracks statistical decisions, not just file changes. Does it capture my modeling assumptions? Can I export the audit trail in a format reviewers and journals will accept? What happens to my data\u2014does it leave my system? Is it HIPAA compliant for clinical data? The copy talks about Excel formulas breaking but not about statistical methodology decisions, which is what I actually need documented.",
    "dealbreaker": true,
    "dealbreaker_reason": "No mention of regulatory compliance, data security, or what specific statistical operations it can actually document. I work with protected health information and need SOC2 or HIPAA compliance guarantees before I can even trial this. Also, if it can't specifically track the statistical decisions in my R scripts\u2014model specifications, transformations, multiple testing corrections\u2014then it's just Git with a pretty interface, which I already have.",
    "gut_reaction": "This reads like version control for people who don't use version control, but I already use Git and documented R Markdown workflows. I need audit capability at the statistical methods level, not the file level.",
    "unanswered_questions": "What does the audit trail actually contain for statistical analyses? Is this HIPAA/FISMA compliant? Does data get uploaded to your servers? Can I run this locally? Does it understand statistical functions in R or just track that the file changed? What format are audit exports in\u2014can I submit them with IRB documentation?",
    "price_reaction": "Institutional pricing at $2,500/year is reasonable if it actually solves compliance and audit documentation for clinical research, but I'd need to see proof it meets regulatory requirements first. Individual pricing is irrelevant since I can't use this without institutional vetting anyway."
  },
  {
    "person_id": 52,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "A version control and documentation layer that sits on top of mixed-format data analysis workflows to create audit trails.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I maintain twelve monthly reports that were built by three different people before me. Now when something breaks, I can see exactly which formula changed and when \u2014 instead of spending two days hunting through files.",
    "what_feels_off": "The technical claims are vague to the point of unbelievability \u2014 parsing multi-language legacy codebases and automatically mapping dependencies is an extraordinarily hard problem, and this copy treats it like a solved commodity feature. The 'health check' that flags 'patterns that actually matter' is meaningless without explanation of the methodology. The institutional pricing testimonial reads like it was written by marketing, not a research director.",
    "objections": "I have no idea what the parsing engine actually does or how it determines what's meaningful versus noise. Does it execute code or just do static analysis? How does it handle custom functions, external dependencies, or database connections? The Step 2 promise sounds like AGI-level code comprehension wrapped in hand-wavy language. I would never recommend a tool to my students or consulting clients without understanding its technical limitations, and this page actively avoids discussing any.",
    "dealbreaker": true,
    "dealbreaker_reason": "The core value proposition requires solving NP-hard problems in program analysis across multiple languages, but the copy provides zero technical substance about how it works, what it can't do, or where it breaks down. A tool that 'doesn't know what it doesn't know' presenting itself as a universal solution is professionally irresponsible to adopt.",
    "gut_reaction": "This reads like a product that watched the Git/Jupyter/data lineage space and decided to claim it does everything without explaining how any of it works. I'd need a 30-page technical whitepaper before I'd even consider a trial.",
    "unanswered_questions": "What is the actual technical architecture? Static analysis or execution? How does it handle code it can't parse? What are the explicit limitations by language? What does the dependency graph actually show? How is 'health check' scoring calculated? What does 'compliant audit exports' mean legally? Has this been validated in any peer-reviewed context?",
    "price_reaction": "Institutional pricing at $2,500/year is reasonable if it works as advertised, but that's a massive 'if' given the technical claims. Individual pricing is fine. The real cost would be organizational risk if the tool gives false confidence about code it doesn't actually understand."
  },
  {
    "person_id": 53,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for mixed analysis workflows across multiple file types and programming languages.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The copy promises to 'make sense of' legacy codebases and 'parse dependencies' across multiple languages with no explanation of how. That's not a trivial technical problem and the vagueness makes me assume it's doing something superficial or unreliable. The 'health check' that flags 'patterns that actually matter' is exactly the kind of black-box AI methodology I avoid. Also 'visual flow' of logic spanning Python, R, MATLAB, and Excel? I need to see what that actually looks like because that sounds like marketing fantasy.",
    "objections": "There's zero methodology transparency here. How does it parse cross-language dependencies? What does 'inconsistencies' mean algorithmically? What assumptions is the health check making? Is it using LLMs to interpret code? If so, how are you validating accuracy? The whole pitch is 'trust us to document your methodology' while providing no documentation of its own methodology. That's disqualifying.",
    "dealbreaker": true,
    "dealbreaker_reason": "I can't adopt a tool that adds an audit layer to my research without understanding what that tool is actually doing under the hood. If I can't explain and defend the methodology of the documentation system itself, it doesn't solve my reproducibility problem\u2014it adds a new black box.",
    "gut_reaction": "This sounds like git meets static analysis meets AI code interpretation, which could be useful, but they're selling the outcome without proving the capability. I'd need to see technical documentation and validation studies before I'd even trial this.",
    "unanswered_questions": "What's the actual technical approach? Is this using LLMs or rule-based parsing? How accurate is cross-language dependency mapping? Can I export the audit trail in a format that meets academic standards? What happens when the tool misinterprets code logic\u2014how would I even know?",
    "price_reaction": "The institutional tier at $2,500/year is reasonable for a research team if the tool actually works, but I'm not getting to pricing evaluation because the methodology gap is disqualifying. I wouldn't spend $29 without technical documentation."
  },
  {
    "person_id": 54,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for data analysis files across multiple tools and languages.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The whole thing reads like it's solving workflow chaos without addressing methodological rigor. The 'health check' that scans for inconsistencies sounds automatic and black-box \u2014 what criteria is it using? Who decides what patterns 'actually matter'? The objection handling says I keep 'full methodological control' but doesn't explain how the tool determines what's worth flagging. This feels like it would give my students a false sense of security that their messy analysis is now 'auditable' when really they've just documented bad methods more thoroughly.",
    "objections": "I don't see any evidence this tool teaches users to evaluate their own work critically. It automates documentation, which is fine for engineers, but my concern is students using tools that make flawed analysis look professional. The testimonials are all about efficiency and traceability, not about catching methodological errors. The 'health check' feature especially worries me \u2014 automated flags without transparent criteria just become another thing people trust without understanding. Also: zero information about what statistical or methodological standards it's checking against.",
    "dealbreaker": true,
    "dealbreaker_reason": "This would make the exact problem I'm worried about worse. My students would upload their work, get a clean audit trail and a visual flow diagram, and assume that means their analysis is sound. The tool seems designed for operational reproducibility, not methodological validity. I can't recommend something that documents bad statistics really well.",
    "gut_reaction": "This is Git for spreadsheets with some parsing magic, marketed to people drowning in technical debt. My immediate thought: this makes inherited chaos legible, but does nothing to ensure the chaos was statistically defensible in the first place.",
    "unanswered_questions": "What methodological standards does the health check use? Can I customize what gets flagged? Does it catch statistical errors or just syntax/reference problems? What happens when students use this to 'validate' fundamentally flawed analyses? Is there any pedagogical component or is this purely operational?",
    "price_reaction": "Institutional pricing at $2,500/year for unlimited users is actually reasonable if it worked for our use case, but I'd never deploy this across graduate programs without knowing exactly what the automated checks are doing and whether it teaches critical evaluation or just process compliance."
  },
  {
    "person_id": 55,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for mixed data analysis workflows across multiple languages and file types.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it.",
    "what_feels_off": "The claim that it can parse dependencies across Python, R, MATLAB, SQL, and Excel from a zip file is technically enormous and the copy glosses over it like it's trivial\u2014that's either vaporware or it's doing something much simpler than implied. The 'health check' that flags 'patterns that actually matter' is pure hand-waving without defining what statistical or methodological criteria it's using. Also 'navigable history' for analysis decisions sounds like Git for spreadsheets, which already exists in various forms.",
    "objections": "No technical documentation about what the dependency parsing actually does, no explanation of the algorithms behind the 'health check,' no discussion of how it handles statistical assumptions or methodological choices versus just tracking cell edits. The testimonials mention broken formulas and readability but nothing about whether it caught actual statistical errors. I've spent 18 months cataloguing how these tools fail on edge cases and this copy doesn't address methodology at all\u2014just file management.",
    "dealbreaker": true,
    "dealbreaker_reason": "The core claim\u2014automatically parsing and mapping dependencies across multiple languages and legacy code\u2014is either massively understated or oversold. If it works as described, show me the technical paper. If it's just tracking changes and visualizing file relationships, that's not enough to trust it with research-grade work. Without seeing how it handles non-textbook cases, this is another tool that will fail in ways I can't detect until I've already published.",
    "gut_reaction": "This reads like Git meets data cataloging meets linting, packaged for people who don't know those tools exist. The multi-language dependency parsing claim is doing a lot of unearned heavy lifting.",
    "unanswered_questions": "What algorithms power the 'health check'? How does it handle statistical assumptions embedded in code? What does 'broken reference' mean in a statistical context versus just a file path? Can it detect methodological errors or just syntax and formatting issues? What happens with Bayesian workflows, custom functions, or simulation code? Is there any validation study?",
    "price_reaction": "Institutional pricing at $2,500/year is reasonable if it actually works, but I'd need to see technical documentation and a pilot with my own messy projects before recommending it to university clients. The individual tier is cheap enough to test, but I'm not spending time on another tool that claims to understand code it probably just indexes."
  },
  {
    "person_id": 56,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for data analysis projects that works across multiple file types and languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Every automated suggestion requires your sign-off before anything changes.",
    "what_feels_off": "The entire framing assumes my problem is documentation and audit trails, when my actual concern is methodological rigor\u2014those aren't the same thing. The copy reads like it was written for compliance officers, not researchers. The phrase 'fragile, person-dependent workflows' is condescending to anyone who actually does careful work. And 'made the logic readable so I could actually sign off on it' from the testimonial is deeply concerning\u2014if you need a tool to understand code before you can approve it, you shouldn't be approving it.",
    "objections": "This doesn't address method selection or statistical validity at all\u2014it's just file management with version control, which Git already does for code. The 'health check' that scans for 'inconsistencies' is vague and probably can't catch actual methodological problems. I have no idea what 'patterns that actually matter' means. The mixed-language parsing sounds useful for inheriting messy projects, but that's not my workflow. I write documented, reproducible R code from the start.",
    "dealbreaker": true,
    "dealbreaker_reason": "This solves a project management problem, not a research methods problem. It might help someone wrangle inherited Excel hell, but it doesn't make analysis more rigorous or help with the intellectual work of choosing appropriate methods. The copy never once mentions statistical validity, assumptions, or analytical decisions\u2014just tracking changes. I can already do that with R Markdown, Git, and proper documentation practices. This is a tool for people who don't know how to structure research workflows properly in the first place.",
    "gut_reaction": "This is version control dressed up as research infrastructure. It might be useful for operations teams managing reports, but it's not built for someone who cares about statistical rigor or reproducible science.",
    "unanswered_questions": "What does the 'health check' actually detect? Can it identify statistical errors, violated assumptions, or inappropriate method choices? How does it handle R Markdown or Jupyter notebooks where code and narrative are integrated? Does it understand the difference between exploratory analysis and confirmatory testing? What happens to my intellectual property when I upload proprietary research code?",
    "price_reaction": "The institutional tier at $2,500/year for unlimited users is actually reasonable compared to other academic software, but I'd need to see evidence it does something meaningful for research quality before recommending it to my department. The individual tier is irrelevant\u2014no serious researcher is paying out of pocket for file management tools when free alternatives exist."
  },
  {
    "person_id": 57,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's a version control and documentation layer for mixed spreadsheets and code files that logs changes automatically.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Every automated suggestion requires your sign-off before anything changes. You keep full methodological control \u2014 CleanSheet handles the documentation and logging so you don't have to.",
    "what_feels_off": "The entire pitch is aimed at people with messy workflows and institutional knowledge problems, not rigorous methodology. The social proof quotes are all about broken reports and validating AI code \u2014 not one person doing actual statistical work. It reads like a tool for operations analysts who inherited Excel chaos, not survey methodologists working with complex sample designs.",
    "objections": "There's zero mention of how this handles statistical analysis workflows, survey weights, variance estimation, or any methodology I actually use. Can it parse SAS or Stata code? Does it understand survey design objects? Can it track weight adjustments and nonresponse corrections in a way that's reproducible according to federal standards? The 'health check' for spreadsheets means nothing for my work \u2014 I need to know if it can document complex sampling procedures, not find broken cell references.",
    "dealbreaker": true,
    "dealbreaker_reason": "This isn't built for survey methodology. It's for people managing reports and inherited Excel files. I work with weighted complex survey data in SAS and Stata \u2014 the copy doesn't even acknowledge that world exists. No mention of statistical software beyond 'MATLAB,' no understanding of reproducibility standards in federal research. I can't evaluate a tool that doesn't speak to my actual methodological requirements.",
    "gut_reaction": "This is a version control tool dressed up for business analysts who work in Excel and are scared of inherited code. It's not for statisticians doing survey analysis.",
    "unanswered_questions": "Does it support SAS, Stata, or SPSS? Can it parse and document survey design specifications, weight calibration, and variance estimation procedures? How does it handle proprietary data security requirements for federal studies? Can it produce audit trails that meet AAPOR or federal reproducibility standards? What does 'compliant audit exports' even mean \u2014 compliant with what?",
    "price_reaction": "The institutional tier at $2,500/year is reasonable for a federal research center budget, but I have no idea if this tool can do anything I need, so price is irrelevant. The 'academic licensing' mention is too vague \u2014 does that mean discounts or just access?"
  },
  {
    "person_id": 58,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's a version control and audit trail system for mixed data analysis files that tracks changes and dependencies.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Every automated suggestion requires your sign-off before anything changes.",
    "what_feels_off": "The whole thing reads like it was written for operations analysts dealing with Excel hell, not statisticians doing actual analysis. The social proof is all about broken formulas and grant committees, not methodological rigor. 'Makes the logic readable' for AI-generated code? That's not my use case and doesn't inspire confidence that this understands technical analysis work.",
    "objections": "This doesn't appear to understand psychometric workflows at all. There's zero indication it knows what an IRT model is, what lavaan output looks like, or how to handle the specialized R packages I actually use. It says it works with R, but so does Dropbox. Can it parse mirt objects? Does it understand simulation studies? Factor loadings? The 'health check' sounds like it's looking for broken Excel references, not statistical validity. And the testimonials are from people managing reports, not running complex analyses.",
    "dealbreaker": true,
    "dealbreaker_reason": "Nothing here demonstrates it can handle the technical depth of psychometric analysis. It's solving a project management problem, not a statistical methodology problem. I need tools that understand the substance of the work, not just track that the work happened.",
    "gut_reaction": "This is Git with some Excel auditing features dressed up as an AI solution. It might help someone inheriting a mess of budget spreadsheets, but I don't see how it addresses my actual workflow.",
    "unanswered_questions": "Can it actually parse statistical model objects or does it just see them as opaque R files? Does it understand the difference between exploratory and confirmatory factor analysis? Can it track simulation parameters and results in a meaningful way? What does 'health check' mean for statistical code versus Excel formulas?",
    "price_reaction": "The institutional tier at $2,500/year is reasonable if it worked for research teams, but the individual price doesn't matter because the product doesn't appear built for technical statistical work in the first place."
  },
  {
    "person_id": 59,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "It's version control and documentation for data analysis workflows across multiple file types and programming languages.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I was spending a day every week just validating AI-generated code. I couldn't trust it until I understood it. CleanSheet made the logic readable so I could actually sign off on it.",
    "what_feels_off": "The health check that 'scans for inconsistencies' and 'flags the patterns that actually matter' is totally vague \u2014 what patterns? How does it handle missing data, which is my actual problem? The copy promises it works with 'messy' data but never explains HOW. Also 'Upload a zip file and we'll make sense of it' sounds like magical AI handwaving.",
    "objections": "I have no idea how this tool actually handles incomplete community health datasets with inconsistent coding, missing values, and data quality issues that vary by collection site. The copy talks about tracking changes and broken formulas, but my problem isn't version control \u2014 it's methodological decisions about imputation, outlier handling, and data quality flags. Does this tool understand statistical methods or just track file changes? The 'health check' feature sounds like it might make assumptions about my data without understanding the domain context.",
    "dealbreaker": true,
    "dealbreaker_reason": "No information about how it handles missing data, data quality assessment methodology, or statistical validity. I've rejected tools before for exactly this kind of surface-level promise without methodological transparency. I need to know what assumptions the 'health check' is making about my data before I let it touch anything.",
    "gut_reaction": "This reads like version control for Excel users, not a research-grade tool. The 'health check' feature is described so vaguely I'd need to see documentation on its statistical methods before I'd trust it with real data.",
    "unanswered_questions": "What statistical methods does the health check use? How does it handle missing data patterns? Can I configure data quality rules based on my domain knowledge? What does 'inconsistencies' mean \u2014 statistical outliers, format issues, or both? Is there any validation that this maintains analytical reproducibility by established research standards?",
    "price_reaction": "The institutional tier at $2,500/year is reasonable IF it actually does what I need, but I'd need a much longer evaluation period than 14 days to test it on real community health data with all its messiness. The pricing isn't the blocker \u2014 trust in the methodology is."
  },
  {
    "person_id": 60,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "A version control and documentation system for spreadsheets and mixed-language analytical code that logs changes automatically.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "I can get a p-value from any of these tools \u2014 what I can't get is a coherent explanation of the assumptions that p-value is resting on.",
    "what_feels_off": "The health check claims to flag 'patterns that actually matter' and 'inconsistencies' but there's zero technical detail about what assumptions it's making \u2014 this is exactly the problem I have with these tools. The testimonials are conveniently vague. 'Made the logic readable' tells me nothing about whether it actually understands statistical methodology or just makes a pretty flowchart. The whole thing reads like it's solving project management problems, not methodological rigor problems.",
    "objections": "This doesn't address statistical assumptions, uncertainty quantification, or methodological documentation at all. It's a file tracker with some dependency mapping. How does it handle simulation parameters? Does it document distributional assumptions? Can it trace how missing data was handled across transformations? The 'health check' language is hand-wavy \u2014 what is it actually checking? I can't recommend this to my students or my center without knowing what it does and doesn't validate. It sounds like Git with a GUI, not a research rigor tool.",
    "dealbreaker": true,
    "dealbreaker_reason": "It doesn't appear to understand or document the statistical methodology that my work depends on. Version control without methodological transparency doesn't solve the reproducibility crisis \u2014 it just gives you a pretty timeline of decisions you still can't evaluate. I need to know what assumptions are being made, not just what changed when.",
    "gut_reaction": "This is project management software cosplaying as a research tool. Nothing here tells me it understands the difference between 'this formula changed' and 'this methodological assumption was violated.'",
    "unanswered_questions": "What does the health check actually validate? Does it understand statistical methods or just syntax? How does it handle Bayesian workflows, simulation studies, or sensitivity analyses? Can it document sampling weights, missing data approaches, or model assumptions? What happens when the 'automated suggestion' is statistically inappropriate?",
    "price_reaction": "The institutional tier at $2,500/year is reasonable for a center budget, but I wouldn't spend it on this without a very thorough pilot that proves it can handle methodologically complex workflows and not just Excel budgets."
  }
]