[
  {
    "person_id": 1,
    "bucket": "thesis_dreader",
    "resonance": "agree",
    "clarity_response": "The problem description reads like it was written about my last three months specifically, which is unsettling in a good way.",
    "intent": "agree",
    "conversion_confidence": "agree",
    "price_perception": "good_deal",
    "strongest_line": "You get an error. You paste the error back in. You wait. You're not sure if the output is right, but you don't know enough to tell.",
    "what_feels_off": "The phrase 'one click' sounds too easy \u2014 I've been burned by things that promised to be simple and weren't. I want to believe it but I'm still a little suspicious. Also 'UX designers' in the target list makes me feel like it's not really for grad students like me.",
    "objections": "If I upload my Qualtrics CSV will it actually know what to do with it, or am I still going to need to prepare the file first? What does 'handle missing data' actually mean in practice? I have a lot of missing responses.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is the first thing I've read that describes exactly what I've been doing for three months without making me feel stupid. I'm clicking the free trial.",
    "unanswered_questions": "Does it work with survey data specifically, or is it more for simple spreadsheets? Can it explain results in a way I can actually put in my thesis methodology section?",
    "price_reaction": "$10 a month is less than I've spent on snacks stress-eating while staring at Excel. The free tier means I can try it before committing, which is the only reason I'd even consider it.",
    "name": "Debra Davidson",
    "clarity_score": "wrong"
  },
  {
    "person_id": 2,
    "bucket": "thesis_dreader",
    "resonance": "agree",
    "clarity_response": "It's very clear what it does \u2014 upload data, get answers without coding \u2014 and the problem section describes my ChatGPT situation almost exactly.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language.",
    "what_feels_off": "The objection about 'can I trust the results' is addressed but the answer doesn't go far enough for me. I've been getting confident-sounding answers from ChatGPT that contradict each other \u2014 I need more reassurance than 'we show the method used.' How do I know the method is correct for my data? Also chi-square is specifically mentioned and that's literally the test I can't run, which feels like a coincidence that's almost too good.",
    "objections": "Will it tell me if my sample size is too small for the test it picks? That's what I actually need to know. What happens when my data has issues \u2014 like if I coded something wrong in my survey?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I'm cautiously hopeful but I've felt hopeful before about DataCamp and ChatGPT and been burned each time. The free tier means I'll try it but I'm not ready to trust it yet.",
    "unanswered_questions": "Does it handle cross-tabulations for categorical variables? Does it explain why a chi-square is or isn't significant in plain language I can cite?",
    "price_reaction": "Ten dollars a month is genuinely nothing on a stipend \u2014 I spend more than that on coffee during thesis panic weeks. The free tier is the right move to get me to try it.",
    "name": "Rachel Rodriguez",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 3,
    "bucket": "thesis_dreader",
    "resonance": "agree",
    "clarity_response": "Clear enough \u2014 I understand it picks the right test for you and explains the results, which is exactly the problem I have.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Click 'Compare groups' \u2014 it runs the right test (chi-square, t-test, ANOVA) and shows you what's significant.",
    "what_feels_off": "My specific problem is regression \u2014 I've been running regressions multiple ways and getting different results. The copy talks about correlation and group comparisons but doesn't explicitly mention regression. That's a gap for me. Also I've been using SPSS and the copy seems aimed at people who don't use any tool yet, rather than people like me who are using the wrong settings in a tool they partially know.",
    "objections": "Does it do regression? What kind \u2014 OLS, logistic? Does it tell you which variables to include or do I still need to figure that out myself? Will it import from SPSS or do I have to start over?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I want this to work and the price is irrelevant \u2014 free trial means I'll test it with my actual data before my next advisor meeting and see if it tells me something different than what I've been doing.",
    "unanswered_questions": "The regression question is critical. The copy implies the platform picks the right test automatically, but I need to know if it can handle the complexity of what I'm actually trying to do with voter suppression variables.",
    "price_reaction": "$10 a month is a non-issue. If it saves me from one wrong regression that makes my advisor question my whole analysis chapter, it's paid for itself many times over.",
    "name": "Troy Barnes",
    "clarity_score": "nailed_it"
  },
  {
    "person_id": 4,
    "bucket": "thesis_dreader",
    "resonance": "agree",
    "clarity_response": "I understand the value proposition but I'm not sure this is actually built for someone at my level \u2014 it reads like it's designed for people who have never touched stats software, and I'm past that.",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "My specific problem is multilevel models \u2014 HLM \u2014 and nothing in this copy suggests it can handle that. The examples given are chi-square, t-test, ANOVA, correlation. Those are intro stats. If this is another tool that handles the basics and breaks on anything real, it won't help me. 'Click correlate' as the main demo feature makes me nervous.",
    "objections": "Does it handle multilevel models? Specifically, can it specify random effects correctly? Can I import my existing R data structures or do I have to start from scratch? If my committee reviews the methodology section, will the exported audit trail be rigorous enough to defend?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "The core pitch resonates \u2014 I'm exactly the person drowning in Claude-generated R code that doesn't match my data \u2014 but I need to know it can do HLM before I get excited. The free trial is how I'll find out.",
    "unanswered_questions": "What is the actual ceiling on complexity? Does it do random effects models or is it limited to fixed-effects analysis? What does the output look like \u2014 is it in a format I can include in a dissertation?",
    "price_reaction": "Ten dollars is nothing if it works. My concern isn't price, it's capability. If it can do what my committee is asking for, I'd pay $50 a month and consider it cheap.",
    "name": "Angel Ellis",
    "clarity_score": "partial"
  },
  {
    "person_id": 5,
    "bucket": "thesis_dreader",
    "resonance": "strongly_agree",
    "clarity_response": "This is the clearest thing I've read in months and it describes my exact situation without making me feel ashamed of it.",
    "intent": "strongly_agree",
    "conversion_confidence": "agree",
    "price_perception": "good_deal",
    "strongest_line": "At this point I don't need to understand the stats, I just need something that won't embarrass me in my defense.",
    "what_feels_off": "Wait, that line isn't in the copy \u2014 that's in my head. What actually feels off: the copy says 'handle missing data with point-and-click tools' but missing data in mixed-methods research is complicated. I have SPSS data and Word-coded qualitative data and I don't know if this handles both. The copy mentions 'interview transcripts' which gives me hope for the qual side but I need to know more.",
    "objections": "Can it actually handle a mixed-methods design \u2014 quantitative SPSS data AND qualitative coded transcripts \u2014 in one project? My committee is pushing me toward R; will the output be defensible if I used this instead? Does it handle the SPSS file format directly?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I have a toddler and a dissertation and I'm exhausted and this page just told me I don't have to become a statistics expert to get through my defense. I am clicking that free trial right now.",
    "unanswered_questions": "Mixed-methods support is everything. I need to know it can handle both components of my study before I commit any time to it. Also: what does the output look like \u2014 is it something I can paste into a dissertation without a committee member flagging it as not rigorous?",
    "price_reaction": "$10 a month is a coffee and a half. I already pay for SPSS through my university and NVivo was too expensive to keep. This price point is the first thing that hasn't made me wince in six months.",
    "name": "Sandra Aguilar",
    "clarity_score": "wrong"
  },
  {
    "person_id": 6,
    "bucket": "thesis_dreader",
    "resonance": "neutral",
    "clarity_response": "The pitch is clear but it's clearly not built for someone like me \u2014 it's aimed at people who are afraid of all statistics, and my problem is much more specific.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Upload a CSV, a stack of interview transcripts, or survey exports.",
    "what_feels_off": "I use Stata fluently. The copy is written for people who don't know what a t-test is. That's condescending for someone at my level, and none of the example features \u2014 'click correlate,' 'it picks Spearman or Pearson' \u2014 address what I actually need, which is qualitative analysis of interview transcripts. The copy mentions transcripts once in the how-it-works section and otherwise doesn't engage with qualitative analysis at all. I need to know what 'qualitative analysis' means in this context specifically.",
    "objections": "What does the qualitative analysis actually do? Thematic coding? Grounded theory? Can I apply a coding scheme I've already developed or does it impose its own? How does it handle the difference between deductive and inductive coding approaches? Can I export in a format that would be publishable in an economics journal?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "The quantitative side of this is completely irrelevant to me \u2014 I have Stata \u2014 but if the interview transcript analysis is actually good, this might solve my one real problem. I'm skeptical because it doesn't talk enough about what qualitative analysis means here.",
    "unanswered_questions": "The entire qualitative analysis workflow. What does it actually do with interview transcripts? Can I define my own coding categories? Can it handle six semi-structured interviews with thematic depth rather than just frequency counts?",
    "price_reaction": "$10 a month is meaningless to me \u2014 I'd pay $100 if it worked. The price is not the issue. Whether it can do actual qualitative analysis of interview data in a way an economics reviewer would accept is the issue, and I can't tell from this page.",
    "name": "Joshua Merritt",
    "clarity_score": "partial"
  },
  {
    "person_id": 7,
    "bucket": "thesis_dreader",
    "resonance": "agree",
    "clarity_response": "The page is clear enough that I immediately understood what it does, which is more than I can say for anything else I've tried.",
    "intent": "agree",
    "conversion_confidence": "agree",
    "price_perception": "good_deal",
    "strongest_line": "Upload your interviews, surveys, or spreadsheets. Click to analyze. See what your data actually means \u2014 without writing a single line of code.",
    "what_feels_off": "The page says 'click to analyze' like it's magic, but I've been burned before by things that seem simple and then fall apart when I actually try them. I have 40 real interviews that I coded in a spreadsheet, not clean formatted data \u2014 I need to know it handles that mess, not just 'CSV uploads.' Also the inter-rater reliability mention in the Team tier is interesting but buried \u2014 that's actually directly relevant to my thematic analysis.",
    "objections": "My skepticism is low so I'm not raising many technical objections, but I am worried: will this actually work on qualitative data from 40 clinical psychology interviews? The copy mentions 'thematic analysis' indirectly in the 'who it's for' section but doesn't explain how the tool handles it. Does it code themes for me, or does it just organize what I've already coded? That distinction matters a lot.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Honestly this is the first thing I've read in months that sounds like it was written for someone like me. The free tier with three projects means I can at least try it without feeling guilty about spending money I don't have.",
    "unanswered_questions": "Can it actually handle thematic analysis of qualitative interview data, or is it mainly for quantitative/survey data? What does the 'audit trail' look like \u2014 is it something I could show my advisor? Does it work on data I've already partially coded?",
    "price_reaction": "Ten dollars a month is absolutely within reach and the free tier makes this feel safe to try. This is the least stressful price I've seen on anything research-related.",
    "name": "Brooke Bass",
    "clarity_score": "partial"
  },
  {
    "person_id": 8,
    "bucket": "thesis_dreader",
    "resonance": "agree",
    "clarity_response": "The page is straightforward and I understood exactly what problem it's solving within the first paragraph.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "The page says the platform 'detects your data types and guides you to the right analysis' \u2014 that sounds great in theory, but my SPSS data structure is a specific headache. I need to know it handles SPSS exports or at least that I can get my data out of SPSS into something this tool accepts without losing my variable labels and value codes. The 'How It Works' section skips over the cleaning step too quickly \u2014 that's where I always get stuck.",
    "objections": "My advisor requires SPSS. If I run the analysis somewhere else, I need to be able to explain that in my dissertation methods section. Can I actually cite this tool in an academic paper? The 'audit trail' mention is encouraging but I need more specifics \u2014 does it produce something my committee would accept as documentation of method?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "As someone who keeps having to restart analysis from scratch in SPSS, the idea of a platform that just handles the messy data management piece sounds like actual relief. I'm cautiously hopeful but I've been cautiously hopeful before.",
    "unanswered_questions": "Does it accept SPSS .sav files, or do I need to export first? What does the audit trail actually include \u2014 enough to satisfy a dissertation committee? Can I add new survey responses without redoing everything from scratch?",
    "price_reaction": "The price is not the problem at all \u2014 $10 a month is nothing compared to the hours I lose restarting analyses. I'm more concerned about whether it actually works than what it costs.",
    "name": "Lauren Shaw",
    "clarity_score": "partial"
  },
  {
    "person_id": 9,
    "bucket": "thesis_dreader",
    "resonance": "neutral",
    "clarity_response": "I understood what it does, but I'm not sure it understands what I do \u2014 quantifying newspaper archives is not the same thing as running a survey.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "You're not alone. The tools that exist today \u2014 NVivo, Atlas.ti, SPSS \u2014 were built for methodologists with institutional budgets and years of training. Not for you.",
    "what_feels_off": "The copy is written for people doing survey data and interviews, and I get that, but I'm doing content analysis of 300 historical newspaper articles. The language around 'correlations,' 'chi-square,' 't-tests' \u2014 I don't know if those are even the right things for my project and this page doesn't help me figure that out. The framing of 'click Compare groups' assumes I know what I'm comparing, which is the exact problem I don't have solved yet.",
    "objections": "I'm philosophically skeptical of quantifying historical documents \u2014 that's a real methodological debate in my field, not just me being difficult. If this tool just pushes me toward numbers without helping me think through whether quantification is appropriate, it's not actually helping me. Also, my coding scheme 'fell apart' in Excel \u2014 what happens when I upload data that isn't clean or consistently coded?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "The problem section resonates because I do feel like I don't belong in this statistical world my committee is pushing me toward. But I'm not convinced this tool is built for my specific use case \u2014 content analysis of archival documents is different from survey analysis and the page doesn't acknowledge that.",
    "unanswered_questions": "Can this handle content analysis, not just survey data? What counts as 'the right analysis' when my data is a coding scheme I applied to newspaper articles? How does it handle data where I have 15 different categories I coded inconsistently across 300 documents?",
    "price_reaction": "Ten dollars a month is within my fellowship stipend if it actually solves my problem. The free tier makes it low-risk to find out if it doesn't.",
    "name": "Daniel Fisher",
    "clarity_score": "partial"
  },
  {
    "person_id": 10,
    "bucket": "thesis_dreader",
    "resonance": "neutral",
    "clarity_response": "The page is clear but it's clearly aimed at people with less data experience than I have, and I need to know if it can handle something as specific as structural equation modeling.",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "Click \"Correlate\" \u2014 it picks Spearman or Pearson based on your data. You don't need to know which.",
    "what_feels_off": "I've been building dashboards for a city government, I know what a pivot table is, so the 'you don't need to know anything' angle is slightly condescending for where I'm at. The copy positions this as a solution for people who have no data background, but my problem is specifically that I need SEM \u2014 structural equation modeling \u2014 which is conspicuously absent from the list of things this tool advertises. Chi-square, t-test, ANOVA, Pearson \u2014 those are undergraduate-level analyses. Where's the advanced stuff?",
    "objections": "The copy lists the basic statistical tests this tool automates. My committee is asking for SEM. Those are not in the same category. If this can handle SEM I need to know that now, if it can't then this isn't a solution to my actual problem even if it's a nice interface. The 'right test' framing implies the tool knows what I need \u2014 but does it know enough for doctoral-level work?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I came in from LinkedIn already annoyed that academia keeps moving the goalposts on me, and this page is pleasant but doesn't answer whether it can do what I actually need. It might be great for my cohort members who are just starting out, but I need to know if it can handle graduate-level modeling before I invest time testing it.",
    "unanswered_questions": "Does it support structural equation modeling, or any advanced modeling beyond regression? What's the ceiling on analytical complexity? If I've already done descriptives in SPSS, can I import that work or start fresh?",
    "price_reaction": "The pricing is completely reasonable \u2014 price isn't the filter here. The question is whether it does what I need at all, and the free tier lets me find out without financial risk.",
    "name": "Jeffery Thompson",
    "clarity_score": "partial"
  },
  {
    "person_id": 11,
    "bucket": "thesis_dreader",
    "resonance": "strongly_agree",
    "clarity_response": "This page spoke directly to the exact situation I'm in \u2014 the fragmented tools, the guilt, the data sitting in folders I'm scared to open.",
    "intent": "strongly_agree",
    "conversion_confidence": "agree",
    "price_perception": "fair",
    "strongest_line": "You get an error. You paste the error back in. You wait. You're not sure if the output is right, but you don't know enough to tell.",
    "what_feels_off": "The free tier is 3 projects and I have a mixed-methods dissertation \u2014 surveys plus focus group transcripts. That's probably already two projects if I split them the way I'd need to. The Team tier mentions inter-rater reliability which is relevant to my focus group coding but it costs $25/month and I'm a full-time case manager trying to finish a dissertation on evenings and weekends. Also the page doesn't say anything about what happens to my data \u2014 after three years of being ABD, I'm paranoid about anything that could compromise my participants' confidentiality.",
    "objections": "I found Dedoose genuinely useful before I couldn't afford it anymore, so I know what good qualitative tools look like. I need to understand if this handles qual-quant integration in a way that makes my mixed-methods analysis coherent, or if it's really two separate workflows. And I need to know about data privacy before I upload focus group transcripts from vulnerable populations.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I felt called out in the best possible way reading the problem section \u2014 that's my life described accurately. The guilt about the transcripts sitting in folders is real. I want this to work badly enough that I'm going to try the free tier tonight if I can get the kids to bed.",
    "unanswered_questions": "What are the data privacy and security practices \u2014 who can see my uploaded transcripts? Does the platform handle mixed-methods integration or is it qual and quant separately? What counts as a 'project' in the free tier \u2014 could my entire dissertation be one project?",
    "price_reaction": "Ten dollars a month is manageable but honestly tight on my current budget. Three years ABD means I've had a lot of unanticipated expenses. I'd commit to Pro if the free tier proves it out \u2014 I just need to see it work first.",
    "name": "Courtney Morgan",
    "clarity_score": "wrong"
  },
  {
    "person_id": 12,
    "bucket": "thesis_dreader",
    "resonance": "neutral",
    "clarity_response": "The page is clear and well-written, but it's pitching me on problems I've already partially solved \u2014 my issue is more specific than 'no code required.'",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "I have actual Python skills and real UX research experience \u2014 the 'without writing a single line of code' pitch isn't my selling point, it might even be a mild red flag. My problem is that statistical analysis in Python keeps breaking, not that I can't code. I need to know if this tool is rigorous enough for a technically demanding Information Science department, or if it's really built for people with no technical background. The copy reads like it's for the latter and I'm somewhere in between.",
    "objections": "My department skews computational. If I use a point-and-click analysis tool for my dissertation, I need to be able to defend that methodologically. The 'audit trail' mention is the one thing that makes this credible to me \u2014 but I need more specifics. Does it output something I can put in a methods section of an academic paper? Does it show me what statistical assumptions it's checking? Can I export results in a format that integrates with a Python or R workflow I might want to run in parallel?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I have real imposter syndrome about code and this tool would let me sidestep the Python statistical analysis piece that keeps breaking \u2014 but I'm also aware that using a no-code tool in a computational department might just shift the imposter syndrome somewhere else. The audit trail claim is what I'd need to verify before committing.",
    "unanswered_questions": "What does the audit trail actually include \u2014 is it detailed enough to put in a dissertation methods section? What statistical assumptions does it check and report? Can I import data I've already cleaned in Python? Is this tool used by anyone doing publication-quality research?",
    "price_reaction": "Completely fine \u2014 price is not the issue here at all. My concern is whether this is academically credible enough for my context, not whether it's affordable.",
    "name": "Taylor Wong",
    "clarity_score": "partial"
  },
  {
    "person_id": 13,
    "bucket": "duct_tape_analyst",
    "resonance": "neutral",
    "clarity_response": "The workflow description is clear enough, but it glosses over the specific integration pain I actually have \u2014 moving data between five separate tools is the problem, and this page doesn't tell me if it solves that or just adds a sixth.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "The copy targets people who can't do analysis at all. I can do the analysis \u2014 my problem is the reformatting, the manual steps between tools, the six hours I lose reformatting the same spreadsheet. Nothing here says it fixes that. 'One place' sounds nice but I've heard that before from tools that don't connect to anything I already use.",
    "objections": "Does it actually replace any of my current tools or just sit on top of them? Can it ingest Google Forms exports directly? Will I still need Excel in the middle? The problem isn't that I can't click 'Correlate' \u2014 I can do that in Excel. The problem is the connective tissue between steps, and this page doesn't address that.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Looks like it's aimed at someone less capable than me, which makes me wonder if it would actually handle my real workload. The price is low enough that I might try the free tier, but I'm not expecting it to fix my actual problem.",
    "unanswered_questions": "What file formats does it actually accept? Does it connect to Google Forms or Tableau? What does 'clean' actually cover \u2014 will it handle the messy multi-tab Excel files I export from Forms? Can I run the same analysis template across five different programs automatically?",
    "price_reaction": "$10/month is nothing. If it saves me even one hour, it's worth it. But 'free tier with 3 projects' might not be enough to actually test it against my real workflow.",
    "name": "Terri Walter",
    "clarity_score": "partial"
  },
  {
    "person_id": 14,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "Clear and direct \u2014 the problem section described my exact workflow almost word for word, which got my attention immediately.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "My clients pay me for insights but I'm secretly spending half my time massaging spreadsheets to make the insights look credible.",
    "what_feels_off": "Wait, that's my quote from my own head \u2014 not the copy. The copy that lands closest is the 'after Excel' objection handling line. But the page doesn't say anything about making the output look polished or client-ready, which is honestly 40% of my problem. I need deliverables that don't look like a grad student made them, and nothing here addresses that visual quality concern.",
    "objections": "How does the export look? Is it a raw chart I still have to style in Canva, or does it produce something I can actually put in front of a client? The copy mentions 'charts and tables' but doesn't show me what they look like. That's the whole ballgame for me.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This talks to my pain better than most tools I've seen, but it doesn't close the loop on the part that matters most to me \u2014 presentation quality. I'm interested but I need to see the actual output before I commit to anything.",
    "unanswered_questions": "What do the exported charts actually look like? Can I customize them to match client branding? Is there a white-label option? How does it handle survey data with open-ended responses alongside quantitative fields?",
    "price_reaction": "$10/month is a rounding error for a consulting business. $25 for team features I don't need yet. The free tier with 3 projects is enough to evaluate whether the outputs are client-ready, which is the real question.",
    "name": "Danny Gonzalez",
    "clarity_score": "partial"
  },
  {
    "person_id": 15,
    "bucket": "duct_tape_analyst",
    "resonance": "neutral",
    "clarity_response": "The copy is clear, but it's solving a different problem than mine \u2014 I understand analysis just fine, my problem is that my tools don't talk to each other and I'm the translation layer.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Upload your interviews, surveys, or spreadsheets. Click to analyze. See what your data actually means \u2014 without writing a single line of code.",
    "what_feels_off": "The whole positioning assumes I don't know how to do analysis. I have a PhD in cognitive psychology. I know what a chi-square is. What I need is a tool that integrates with Dovetail and Qualtrics so I stop copy-pasting between six systems. This copy would resonate with someone who's afraid of data \u2014 I'm not afraid of data, I'm buried by it. The product might actually help me, but the copy is talking to someone else.",
    "objections": "No mention of integrations. No mention of whether it can connect to existing data sources. The 'upload your CSV' approach means I'd still have to export from Qualtrics, clean it, then upload it \u2014 which is exactly what I do now, just with an extra step. Also: does it do qualitative synthesis? 'Interview transcripts' is mentioned once but not explained.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "The price is so low I'd try the free tier out of curiosity, but the copy hasn't convinced me this is my solution \u2014 it reads like it was written for someone with much less research experience than me. I might discover it's actually useful, but I'd have to find that out despite the marketing, not because of it.",
    "unanswered_questions": "Does it integrate with Dovetail? Qualtrics? What exactly does it do with interview transcripts \u2014 thematic coding, sentiment, something else? How does it handle a study where I have both survey data and interview data from the same participants?",
    "price_reaction": "Laughably cheap. If it actually works, this is a no-brainer. But cheap pricing sometimes means the capability isn't there yet, so I'd go in with modest expectations.",
    "name": "Diana Wright",
    "clarity_score": "partial"
  },
  {
    "person_id": 16,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "The copy is clear, but it's describing a simpler problem than what I'm dealing with \u2014 my workflow is entrenched across eight years and multiple organizational systems, and nothing here addresses how I'd migrate to this without breaking something during grant reporting season.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "The copy assumes starting fresh is possible. My situation is that I have historical data in Qualtrics, analysis in Excel, visualizations in Tableau, narratives in Word \u2014 all of it interconnected and all of it actively in use. This product looks like it wants me to start over. 'One place' only helps if I can bring everything I already have into it, and I see no mention of migration, import from existing systems, or backward compatibility. The 'Clean' step in the how-it-works also concerns me \u2014 'fix inconsistencies, handle missing data' sounds great in theory but my data has very specific conventions my funders require.",
    "objections": "Can I import my historical Excel files without reformatting them? Does it produce outputs that match my funder's reporting templates? What's the data security posture \u2014 my organization's IT blocked Power BI; will this have the same problem? Quarterly report is in two weeks and I cannot risk experimenting.",
    "dealbreaker": true,
    "dealbreaker_reason": "No information about data security, organizational IT compatibility, or migration from existing workflows. I've been burned by Power BI once. I need those answers before I can even consider this, and the page doesn't provide them.",
    "gut_reaction": "This is clearly not built for someone in my situation \u2014 it's for an individual who can start fresh, not a director managing eight years of organizational data across multiple systems with funder requirements. The 'one click' pitch actually makes me more skeptical, not less.",
    "unanswered_questions": "Data residency and security details. IT approval requirements. Can it import historical Excel-formatted data and preserve the structure my funders expect? What happens to my Tableau visualizations \u2014 do I have to rebuild them? Is there an organizational/enterprise tier?",
    "price_reaction": "$25/month for the team tier is trivially cheap, which makes me wonder what I'm not seeing. Organizational software at this price point usually comes with serious capability limitations or data security trade-offs.",
    "name": "Anna Henderson",
    "clarity_score": "partial"
  },
  {
    "person_id": 17,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "The copy is clear about what the tool does for an individual, but I'm running a two-person LLC with government contracts and the copy says nothing about multi-user workflows, client-deliverable standards, or the kind of methodological documentation my contracts require.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "I bill $175/hour on government contracts. My clients expect SPSS and NVivo outputs with specific formatting. They do not expect a 'click to analyze' platform they've never heard of. The whole positioning is aimed at someone who is learning or struggling \u2014 I'm past that. My problem is operational coordination across two employees with different skill sets, four overlapping deadlines, and clients with specific software expectations. None of that is addressed here. The 'inter-rater reliability' feature in the Team plan caught my eye for a second \u2014 that's relevant to my qualitative work \u2014 but one feature doesn't make a platform.",
    "objections": "Do my government clients accept this as a documented analysis method? Can I produce outputs that pass a federal program review? What does the audit trail actually look like \u2014 is it the kind of documentation I can attach to a deliverable report? Does it support the same analyses I'm doing in SPSS right now, or is it a subset?",
    "dealbreaker": true,
    "dealbreaker_reason": "My clients specify SPSS and NVivo. No platform that doesn't address client-facing deliverable standards and federal contract documentation requirements is a viable replacement for me. This reads like a consumer tool, not a professional services tool.",
    "gut_reaction": "This is clearly aimed at grad students and small nonprofits, not at someone running government evaluation contracts. The price suggests it's not trying to serve my market, and the copy confirms it. Not my product.",
    "unanswered_questions": "Does the audit trail meet federal program review standards? Can it produce SPSS-formatted output? What's the data security classification \u2014 can I upload federal program data? How does the Team plan handle different permissions for my two employees working on separate client projects?",
    "price_reaction": "$25/month is what I charge for about 8 minutes of my time. Price is irrelevant at this level \u2014 it's not a factor in my decision at all. Capability and compliance are.",
    "name": "Paula Moreno",
    "clarity_score": "partial"
  },
  {
    "person_id": 18,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "This actually sounds like what I need \u2014 the description of uploading data and getting real answers in plain language is exactly the gap I feel every month when I'm manually exporting 8,000 rows from Salesforce.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Upload a CSV, a stack of interview transcripts, or survey exports. The platform detects your data types and guides you to the right analysis.",
    "what_feels_off": "The copy talks about 'upload your CSV' like that's simple, but my problem is that my data lives in Salesforce and getting it out of Salesforce correctly is half the battle. I'd still have to do that step manually. Also, 'Team \u2014 $25/month' for shared workspaces \u2014 who on my team would use this? I'm the only one doing analysis. The 'nonprofit program directors proving an intervention works' line in the Who It's For section is basically me, but I need a little more than one line to know this actually handles what I do.",
    "objections": "Can it handle Salesforce exports specifically \u2014 the way Salesforce formats its Excel exports is messy and inconsistent. Does it understand federal grant reporting metrics or do I have to define everything from scratch? What does 'proves an intervention works' actually look like in the output \u2014 is it something I can put directly into a funder report?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is the first tool in a while where I read the problem section and thought 'yes, that's me' \u2014 but I'm skeptical the solution is as simple as they're making it sound, because I've learned that 'just upload your data' always comes with complications I discover after I'm already committed.",
    "unanswered_questions": "Does it handle Salesforce-exported Excel files without reformatting? What does the funder-ready export actually look like \u2014 is it a formatted report or raw charts? Can I save my analysis setup so I'm not re-doing it from scratch every month when new data comes in? Is there a way to automate the monthly update process?",
    "price_reaction": "$10/month is nothing compared to the time I lose every month on this. If it actually works, that's an absurd deal. The free tier with 3 projects would let me test it on my actual data before committing, which is all I'm asking for.",
    "name": "Crystal Robinson",
    "clarity_score": "partial"
  },
  {
    "person_id": 19,
    "bucket": "duct_tape_analyst",
    "resonance": "neutral",
    "clarity_response": "The page is clear enough about what the tool does, but it glosses over the qualitative analysis piece I actually need \u2014 I can get correlations in Excel already.",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "The pitch is aimed at people who are lost with stats entirely. I'm not lost \u2014 I'm specifically weak on qualitative thematic synthesis. The page doesn't convince me this handles that rigorously. 'Click to analyze' sounds like it's solving the wrong problem for me.",
    "objections": "The qualitative side is vague. What does 'synthesizing 30 user interviews into actionable patterns' actually mean mechanically? Is it doing real thematic analysis or generating AI summaries I can't verify? My clients are starting to demand defensible methods and a black box won't cut it even if it has an audit trail button.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Interesting enough to try the free tier just to see the qual analysis side, but I'm not convinced this is better than what I'm already cobbling together. The pricing won't stop me, the methodology opacity might.",
    "unanswered_questions": "How does it handle focus group transcripts vs. individual interviews? What exactly is 'thematic analysis' in this context \u2014 is it AI-generated or structured coding? Can I export something a client would find credible, or does it look like a chatbot output?",
    "price_reaction": "Ten dollars a month is nothing if it actually works. I'd expense it without thinking. The free tier makes it easy to test before committing.",
    "name": "Ronnie King",
    "clarity_score": "partial"
  },
  {
    "person_id": 20,
    "bucket": "duct_tape_analyst",
    "resonance": "neutral",
    "clarity_response": "The tool's core function is clear, but it's described as a solo user solution and I manage three evaluators who each need to use it differently.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "The copy is entirely individual-focused. My problem isn't that I personally can't analyze data \u2014 it's that three people on my team are doing it three different ways and I can't reconcile the outputs. Nothing on this page speaks to standardization across a team or consistent outputs across projects. The Team plan mentions 'shared workspaces' but that's one line with no explanation.",
    "objections": "I've tried to get my team onto single platforms before and it always fails because of edge cases. This page gives me no reason to believe it handles the variety of data types my team works with. Also, 'inter-rater reliability' in one sentence on the Team plan is doing a lot of work \u2014 that's actually the thing I'd care about most and it gets zero elaboration.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "It reads like it was built for a graduate student or a solo practitioner, not for someone managing a team with inconsistent tool preferences. The price is low enough that I'd try it personally but I'm not forwarding this to my team yet.",
    "unanswered_questions": "What does 'shared workspaces' actually mean \u2014 can analysts work in parallel on the same dataset? How does inter-rater reliability work specifically \u2014 is it Cohen's kappa, something else? Can we standardize templates across projects so outputs look consistent regardless of who runs the analysis?",
    "price_reaction": "Twenty-five a month for a team is almost suspiciously cheap. Either the Team plan is underdeveloped or the pricing hasn't been figured out yet. I'd want to see what's actually in it before getting excited.",
    "name": "Shirley Suarez",
    "clarity_score": "partial"
  },
  {
    "person_id": 21,
    "bucket": "duct_tape_analyst",
    "resonance": "disagree",
    "clarity_response": "The page is clear but it's clearly not talking to me \u2014 it's built for people who feel lost with data, and I've been doing serious policy analysis for fifteen years.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "The positioning is entirely around people who don't know statistics. That's not my problem. My problem is that my workflow is stuck in 2008 \u2014 copy-pasting SPSS tables into Word, manually reformatting everything. This page doesn't address workflow integration at all. There's nothing here about connecting to existing tools, handling SPSS output, or producing publication-ready outputs in formats I actually need.",
    "objections": "The 'click to analyze and it picks the right test' framing is a red flag for someone at my level. I know which test I want \u2014 I need a tool that executes well, integrates cleanly, and produces outputs I don't have to reformat. This sounds like it's for someone who needs guidance on test selection, which is not my bottleneck. Also: nothing about SPSS or Stata import, which is where my data actually lives.",
    "dealbreaker": true,
    "dealbreaker_reason": "The tool appears to be designed for users who need methodological guidance, not experienced analysts who need workflow efficiency. Without any indication it handles SPSS data import or integrates into existing professional workflows, it doesn't solve my actual problem.",
    "gut_reaction": "This is not for me. The copy is talking to graduate students and nonprofit program managers, not senior policy analysts. I'd feel patronized clicking 'Compare groups \u2014 it runs the right test for you' when I've been running regressions since before some of their target users were in high school.",
    "unanswered_questions": "Can it import SPSS .sav files? Does it produce outputs that can be directly inserted into Word documents without reformatting? Does it support more complex multivariate analysis, not just the basics listed?",
    "price_reaction": "The price is irrelevant. Even at ten dollars a month I wouldn't pay for a tool that doesn't solve my problem.",
    "name": "Katherine Whitehead",
    "clarity_score": "partial"
  },
  {
    "person_id": 22,
    "bucket": "duct_tape_analyst",
    "resonance": "agree",
    "clarity_response": "This is straightforwardly written and I immediately understood who it's for and what problem it solves.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Going independent was the right call but I didn't fully account for what enterprise software licenses were hiding from my P&L.",
    "what_feels_off": "Actually that line isn't from the copy \u2014 the copy doesn't speak to my specific pain at all, which is that I lost my enterprise licenses when I went independent and I'm now paying per-project for tools that used to be free to me. The page doesn't mention anything about replacing enterprise tools for independent consultants. It's priced extremely cheaply, which either means it's lightweight or it's a new product still figuring itself out.",
    "objections": "The free tier is 3 projects, which could get used up fast if I'm running concurrent client work. The Pro tier at $10/month sounds too good to be true for something that's supposed to replace Qualtrics, Dovetail, and Excel integration. What's the catch \u2014 data limits? File size? Feature gaps that only appear when you're mid-project?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually addresses my problem more directly than I expected \u2014 I need something that handles both qual and quant without requiring me to pay for five enterprise tools. The price makes me suspicious but curious enough to try the free tier immediately.",
    "unanswered_questions": "What are the project limits on the free tier \u2014 is it just count or also file size? Does it actually handle interview transcripts and survey exports in the same project? Is there a Qualtrics import or do I have to export to CSV first?",
    "price_reaction": "Ten dollars a month for unlimited projects as a solo consultant doing 8-10 projects a year is remarkable if it actually works. I'd be a fool not to try it. My current improvised stack costs me time and mental overhead that's worth way more than ten dollars.",
    "name": "Anne Jordan",
    "clarity_score": "partial"
  },
  {
    "person_id": 23,
    "bucket": "duct_tape_analyst",
    "resonance": "strongly_disagree",
    "clarity_response": "The page is clear about what it does, and that clarity is exactly why I know it won't solve my problem.",
    "intent": "strongly_disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "I've onboarded twelve data systems in the last decade and they all looked good in the demo. This page reads exactly like every other demo I've ever seen. 'One place. One click.' \u2014 I've heard that before. The actual coordination problem I have is not that my team can't analyze data, it's that my team is in three time zones, at four different skill levels, working with field data that has connectivity gaps and nonstandard collection formats. Nothing on this page tells me how it handles any of that.",
    "objections": "The upload workflow assumes stable, clean, desktop-accessible data. My field teams in three countries are collecting data on tablets with intermittent internet. The 'Clean \u2014 map values, fix inconsistencies, handle missing data' step is doing an enormous amount of work in one line. My missing data problems are structural, not cosmetic. The audit trail is a nice feature but not the bottleneck.",
    "dealbreaker": true,
    "dealbreaker_reason": "The tool is built for individual researchers with relatively clean data. My problem is team coordination across geographies with field data collection complexity this product doesn't appear to address. The marketing is indistinguishable from every other tool I've evaluated and rejected.",
    "gut_reaction": "It looks good in the demo. That's exactly what worries me. I'll believe it works when I see it handle real messy field data with a team of five people who have different training and connectivity levels, not when I read a landing page.",
    "unanswered_questions": "Does it work offline or with intermittent connectivity? Can multiple users work on the same dataset simultaneously from different locations? What does 'handle missing data' actually mean \u2014 listwise deletion? Multiple imputation? Does it say?",
    "price_reaction": "The price is low enough that it's not the issue. The issue is onboarding cost \u2014 the invisible cost of my team's time to adopt a new system is always higher than the subscription fee. That's not addressed anywhere on this page.",
    "name": "Travis Gonzalez",
    "clarity_score": "partial"
  },
  {
    "person_id": 24,
    "bucket": "duct_tape_analyst",
    "resonance": "neutral",
    "clarity_response": "The page tells me what it does clearly, but it's built around the assumption that I'm struggling or desperate, and I'm neither.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "If you're spending $10-20/month on ChatGPT to debug R code, this replaces that.",
    "what_feels_off": "The entire emotional appeal is around frustration, panic, and being out of your depth. That's not where I am. I love my spreadsheets. I've been doing this for thirty years and my workflow works. The copy is essentially telling me I have a problem I don't feel I have. The AI angle is interesting \u2014 I actually was surprised that ChatGPT was useful for interpretation \u2014 but this page doesn't show me what it does differently or better than just asking ChatGPT with my Excel output.",
    "objections": "The 'click and it picks the right test' framing assumes I don't know what test I want, which is wrong. I know exactly what I want \u2014 I want the results faster and in a cleaner format. This page doesn't show me how it compares to just continuing what I'm doing. Also: I've rejected three tools that younger colleagues pushed on me. What makes this one worth my time to learn?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "The price is low enough that the downside of trying it is basically zero, and I'm mildly curious whether the AI interpretation is actually better than what I get from ChatGPT with my pivot table output. But the copy didn't make me feel like this was made for me \u2014 it made me feel like it was made for someone who's afraid of data.",
    "unanswered_questions": "Can I bring in my existing Excel models and have it interpret the outputs, or do I have to rebuild my analysis from scratch inside the platform? What does it do with complex pivot tables or Power Query outputs? Is there a learning curve or does it actually work on first use?",
    "price_reaction": "Ten dollars a month is irrelevant to my decision. I'm comfortable financially and I'm not price-sensitive on tools. The question is whether it's worth thirty minutes of my attention to try, not whether it's worth ten dollars.",
    "name": "Benjamin Perry",
    "clarity_score": "partial"
  },
  {
    "person_id": 25,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "The page describes my exact situation \u2014 paste into ChatGPT, get an error, paste the error back in \u2014 almost word for word.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "If you're spending $10-20/month on ChatGPT to debug R code, this replaces that.",
    "what_feels_off": "The 'one click' framing is doing a lot of heavy lifting. I've been burned enough times by tools that sounded this simple and weren't. Also, clinical trial data has specific structure and compliance considerations \u2014 nothing on this page tells me this handles that, or that it handles it correctly.",
    "objections": "Does this actually work with clinical trial data structures \u2014 timepoints, repeated measures, survival endpoints? The copy is pitched at people doing basic survey analysis. My needs are more specific than that. Also, 'the platform picks the right test' is a claim I need to verify before I trust it with real study data.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is speaking directly to my frustration and the price is low enough to try. But I've told myself 'worth trying' before and then spent two hours figuring out it doesn't do what I need.",
    "unanswered_questions": "Does it handle repeated-measures designs? Can I see the actual statistical output, not just the plain-language summary? What does the audit trail actually look like \u2014 is it citable?",
    "price_reaction": "Ten dollars a month is nothing compared to the time I'm losing. The free tier is smart \u2014 I'll start there before committing.",
    "name": "Erin Lin",
    "clarity_score": "partial"
  },
  {
    "person_id": 26,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "The problem description is accurate \u2014 the gap between knowing what I want statistically and getting the software to do it is real \u2014 but the solution feels underspecified for my actual use case.",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "The platform detects your data types and guides you to the right analysis.",
    "what_feels_off": "I'm a cognitive science PhD student \u2014 I know what a mixed-effects model is and I know why it matters. The copy talks about t-tests and chi-squares like those are the ceiling of what the tool does. If it can't handle lme4-equivalent modeling, it's not going to solve my actual problem. The 'you don't need to know which' framing would make me trust it less, not more \u2014 I do need to know which.",
    "objections": "The copy is written for someone who doesn't know statistics at all. I know statistics; I just can't make the code work. If this tool handles complex mixed models correctly, why isn't that mentioned? If it doesn't, this is not for me.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I want this to be for me, but I'm not confident it is \u2014 the examples skew toward basic survey analysis and nothing about this suggests it can handle convergence issues in hierarchical models.",
    "unanswered_questions": "Does it support mixed-effects models? What happens when a model doesn't converge \u2014 does it tell me, or does it silently return something wrong? Can I see the model specification, not just the output?",
    "price_reaction": "Price is not the issue. The free tier exists so I'll try it, but I'm not optimistic it handles what I actually need.",
    "name": "Lynn Perez",
    "clarity_score": "partial"
  },
  {
    "person_id": 27,
    "bucket": "ai_loop_prisoner",
    "resonance": "strongly_agree",
    "clarity_response": "Every sentence in the problem section could have been written about me, which is either reassuring or embarrassing \u2014 probably both.",
    "intent": "strongly_agree",
    "conversion_confidence": "agree",
    "price_perception": "good_deal",
    "strongest_line": "You get an error. You paste the error back in. You wait. You're not sure if the output is right, but you don't know enough to tell.",
    "what_feels_off": "The copy says 'auditable' and 'export the full audit trail' but I don't know what that means practically. If my senior colleagues look at a report I produced with this tool, will it be obvious I used it? Is that a problem? The copy doesn't help me think through that.",
    "objections": "Will the outputs look credible to someone who knows statistics? Can I actually pass off these results as analysis I did, or will the format be obviously auto-generated? I'm not trying to be dishonest \u2014 I'm trying to survive a job where I'm in over my head.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is exactly what I need and the price is low enough that I'd pay it myself without waiting for expense approval. I'm going to try the free tier tonight.",
    "unanswered_questions": "What do the exported outputs actually look like? Are there templates or does it generate something I'd still need to format? Does it handle segmentation and basic predictive modeling or just descriptive stats?",
    "price_reaction": "Ten dollars a month is cheaper than my anxiety about getting caught. Signing up for the free tier immediately.",
    "name": "Jason Daniels",
    "clarity_score": "wrong"
  },
  {
    "person_id": 28,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It clearly describes the problem I'm living with, though 'auditable' and 'no black boxes' are claims that need to mean something specific for me to trust them.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "The objection handling section says 'no black boxes' but that's exactly what ChatGPT also says in effect \u2014 it gives me a method name, I paste it in the report, and once it said something was 'trending toward significance' and I didn't catch that it was just nonsense. What does 'auditable' actually mean here \u2014 can I verify the p-value independently? Because if I can't, this is the same problem in a nicer wrapper.",
    "objections": "My core issue isn't ease of use, it's correctness I can verify. If this tool is essentially a friendlier AI wrapper, I might make the same category of mistake more confidently. I need to know what the audit trail actually shows \u2014 method name only, or the actual test statistics I could cross-check?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I'm interested and the price is irrelevant at these numbers, but I've learned to distrust tools that promise trustworthiness without demonstrating it \u2014 I need to see the actual output format before I commit.",
    "unanswered_questions": "What does the audit trail look like \u2014 just the method name or actual test statistics and assumptions? Is there a sample output I can look at before I upload client data? How does it handle Qualtrics exports specifically?",
    "price_reaction": "Pricing is not a concern at all \u2014 the free tier is enough to test whether the outputs are something I can stand behind professionally.",
    "name": "Shannon Jones",
    "clarity_score": "partial"
  },
  {
    "person_id": 29,
    "bucket": "ai_loop_prisoner",
    "resonance": "strongly_agree",
    "clarity_response": "The copy describes my situation precisely and the solution sounds like exactly what I need, but I'm worried it won't actually understand why my residuals look wrong.",
    "intent": "agree",
    "conversion_confidence": "agree",
    "price_perception": "good_deal",
    "strongest_line": "Stop brute-forcing your analysis.",
    "what_feels_off": "The copy frames this as for people who 'don't know statistics' but I sort of know statistics \u2014 I know what a residual is, I know something's wrong, I just don't know how to fix it. If the tool's answer to weird residuals is a plain-language sentence that says 'your regression results are significant,' that's not going to help me. I need the tool to flag problems, not just produce outputs.",
    "objections": "Does this tool tell me when something looks wrong, or does it just run the analysis and give me a result? ChatGPT's problem is it gives confident answers regardless of whether they're correct. If this tool does the same thing in a prettier interface, I've just paid for a nicer version of my problem.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I'd sign up today if the free tier exists \u2014 the stakes are high enough that a ten-dollar solution would feel like a miracle, but I'm going in expecting to be disappointed and hoping to be surprised.",
    "unanswered_questions": "Does it do any diagnostic checking \u2014 heteroskedasticity, residual plots, multicollinearity? What does it do when the data suggests a regression assumption is violated? Can I run this through Excel data I already have?",
    "price_reaction": "At ten dollars a month, the risk is basically zero financially. The risk is that I trust an output I shouldn't, which is my current problem anyway \u2014 so at minimum it's no worse than what I'm doing now.",
    "name": "Cassandra Gaines",
    "clarity_score": "partial"
  },
  {
    "person_id": 30,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "The copy is clear and I understand the value proposition, but 'the platform picks the right test' is precisely the kind of claim that's gotten me burned before \u2014 I need to know the mechanism, not just the promise.",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "I need a tool that actually understands stats, not one that sounds confident about things I can't verify.",
    "what_feels_off": "That last line is actually my own quote from my life, which is either a sign this product is for me or a sign the marketing is well-targeted. The problem is the copy doesn't tell me anything about how the statistical engine works \u2014 it just tells me to trust it. That's the same ask ChatGPT makes. I've been burned twice on confidence intervals from ChatGPT; this page doesn't give me any reason to believe this tool would have caught those errors.",
    "objections": "The 'no black boxes' objection handling is almost insulting to someone at my level \u2014 of course I want to see the method, that's the minimum. The real question is whether the method is correct, and nothing on this page addresses that. Show me that your A/B test interpretation handles multiple comparison correction, or that your confidence intervals account for sample size properly. Don't tell me you show the method name.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "The product might be exactly what I'm looking for \u2014 a statistical check on my work rather than just a code generator \u2014 but this page doesn't convince me it's that, it just tells me to trust it, which is what I'm trying to move away from.",
    "unanswered_questions": "How does the tool handle A/B test analysis specifically \u2014 does it apply corrections for multiple testing? Can it flag when a result looks anomalous before I export it? What's the actual statistical engine \u2014 is this a wrapper around R, Python statsmodels, or a proprietary model?",
    "price_reaction": "At these prices, the cost isn't the barrier. The barrier is whether I can verify the outputs are correct before I put them in a report, and that requires actually testing it \u2014 free tier first, Pro only if it demonstrates it can catch what ChatGPT misses.",
    "name": "Victoria Baker",
    "clarity_score": "partial"
  },
  {
    "person_id": 31,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "The problem description is accurate and the workflow it describes \u2014 paste into ChatGPT, get error, paste error back \u2014 is exactly my life right now.",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "You're not alone. The tools that exist today \u2014 NVivo, Atlas.ti, SPSS \u2014 were built for methodologists with institutional budgets and years of training. Not for you.",
    "what_feels_off": "The reproducibility problem is my actual nightmare \u2014 when an editor asks me to re-run analysis with one change, I need to know the tool will produce the same result. The copy says 'auditable' and 'exportable' but doesn't tell me whether re-running the same analysis on the same data gives me the same numbers every time. That's the thing I need answered before I pay anything.",
    "objections": "The claim that it 'picks the right method' is doing a lot of work here. Who decided what the right method is? What if my data situation doesn't fit the textbook case? I've been burned by AI that was confident and wrong \u2014 this copy is asking me to trust another AI to pick my methods without explaining the selection logic at all. Also, the audit trail mention is good but vague: does the export actually show me the test statistic, degrees of freedom, and exact p-value, or is it just a plain-language summary?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This hits close to home and the price is low enough that I'd probably try the free tier. But the reproducibility question is unresolved and that's the thing that could embarrass me professionally in print.",
    "unanswered_questions": "Does re-running the same analysis on the same dataset always produce the same result? What exactly is in the 'audit trail' export? Can I specify my own analysis parameters if I disagree with what the tool chose? How does it handle the kinds of edge cases that trip up ChatGPT?",
    "price_reaction": "Ten dollars a month is less than I spend on coffee in a week. If it actually works, the price is not the issue \u2014 the question is whether it works.",
    "name": "Travis Vance",
    "clarity_score": "partial"
  },
  {
    "person_id": 32,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "The problem section describes my situation almost verbatim, which is either reassuring or a little unsettling.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "The copy says it will interpret SPSS-style output in plain language, which is basically what I've been doing with Claude already \u2014 pasting tables in and asking what they mean. What I need to know is whether this tool's interpretations are more reliable than what I've been getting, and the copy gives me no way to evaluate that. 'The platform picks the right method' sounds great until it picks the wrong one and I don't know enough to catch it.",
    "objections": "My core problem is that I can't verify the AI's interpretations against a ground truth \u2014 I'm not sure which of Claude's answers about my logistic regression output are correct. This tool seems to be solving the interface problem, not the trust problem. If I can't verify the results, I'm in the same position I'm already in, just with a prettier dashboard. Also: the objection-handling section doesn't address the scenario where the tool's plain-language interpretation is wrong.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I'd try the free tier because three projects at no cost is a reasonable test, but I'm skeptical that 'the platform picks the right method' is going to hold up on federal compliance reporting where the stakes for a wrong interpretation are real.",
    "unanswered_questions": "How do I verify that the method the platform chose is actually correct for my data? Is there a way to override the automatic test selection? What does the plain-language output look like \u2014 can I see an example? Does it handle logistic regression correctly?",
    "price_reaction": "Ten dollars a month is genuinely affordable for a mid-career professional, but I won't upgrade from free until I'm confident the interpretations are accurate.",
    "name": "Monica Herrera",
    "clarity_score": "wrong"
  },
  {
    "person_id": 33,
    "bucket": "ai_loop_prisoner",
    "resonance": "agree",
    "clarity_response": "It describes exactly the gap I'm in \u2014 knowing what I want to produce, not knowing how to produce it without leaning on AI I can't fully vouch for.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "If you're spending $10-20/month on ChatGPT to debug R code, this replaces that.",
    "what_feels_off": "The copy positions this as a replacement for the ChatGPT-to-write-R-code workflow, which is accurate to my situation, but it doesn't address the credibility problem: my clients expect R-based reproducible analysis and I've been submitting ChatGPT-written code as my own. If this tool outputs results but not the underlying R code, I can't submit the tool's output as the deliverable my clients contracted for. I need to know if this produces exportable code or just results.",
    "objections": "The professional credibility issue is real: if clients are paying for R-based analysis and I switch to a point-and-click tool, am I still delivering what they hired me for? The copy doesn't address consultant-to-client deliverable format at all. Also, the 'audit trail' language is promising but I need to know if it produces something I could show a methodologically literate client without embarrassment.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "The price is low enough and the problem description accurate enough that I'd try it, but this doesn't solve my real problem unless it can produce deliverables in a format my clients expect, not just a summary report.",
    "unanswered_questions": "Does the platform export R code alongside results, or just summary outputs? What does an 'audit trail' look like in practice \u2014 is it rigorous enough to share with a methodologically sophisticated client? Can I brand or customize the exported reports?",
    "price_reaction": "Ten dollars a month is nothing. If it replaced even two hours of ChatGPT debugging per month it would pay for itself. The price is not the question.",
    "name": "Cheryl West",
    "clarity_score": "partial"
  },
  {
    "person_id": 34,
    "bucket": "ai_loop_prisoner",
    "resonance": "neutral",
    "clarity_response": "The problem framing is accurate for a lot of people I know, but it undersells the stakes \u2014 the copy makes this sound like a convenience problem when for me it became a methodological credibility problem.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "I got burned by a methodological error that a biostatistician had to catch. The copy is trying to tell me this tool is trustworthy with the 'auditable' and 'every result shows the method' language, but it's not specific enough to rebuild confidence. What does 'auditable' mean for propensity score matching? Does the tool even support propensity score matching? Survival analysis? The use cases listed are basic \u2014 the copy implies more sophistication than the feature list confirms.",
    "objections": "After my review incident, 'the platform picks the right method' is exactly the phrase that makes me nervous. That's what ChatGPT was doing when it wrote my propensity score code incorrectly. I need transparency about what the tool can and cannot do, not marketing language about how it picks the right method. The copy is too confident and not specific enough about the boundaries of its capabilities.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "The price is low so I'd try the free tier, but the vagueness about advanced methods makes me think this is aimed at people doing basic analysis, not the clinical outcomes work I've been doing \u2014 and that's where I need the help.",
    "unanswered_questions": "What advanced methods does this actually support? Does it handle survival analysis, propensity score matching, time-series data? How does it communicate the boundaries of its own capabilities when it encounters a method it can't handle reliably? Can I see the underlying statistical output, not just the plain-language summary?",
    "price_reaction": "The price is essentially free to try. I'm not price-sensitive here \u2014 I'd pay $50/month if I trusted the methodology. The price is irrelevant if the tool doesn't handle the analyses I need.",
    "name": "Joshua Taylor",
    "clarity_score": "partial"
  },
  {
    "person_id": 35,
    "bucket": "ai_loop_prisoner",
    "resonance": "strongly_agree",
    "clarity_response": "This is written for me \u2014 the ChatGPT loop, the R struggle, the 'I've been trying to learn R for two years and I'm still not there' \u2014 it's almost uncomfortably accurate.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "You get an error. You paste the error back in. You wait. You're not sure if the output is right, but you don't know enough to tell.",
    "what_feels_off": "The copy tells me it's going to work but doesn't show me what success looks like. What does the output actually look like? Do I get a report I can hand to my program director or just a visualization on a screen? My funder needs specific metrics \u2014 program completion rates, demographic breakdowns, outcome comparisons \u2014 and I need to know this tool produces those, not just generic charts. The 'ready for your funder' line is promising but not backed up with anything.",
    "objections": "I've been hopeful before. I've tried tools before. The copy sounds like every other tool that was going to solve this problem and didn't. What makes this different from just pasting my data into Claude and asking for analysis? The copy mentions 'detects your data types' and 'guides you to the right analysis' but I've had Claude do both of those things and it still got things wrong. I need something more than 'trust us, it works.'",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I want to believe this is the thing that finally works, and the free tier means I can find out at no cost. But I've been hopeful before and I'm not going to upgrade until I see it actually produce a report I could submit to my CDC program officer.",
    "unanswered_questions": "What does the output actually look like? Can I see a sample report? Does it handle the kinds of outcome tracking metrics that federal cooperative agreements require? Is there customer support when something breaks, or am I back to debugging on my own?",
    "price_reaction": "Ten dollars a month is less than what I'd pay for a single hour of a consultant's time. If it does what it says, it's a steal. But I've spent money on tools before that didn't deliver.",
    "name": "Crystal Johnson",
    "clarity_score": "partial"
  },
  {
    "person_id": 36,
    "bucket": "ai_loop_prisoner",
    "resonance": "neutral",
    "clarity_response": "It correctly identifies the problem but the framing \u2014 'you're not a data person' \u2014 doesn't fit me or my team, which makes me wonder if this is actually built for what we need.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "We have a team with three PhD-level researchers and a combined 40 years of experience and we spent last Tuesday afternoon all staring at a ChatGPT conversation together.",
    "what_feels_off": "The copy pitches to individuals who are struggling with basic analysis, but my problem is team-level: I need a standardized approach to AI-assisted analysis across five people with different skill levels. The Team tier mentions 'shared workspaces' and 'role-based permissions' but says nothing about version control, analysis templates, or whether multiple people working on the same project produces consistent results. The team features feel like an afterthought.",
    "objections": "The 'platform picks the right method' line is going to land differently with three PhD researchers who all have opinions about method selection. My team isn't going to accept a black box recommendation without being able to interrogate it \u2014 and the copy's answer to 'can I trust the results' is just 'we show you the method and the audit trail,' which doesn't tell me whether the method selection logic will satisfy a team of methodologists who argue about this stuff for a living. Also: twenty-five dollars a month for a team tier seems low, which makes me wonder what's missing from it.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "The price is low enough that I'd let one team member try it, but I'm skeptical this is built for a team of advanced researchers \u2014 it reads like it's optimized for solo users who are intimidated by statistics, which is not my team's problem.",
    "unanswered_questions": "How does the team tier handle version control and audit trails across multiple analysts? Can researchers override or document dissent from the platform's method recommendations? What happens when the platform can't handle the analysis a researcher needs \u2014 does it fail gracefully or silently? Is there a way to evaluate this with a research-grade test dataset before committing?",
    "price_reaction": "Twenty-five dollars a month for a team is suspiciously cheap \u2014 either the team features are limited or this is a loss-leader pricing strategy. I'd want to see exactly what's in the team tier before reading too much into the price.",
    "name": "Guy Molina",
    "clarity_score": "partial"
  },
  {
    "person_id": 37,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "The page is readable but tells me nothing about data security, compliance architecture, or how research data is handled \u2014 the things I actually need to know before I can bring this to IT.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "The entire page is written for an individual user who is scared of statistics, not for someone procuring a tool for a team. COPPA sensitivity and data residency are my first questions and this page doesn't acknowledge they exist. 'Try it free' is not the conversation I need to have.",
    "objections": "Where is the data stored? What is your SOC 2 status? What happens to uploaded research data \u2014 is it used for model training? Can I control who on my team sees what projects? None of this is answered and those are my opening questions before I'd consider a demo.",
    "dealbreaker": true,
    "dealbreaker_reason": "No mention of data security, compliance certifications, or enterprise controls. I manage a COPPA-sensitive research environment and I cannot evaluate a tool that doesn't surface its data handling policy on the landing page. I'd leave this page and look for those answers before taking another step.",
    "gut_reaction": "The price is almost suspiciously low which makes me wonder what the actual business model is \u2014 are they monetizing the data? The page is polished but it's selling to grad students, not to me.",
    "unanswered_questions": "What model is running the analysis? Where does uploaded data live and for how long? Is there a BAA or DPA available? What are the role-based permission controls in the Team tier? Is there an enterprise tier with SSO? What does the audit trail actually look like \u2014 can it satisfy an IRB?",
    "price_reaction": "Ten dollars a month for the Pro tier makes me more suspicious, not less. Either this is a freemium lead-gen play or the data is the product. At my company's scale I'd need an enterprise contract with defined SLAs and that's not even on the page.",
    "name": "Jamie Mayo",
    "clarity_score": "partial"
  },
  {
    "person_id": 38,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "The copy is clear enough about what the tool does functionally, but it gives me zero information about what's running under the hood \u2014 no mention of the underlying model, API architecture, data residency, or reproducibility guarantees.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "The framing is entirely oriented toward non-technical individual users who are afraid of stats. I need to know the architecture before I can evaluate whether this could sit alongside Snowflake and Looker. 'Click to analyze' is not a sentence that builds confidence with my procurement team.",
    "objections": "What model is selecting the statistical test? Is the test selection deterministic or stochastic \u2014 if I run the same dataset twice, do I get the same result? What does 'auditable' actually mean \u2014 is there a log file I can pull? What is the data residency model? Does uploaded data leave the user's environment? None of this is here.",
    "dealbreaker": true,
    "dealbreaker_reason": "I cannot put this in front of procurement without knowing the data handling policy, the underlying architecture, and whether results are reproducible. The page is written for someone who doesn't know what a t-test is; I need the technical documentation that this page doesn't link to.",
    "gut_reaction": "Looks like a well-designed consumer product that hasn't figured out how to sell to enterprise yet \u2014 I'd need a completely different conversation than what this page is offering.",
    "unanswered_questions": "Can I get the same result twice? What model is making test selection decisions? Where does the data go after upload? Is there an API so this could integrate with our existing stack? What does the Team tier's role-based permissions actually look like in practice?",
    "price_reaction": "Twenty-five dollars a month for a team tier is essentially free in my budget context, which tells me this pricing was built for grad students and nonprofits, not teams at my scale. If this is real software for professional use, I'd expect an enterprise tier with actual contracts.",
    "name": "David Douglas",
    "clarity_score": "partial"
  },
  {
    "person_id": 39,
    "bucket": "budget_gatekeeper",
    "resonance": "strongly_disagree",
    "clarity_response": "The page clearly describes a tool for people who don't know statistics, which is not my situation \u2014 my team knows statistics, my problem is FERPA compliance, and this page says nothing about it.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "I've turned down three tools in the last six months because of FERPA. This page doesn't mention student data privacy, compliance certifications, or data governance once. The copy feels like it was written for a grad student running a personal thesis, not for an institutional research office handling data about 18,000 students. The $10/month Pro tier alone tells me this was not built for my context.",
    "objections": "My university's data governance policy prohibits sending student data to third-party AI systems. Is this tool processing uploaded data through an AI model? If so, is there a FERPA-compliant data processing agreement available? Is the analysis happening locally or server-side? Can we get a BAA-equivalent for FERPA? This is not optional for me.",
    "dealbreaker": true,
    "dealbreaker_reason": "No FERPA language anywhere on the page. Until I know that student data is not being processed by an external AI model \u2014 or that there is a proper institutional data agreement \u2014 I cannot even start an evaluation. This page gives me no basis to believe this tool has thought about my compliance environment at all.",
    "gut_reaction": "I immediately liked the idea and immediately knew I couldn't touch it without answers this page doesn't provide \u2014 that's a frustrating place to land.",
    "unanswered_questions": "Does upload data get sent to an external AI model for processing? Is there a FERPA-compliant data processing agreement? Is the tool FERPA-eligible at all? What is the data retention policy? Can data be deleted after analysis? Is there an institutional or enterprise tier built for higher education?",
    "price_reaction": "The pricing structure tells me everything about who the target customer is \u2014 this is built for individual researchers, not institutions. An IR office at a public university cannot run on a $25/month team plan. If there's no enterprise tier with institutional contracts, this conversation ends here.",
    "name": "Kristina Baker",
    "clarity_score": "partial"
  },
  {
    "person_id": 40,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "The page communicates the value proposition clearly for non-technical users, and I can see how this might work for grantees with less research training, but I need to understand data sovereignty and whether this can serve the range of technical ability on my team before I'd move forward.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "The page is written entirely for the user who is afraid of statistics. I need to evaluate whether this could also work for my technical staff who currently use Qualtrics, NVivo, and Stata \u2014 users who will immediately ask hard questions about what the tool is actually doing. There's nothing here that would satisfy them. Also, I've been burned by data sovereignty clauses before and this page doesn't mention data ownership at all.",
    "objections": "I lost two platforms at the contract stage over data sovereignty \u2014 who owns the data I upload? What rights does the platform retain over analysis outputs? Can grantees with varying levels of technical sophistication actually use this without support? What happens to data after a project is closed?",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is promising enough to request a demo and ask hard questions, but I'd approach that demo with a specific list of what broke the last two evaluations \u2014 and this page hasn't given me any reason to think those things are resolved.",
    "unanswered_questions": "Who owns uploaded data and analysis outputs? What is the data sovereignty policy? Can this integrate with Qualtrics exports? What does inter-rater reliability in the Team tier actually look like \u2014 is that sufficient for qualitative coding? What would a grantee onboarding experience look like for someone with minimal research training?",
    "price_reaction": "The price is genuinely attractive and would allow us to provide access to grantees without meaningful cost \u2014 that's actually appealing. But I've been burned by cheap tools with expensive data clauses, so the price alone doesn't move me.",
    "name": "Kimberly Blair",
    "clarity_score": "partial"
  },
  {
    "person_id": 41,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "The page describes a coherent product clearly, but it doesn't address any of the things that have caused the three demos I attended this month to fail \u2014 it reads like a pitch to someone who doesn't know what they need, and I've known what I need for eight months.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "I have budget and organizational buy-in and I've been specifically burned by tools that look good on the landing page and fail in the demo. This page is smooth but it's optimized for converting someone who is scared and desperate, not someone who is experienced and has specific requirements. I don't see anything about migrating from SPSS or Dedoose, which is my actual problem. And the pricing suggests this was not built for a team of six doing policy-facing research.",
    "objections": "My team uses SPSS, Qualtrics, and Dedoose \u2014 can data and analysis history be migrated from any of these? What does the 'auditable' export actually look like \u2014 is it sufficient for board-level presentations and donor reporting? The Team tier at $25/month feels like it was built for grad students sharing a thesis; I need something built for a professional research organization.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "I'm interested enough to click through and request a demo, but I've been disappointed in three of those already this month so my baseline is skepticism \u2014 this page is not doing enough to differentiate itself from the tools that looked fine and then fell apart.",
    "unanswered_questions": "What does migration from existing tools (SPSS, Dedoose) look like? What does the audit trail export contain specifically? How does the platform handle qualitative analysis \u2014 is that actual thematic analysis or just keyword search? Is there an enterprise tier beyond the Team plan? What does collaboration actually look like for a distributed team?",
    "price_reaction": "The price is not my concern \u2014 I have budget. At $25/month for a team I'm actually wondering if this is too cheap to be the professional-grade tool I need. I'd pay ten times that if it actually worked for my use case.",
    "name": "Carolyn Daniel",
    "clarity_score": "partial"
  },
  {
    "person_id": 42,
    "bucket": "budget_gatekeeper",
    "resonance": "strongly_disagree",
    "clarity_response": "The page tells me what the tool does at a surface level, but it says nothing about documentation trails, data security, or reproducibility \u2014 the three things I would need to demonstrate in a federal audit, and none of which appear anywhere on this page.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "The word 'auditable' appears once in the copy and is not explained. In my world, auditable means I can produce a documented record of every analytical decision made, trace it back to a specific method with a citable basis, and defend it to a federal project officer. I have no idea if that's what this means. The page is selling ease and accessibility \u2014 my compliance environment requires rigor and traceability, and those are different things that this copy doesn't address.",
    "objections": "My federal contracts require documented analysis methods, data security standards, and reproducible results. Does this tool produce analysis that can be defended in a CDC or NIH audit? What does the audit trail contain specifically \u2014 method name only, or full parameter documentation? What is the data security posture \u2014 FedRAMP, SOC 2? Is uploaded data used for model training? Can I get the same result from the same dataset twice?",
    "dealbreaker": true,
    "dealbreaker_reason": "I have tried to standardize my firm's analysis approach twice and both times a senior analyst left and it fell apart \u2014 I'm not adopting another tool without a clear compliance story. This page doesn't mention federal contracts, data security, or reproducibility guarantees once. The risk calculus with federal contracts is unforgiving and this page is not written for someone operating in that environment.",
    "gut_reaction": "The tool might be legitimate but this landing page was written for a nervous grad student, not for a research director managing CDC contracts \u2014 there is a fundamental mismatch between the audience this page is talking to and the audience that would actually need organizational buy-in to use it.",
    "unanswered_questions": "What is the formal data security posture \u2014 SOC 2, FedRAMP? Is uploaded data retained and for how long? Is there a documented method selection algorithm I can cite in a methods section? Can analysis be reproduced exactly \u2014 same inputs, same outputs, every time? Is there enterprise-level documentation I could use to support a federal compliance review? What would a methods citation in a federal report look like for this tool?",
    "price_reaction": "Twenty-five dollars a month for a team tells me this tool was not built for a firm billing on federal contracts. That price point signals a consumer or nonprofit market. If there's an enterprise tier with proper contractual protections I don't see it here, and without it I'm not the right customer for this product.",
    "name": "Christopher Mcdonald",
    "clarity_score": "partial"
  },
  {
    "person_id": 43,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "The page is clear enough about what it does, but it doesn't speak to the organizational complexity I'm actually dealing with \u2014 getting non-technical program staff to trust numbers they didn't produce.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "The page talks a lot about individuals solving their own analysis problems, but I'm not buying this for myself \u2014 I'm buying it for my evaluation coordinator and a dozen program managers who need to trust the output without understanding how it was made. Nothing on this page addresses team adoption, change management, or whether my non-technical staff will actually use it versus just call my evaluation person anyway.",
    "objections": "I approved a Tableau license two years ago that sits mostly unused. What makes me think this will be different? The page doesn't tell me how to get my team actually using a new tool. I also want to know what happens to my organization's participant data when it goes into this platform \u2014 no privacy or data handling language anywhere.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This looks like it solves the right problem for individual researchers, but I'm not an individual researcher \u2014 I'm trying to change how twelve people work, and this page doesn't give me a reason to believe that's possible here.",
    "unanswered_questions": "How does this handle team adoption for non-technical users? What are the data handling and privacy terms? Is there any onboarding or implementation support, or do I have to figure that out myself? Can my program staff view reports without needing an account?",
    "price_reaction": "The free tier is not a risk, and $25/month for a team is genuinely cheap \u2014 if it works. The price is not my concern. The adoption problem is.",
    "name": "Jane Ferrell",
    "clarity_score": "partial"
  },
  {
    "person_id": 44,
    "bucket": "budget_gatekeeper",
    "resonance": "neutral",
    "clarity_response": "I understand what the product does, but I need to understand what it does in a read-only, data-governance-controlled environment before this is a real conversation.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "This is written for someone doing their own analysis, not for an IR director trying to give department chairs a self-service tool with guardrails. The page says nothing about data governance, access controls beyond 'role-based permissions,' or whether this can operate in a read-only mode where non-technical users can query without touching the underlying data. That's my entire use case and it's not addressed.",
    "objections": "My concern isn't whether the tool works \u2014 it's whether it generates more IT support tickets than it saves. The page doesn't address integration with institutional data systems like Banner or Ellucian, doesn't mention SSO, doesn't mention any enterprise or institutional tier. The 'Team' plan at $25/month sounds like it's designed for a small research team, not a 15,000-student institution with formal data governance requirements.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "Interesting tool, but this landing page is aimed at graduate students and individual analysts \u2014 I don't see anything here that tells me it can operate at institutional scale with proper data controls.",
    "unanswered_questions": "Is there an institutional or enterprise tier? Can it connect to existing institutional data warehouses? Does it support SSO? What are the data residency and security specifications? Can non-licensed users view read-only outputs, or do they need accounts?",
    "price_reaction": "$25/month team plan is not a realistic price point for institutional procurement \u2014 we'd need custom enterprise pricing with a contract and probably a security review. This price makes it feel like a small-team product.",
    "name": "Marcus Johnson",
    "clarity_score": "partial"
  },
  {
    "person_id": 45,
    "bucket": "budget_gatekeeper",
    "resonance": "agree",
    "clarity_response": "It's clear and easy to understand, which is actually somewhat reassuring given my team's technical level.",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Upload your interviews, surveys, or spreadsheets. Click to analyze. See what your data actually means \u2014 without writing a single line of code.",
    "what_feels_off": "My real obstacle isn't whether this tool works \u2014 it's whether I can get it through my organization's software approval process. The page doesn't address anything about security reviews, data handling, or compliance, which is exactly what IT and legal will ask me about. I've been through this gauntlet before with Power BI and Tableau. I need language I can hand to four different stakeholders with four different objections.",
    "objections": "The free tier is appealing for a pilot, but I've been burned before by free tiers that don't survive contact with real organizational data requirements. I need to know who has access to the data I upload, where it's stored, and whether there's a data processing agreement available. The page doesn't mention any of this.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This actually looks usable for my team's skill level, and the price is so low it would barely register in my budget \u2014 but I've learned that cheap and technically capable doesn't mean organizationally approvable, and this page doesn't help me get through the approval process.",
    "unanswered_questions": "Where is data stored and who has access to it? Is a data processing agreement available? How does it handle sensitive grantee data? Is there any documentation I can give to IT and legal to support an approval request?",
    "price_reaction": "The price is almost irrelevant at this level \u2014 $25/month wouldn't even require VP approval. My problem has never been the price. It's been the four-stakeholder approval gauntlet, and nothing on this page helps me navigate that.",
    "name": "Jennifer Brown",
    "clarity_score": "partial"
  },
  {
    "person_id": 46,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "The page is clear about what it does for an individual user, but I've been in higher ed assessment for 25 years and nothing here tells me it's been built to operate at institutional scale or survive a security review.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "I evaluate tools slowly and deliberately for a reason. I've watched a Nuventive implementation go sideways. This page reads like it was written for a grad student in a panic, not for an institution with 40 academic programs, HLC reaffirmation requirements, and a formal security review process. The 'audit trail' language is promising but not developed enough \u2014 what does 'auditable' actually mean here? Can I produce a reproducibility record that satisfies an accreditor?",
    "objections": "The objection handling section is weak. It responds to 'can I trust the results?' with two sentences about transparency. That's not enough for institutional adoption. I need to know: has this been third-party validated? Does it have documented method selection logic I can share with my accreditor? Is there an SLA? Who do I call when something breaks during our accreditation review?",
    "dealbreaker": true,
    "dealbreaker_reason": "No security documentation, no institutional compliance language, no SLA or support tier for institutional customers. I require a full security review and 90-day pilot before committing to any tool, and this page gives me nothing to start that process.",
    "gut_reaction": "This might be a useful tool for a graduate student or a small nonprofit team, but it is nowhere near ready for my consideration as an institutional procurement decision.",
    "unanswered_questions": "Has this tool undergone a third-party security audit? What are the data residency guarantees? Is there an enterprise or institutional tier with proper SLA terms? What is the method selection logic and is it documented in a form I can share with accreditors? What is the tool's uptime guarantee?",
    "price_reaction": "The pricing tier tells me everything I need to know about who this is built for. $25/month team plans are not institutional pricing. This is a product for individuals and small teams, not a 40-program academic institution.",
    "name": "Sharon Cherry",
    "clarity_score": "partial"
  },
  {
    "person_id": 47,
    "bucket": "budget_gatekeeper",
    "resonance": "agree",
    "clarity_response": "This is the clearest explanation of an analysis tool I've read \u2014 I actually understand what it does, which is unusual.",
    "intent": "agree",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "My board wants dashboards and better data storytelling, but this page focuses almost entirely on analysis \u2014 not on what the output looks like or how it communicates to a non-technical audience. I need to see what a funder report or board presentation looks like when it comes out the other side. 'Visualize' is mentioned but I don't get a sense of the quality or customizability of those outputs.",
    "objections": "I approved a SPSS license three years ago that only one person uses. I'm worried I'd buy this and it would end up the same way. Nothing on this page tells me how to get my two staff members to actually adopt it and change their workflow. The 'try it free' offer is good but I'd want to see what a real output looks like before I involve my team.",
    "dealbreaker": false,
    "dealbreaker_reason": null,
    "gut_reaction": "This is genuinely appealing \u2014 the price is trivial for my organization and it sounds like it could help my team move from static Word documents to something my board could actually engage with, but I need to see the output quality before I'd commit to it.",
    "unanswered_questions": "What do the visualization outputs actually look like \u2014 are they board-presentation quality? Can they be customized with our branding? What's the learning curve for a small team that currently uses Qualtrics and Excel? Is there any onboarding support?",
    "price_reaction": "The price is genuinely not a factor \u2014 $25/month for a team is a rounding error in our budget. If it does what it says, this is an obvious yes on price. The question is whether it does what it says.",
    "name": "Whitney Peters",
    "clarity_score": "partial"
  },
  {
    "person_id": 48,
    "bucket": "budget_gatekeeper",
    "resonance": "disagree",
    "clarity_response": "The page is clear about what it does, but it says nothing about HIPAA, data handling, or the compliance infrastructure that would need to exist before my legal department would let me touch it.",
    "intent": "strongly_disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable. Ready for your committee, your funder, or your board.",
    "what_feels_off": "I've walked away from two AI analysis tools in the last 18 months for exactly this reason: no HIPAA compliance documentation. This page doesn't mention HIPAA, BAA, data residency, or any of the compliance infrastructure I need before patient data can leave our environment. The word 'auditable' appears once in the context of method transparency, not data handling \u2014 these are very different things.",
    "objections": "The entire page is written for researchers who are primarily worried about whether the analysis is right. That's a legitimate concern, but it's not my primary concern. My primary concern is whether this creates a HIPAA liability for my health system. No BAA mention, no data security documentation, no mention of whether data is processed on our servers or theirs. My legal team will not let this move forward without a BAA and a vendor security questionnaire, and I can't even start that conversation from this landing page.",
    "dealbreaker": true,
    "dealbreaker_reason": "No HIPAA compliance language anywhere on the page. I've been in this situation twice before. Without a BAA and clear data handling documentation, I cannot bring this to my legal department, and my legal department has final say. This is a complete non-starter for a hospital system.",
    "gut_reaction": "I can see this would be genuinely useful for my team's throughput, and I'm not dismissing it \u2014 but this page is aimed at academic and nonprofit users, not healthcare, and nothing about it suggests the vendor has thought about HIPAA.",
    "unanswered_questions": "Is a BAA available? Where is uploaded data stored and processed? Has the platform undergone a HIPAA security risk assessment? Is there a healthcare-specific tier or compliance documentation package? What happens to uploaded data after analysis \u2014 is it retained, and for how long?",
    "price_reaction": "Price sensitivity is not my issue. We have budget. The constraint is legal and compliance, not financial. A $25/month tool that creates a HIPAA liability costs us infinitely more than a $2,500/month tool that doesn't.",
    "name": "Mark Hill",
    "clarity_score": "partial"
  },
  {
    "person_id": 49,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "The page is clear about what it does, but it deliberately avoids the questions that actually matter to me.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "The entire framing is built around removing methodological judgment from the user \u2014 'you don't need to know which' \u2014 which is precisely what makes me distrust it. That's not a feature, that's a liability. The objection handling section answers easy objections from unsophisticated users and completely sidesteps the question a researcher would actually ask: how does the test-selection algorithm work, what are its assumptions, and what happens when the data doesn't cleanly fit a category? The claim of 'auditable' is doing enormous work with zero supporting detail.",
    "objections": "What exactly does 'detects your data types' mean methodologically \u2014 is it checking for normality, skewness, sample size, or just column data types? When it 'picks Spearman or Pearson,' what decision rule does it use and what are the edge cases? What does the audit trail actually contain \u2014 the name of the test and the output, or the actual decision pathway? How does it handle violations of assumptions? Is this built on a validated statistical engine or is it an LLM making real-time test selection decisions?",
    "dealbreaker": true,
    "dealbreaker_reason": "The copy's selling point \u2014 'you don't need to know which test' \u2014 is my primary concern. A tool that makes consequential statistical decisions opaque to the researcher is a methodological risk, not a convenience. The page offers no technical transparency about the decision logic, and the word 'auditable' without specifics is not evidence.",
    "gut_reaction": "This is built for people who are afraid of statistics, not people who use statistics \u2014 I understand why that market exists, but the copy actively reassures me that I can ignore the decisions the tool is making, which is the opposite of what I need to hear.",
    "unanswered_questions": "What statistical engine or framework underlies the test selection? Is the test-selection logic documented anywhere? What does the audit trail actually log? How does it handle non-normal distributions, small n, or mixed data types in the same analysis? Has it been validated against known datasets with documented correct answers?",
    "price_reaction": "Price is irrelevant at this stage \u2014 I wouldn't pay $10/month or $0/month for a tool I can't evaluate methodologically.",
    "name": "Rebecca Valencia",
    "clarity_score": "partial"
  },
  {
    "person_id": 50,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "Perfectly clear as a pitch to non-statisticians, which is exactly why it doesn't land for me.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "I've actually tested tools like this out of professional curiosity, and the pattern is consistent: they handle the textbook cases and fall apart on anything with complexity \u2014 partially crossed designs, non-independence, hierarchical structures. The copy doesn't acknowledge this at all. It presents a simplicity that's reassuring to scared users and alarming to me. 'It picks the right test' is a claim that requires extraordinary support and gets none here. Also, I notice 'inter-rater reliability' is listed as a Team feature with no description \u2014 that's a technically loaded term and tossing it in a feature list without context tells me nothing about whether they've implemented it correctly.",
    "objections": "What design types does it support \u2014 can it handle crossed random effects, nested designs, repeated measures with multiple factors? What happens when the 'right test' conditions aren't met \u2014 does it flag assumption violations or silently proceed? How is inter-rater reliability calculated \u2014 Cohen's kappa, Krippendorff's alpha, percent agreement? Is the analysis reproducible with a fixed seed?",
    "dealbreaker": true,
    "dealbreaker_reason": "I consult for other PhD students and I've seen what happens when they use outputs they don't understand \u2014 the errors are subtle and consequential. A tool that proudly advertises removing the need for methodological judgment is one I would actively warn people away from until I can verify its decision logic is sound.",
    "gut_reaction": "This is the tool I'd tell my advisees to use for exploratory data familiarization only, and even then I'd want to see what it actually does with a complex design before recommending it.",
    "unanswered_questions": "Does it support factorial and nested designs? What assumption checks does it run before selecting a test? What is the source of the test-selection decision logic \u2014 is it rule-based or model-based? Can I export the raw analysis in R or Python to verify the output independently?",
    "price_reaction": "At $10/month the price is low enough that I might try it as a professional exercise, but I would not use the outputs for anything I'm advising on without extensive validation.",
    "name": "Paul Wilson",
    "clarity_score": "partial"
  },
  {
    "person_id": 51,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "The copy is clear and well-written, but it describes a workflow that would create accountability problems in my work environment.",
    "intent": "strongly_disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "My job is to be the person who can reproduce and verify analyses. 'Auditable' and 'exportable' are words that sound right but have no specification here \u2014 auditable to what standard? Can I reproduce the exact result with the exact same inputs a week later? Can I see the code that was executed? When I review a colleague's AI-assisted analysis, I need to be able to trace every decision. The copy implies transparency but delivers reassurance, which is not the same thing. Also the framing around 'you don't need to know which test' is a direct conflict with the clinical research environment \u2014 I need the analyst to know which test and why, because they're going to have to defend it.",
    "objections": "Is the analysis reproducible to the bit level given the same inputs? Can the full analysis be exported as runnable R or Python code? What does the audit trail log \u2014 just the test name and result, or the complete decision pathway including assumption checks? How does it handle missing data \u2014 listwise deletion, multiple imputation, something else \u2014 and is that configurable? What's the false positive rate for its test recommendations on non-textbook data?",
    "dealbreaker": true,
    "dealbreaker_reason": "I've seen AI-assisted errors reach downstream collaborators and get incorporated into papers. The copy's promise of 'auditable' results with no technical specificity is not sufficient for a clinical research context where my name is on the output.",
    "gut_reaction": "I want a tool that makes my colleagues more accountable for their analysis decisions, not less \u2014 this tool appears to move in the opposite direction.",
    "unanswered_questions": "What is the reproducibility guarantee? Can I export runnable code? How are assumption violations surfaced to the user? Who is responsible for methodological correctness \u2014 the tool or the analyst?",
    "price_reaction": "The pricing is irrelevant to my decision \u2014 this is a methodology question, not a budget question.",
    "name": "Gina Moore",
    "clarity_score": "partial"
  },
  {
    "person_id": 52,
    "bucket": "skeptical_methodologist",
    "resonance": "neutral",
    "clarity_response": "The problem framing is accurate for a specific user type and the solution description is coherent, but it systematically avoids the methodological questions I'd need answered.",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "fair",
    "strongest_line": "The tool that doesn't know what it doesn't know is more dangerous than no tool at all.",
    "what_feels_off": "This copy was clearly written for people who are afraid of their data, which is a real and large market I understand. But I've been watching AI analysis tools for two years and my consistent finding is that they're confident in edge cases where they should be uncertain. The copy doesn't acknowledge this \u2014 it promises the tool will pick the right test without any discussion of what 'right' means, what happens at the boundary conditions, or how it communicates its own uncertainty. For a tool claiming to democratize rigorous analysis, the absence of any discussion of limitations is itself a red flag.",
    "objections": "How does the tool communicate uncertainty about its own recommendations? Does it ever say 'this analysis may not be appropriate for your data' and under what conditions? What's the scope of analysis types supported \u2014 is it limited to common parametric tests or does it extend to more complex designs? Has it been benchmarked against expert analyst outputs on the same datasets?",
    "dealbreaker": false,
    "dealbreaker_reason": "I would try it \u2014 I try most tools in this space \u2014 but I'm not convinced it would meet my methodological standards for consulting or teaching use.",
    "gut_reaction": "I'd run it through its paces with some deliberately tricky datasets to see where it breaks, which is what I do with every tool in this category, and I expect this one will have the same failure pattern the others have.",
    "unanswered_questions": "Does it communicate uncertainty or limitation conditions? What analysis types are out of scope? Has it been externally validated? Can I see the test-selection logic documented anywhere?",
    "price_reaction": "Ten dollars a month is low enough that I'd pay it to evaluate the tool professionally \u2014 that's roughly what I'd spend to access a paper I need.",
    "name": "Michael Travis",
    "clarity_score": "partial"
  },
  {
    "person_id": 53,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "The copy communicates what the tool does, but it treats transparency as a marketing claim rather than a demonstrated property.",
    "intent": "strongly_disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "I've declined to evaluate tools like this twice when product managers asked because the methodology documentation wasn't there. This landing page has the same problem \u2014 it claims the results are transparent and auditable, but there is no methodology documentation here, no description of the underlying statistical engine, no specification of what 'detects your data types' actually means in terms of the assumptions being checked. 'Auditable' is a word that means something specific in research contexts and its use here looks like it was chosen for emotional reassurance rather than technical precision. I'm not opposed to AI in research \u2014 I'm opposed to AI in research that can't tell me what assumptions it's making, and this page has not told me.",
    "objections": "Where is the methodology documentation? What statistical framework underlies the test selection \u2014 is it a decision tree, a model, an LLM prompt? What assumptions does the tool check before recommending a test, and how does it handle data that violates those assumptions? What does 'auditable' mean operationally \u2014 can I get a complete log of every decision the system made during my analysis?",
    "dealbreaker": true,
    "dealbreaker_reason": "I would want methodology documentation before evaluating the tool at all. A landing page that leads with emotional resonance and buries (or omits) technical specifics is not a tool I trust with research decisions.",
    "gut_reaction": "Well-written copy for its target audience, which is not me \u2014 the deliberate removal of methodological visibility is the product's selling point and my primary objection.",
    "unanswered_questions": "Where is the technical documentation? What is the statistical basis for test selection? What are the documented limitations of the system? Has it been validated by statisticians?",
    "price_reaction": "Price is not a factor in my decision \u2014 I would not use a tool whose methodology I cannot evaluate regardless of cost.",
    "name": "Christine Wright",
    "clarity_score": "partial"
  },
  {
    "person_id": 54,
    "bucket": "skeptical_methodologist",
    "resonance": "neutral",
    "clarity_response": "The copy is genuinely clear and addresses a real problem, but it sidesteps the concern I'd actually bring to a tool like this.",
    "intent": "neutral",
    "conversion_confidence": "neutral",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "I understand why this tool exists and I'm not hostile to it \u2014 my concern is downstream. My graduate students are going to find this, use it, and accept its outputs without knowing how to evaluate them. The copy actively celebrates that: 'you don't need to know which test.' That's useful for a practicing professional who needs to move fast, but for a student who needs to develop methodological judgment, it's potentially harmful. The page doesn't address this at all \u2014 there's no framing around what the tool is appropriate for versus what requires a methodologist. I'm not the primary audience here, but I teach the people who are, and I'd want to see evidence that the tool builds understanding rather than substituting for it.",
    "objections": "Does the tool explain why a test was chosen in enough detail that a student could learn from it? Or does it just tell them the answer? Are there any guardrails for high-stakes analysis contexts where methodological oversight is required? Does it communicate the limits of its own analysis capabilities anywhere visible in the product?",
    "dealbreaker": false,
    "dealbreaker_reason": "This is not a dealbreaker for my personal use \u2014 I'd try the free tier and evaluate it on that basis. My concern is how it's used by people with less statistical training than I have, which a landing page can't fully resolve.",
    "gut_reaction": "I can see myself recommending this to certain students for exploratory work while being clear about its limitations, but I'd need to spend time in the product first to understand whether it teaches or just delivers.",
    "unanswered_questions": "Does the tool explain its reasoning in a way that builds statistical literacy, or does it just output an answer? How does it handle analysis requests that are outside its competence? Is there documentation I can review before the free trial?",
    "price_reaction": "At $10/month this is accessible enough that I might recommend it to students as a paid tool \u2014 the free tier being unlimited in time is genuinely useful for my context.",
    "name": "Shannon Walker",
    "clarity_score": "partial"
  },
  {
    "person_id": 55,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "The copy is clear enough about what the product does, but it never addresses the questions I would actually need answered before using it.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "The Pearson vs. Spearman example is the most technically specific thing on the page, and it reveals the problem: the copy treats test selection as a black box that users should be glad they don't have to think about. I've spent 18 months cataloguing exactly where that reasoning breaks down. When a tool picks Spearman 'based on your data,' what criteria is it using? Normality tests? Monotonicity checks? Both? Neither? The copy treats this as a feature to celebrate. I see it as the thing I need explained before I can evaluate the product at all. The 'no black boxes' line is contradicted by everything else on the page.",
    "objections": "The auditable trail claim is the only substantive technical commitment made, and it's vague. My clients are universities and government agencies \u2014 I need to know what the audit trail actually contains before I could recommend this to anyone. The 'it picks the right test' framing is the opposite of what methodological accountability looks like. Right by whose definition, under what data conditions, with what handling of violations? The whole value proposition is built around removing expert judgment from analysis, which is precisely the thing I get paid to supply.",
    "dealbreaker": true,
    "dealbreaker_reason": "The product's core value proposition \u2014 automating test selection and hiding methodological decisions from the user \u2014 is the exact failure mode I document in my consulting work. I cannot recommend a tool to university and government clients that makes consequential analytical decisions it won't explain, regardless of how cheap it is.",
    "gut_reaction": "This is a well-written page for the wrong audience if you send it to me \u2014 it's built for people who are afraid of analysis, and I'm someone who evaluates whether analysis is being done correctly. The things it treats as selling points are the things I'd flag as risks.",
    "unanswered_questions": "What algorithm governs test selection? What happens with non-normal data, small samples, or violations of test assumptions \u2014 does it flag these or silently proceed? What does the audit trail actually contain \u2014 enough to reproduce the analysis independently? Has the platform been validated against any benchmark datasets? Does it handle weighted survey data?",
    "price_reaction": "The pricing is almost irrelevant to my evaluation \u2014 $10/month is trivially low for professional use, but a free tool I can't trust methodologically is worth less than a tool I trust that costs $500/month. Price is not the variable I'm solving for.",
    "name": "Victoria Garcia",
    "clarity_score": "partial"
  },
  {
    "person_id": 56,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "The page is clear about its pitch \u2014 I just find the pitch professionally troubling.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language.",
    "what_feels_off": "I've said publicly on my department's methods listserv that tools like this are useful for exploratory orientation but not rigorous enough for publication-quality work, and nothing on this page would change that view. The framing 'you don't need to know which' test to use is presented as liberation \u2014 I read it as methodological abdication. Method selection is not a burden to be eliminated; it's where disciplinary expertise lives. The copy addresses itself to people who don't want to develop that expertise, which is a legitimate market, but I want to be honest that it's a different market from the one I work in.",
    "objections": "I've tried three platforms like this. They all handled basic cases correctly and made subtle, hard-to-detect errors on more complex designs. This page gives me no reason to believe this product is different. The 'auditable' claim is the only thing that gestures toward methodological transparency, but it's not substantiated. What does an audit trail from an automated test-selection tool actually contain? If it's just 'we ran a t-test because we detected two groups,' that's not auditable in any sense that matters.",
    "dealbreaker": true,
    "dealbreaker_reason": "I've been publicly vocal that tools in this category are not ready for publication-quality social science research, and I would need substantial evidence to revise that position \u2014 evidence this page does not provide.",
    "gut_reaction": "Competently written for its target audience, which is not me \u2014 I'd probably bookmark the 'audit trail' claim and come back to test it against something genuinely complex, but I would not recommend this to grad students until I'd done that testing myself.",
    "unanswered_questions": "What is the actual audit trail format? Has this been tested against publication-quality research standards? How does the test selection algorithm behave on edge cases \u2014 small samples, ordinal data treated as continuous, violations of independence? Is there a way to override the automated test selection and specify your own?",
    "price_reaction": "$10/month is nothing, but I'm not evaluating this on price. I'm evaluating it on whether I could put my name behind recommending it to the PhD students who come to my workshops \u2014 and I can't do that based on what this page tells me.",
    "name": "Jason Powell",
    "clarity_score": "partial"
  },
  {
    "person_id": 57,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "The page describes a general-purpose analysis tool, but it never mentions survey methodology, sampling, or weighting, which are the things I would need to see addressed.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "good_deal",
    "strongest_line": "Upload a CSV, a stack of interview transcripts, or survey exports.",
    "what_feels_off": "Every AI analysis tool I've evaluated fails on complex survey data \u2014 weighted samples, stratified designs, nonresponse adjustment. This page doesn't mention any of that. The word 'weight' doesn't appear. The word 'sampling' doesn't appear. The problem statement addresses graduate students pasting things into ChatGPT, which is real, but that's not my world. My world is federally funded surveys with design effects, calibration weights, and variance estimation using replicate weights. The page doesn't tell me whether this tool is in the same universe as those requirements, or whether it's built for simple flat-file CSV analysis. That ambiguity is itself a red flag.",
    "objections": "The 'click Correlate and it picks Spearman or Pearson' example is a basic bivariate case. My simplest analysis is more complex than that. If the tool's paradigm case is Pearson vs. Spearman, I have serious doubts about whether it handles Taylor series linearization for variance estimation. I sit on an AAPOR standards committee. I can't use or recommend tools I can't verify against those standards.",
    "dealbreaker": true,
    "dealbreaker_reason": "The tool appears not to address complex survey design at all \u2014 no mention of weights, stratification, clustering, or replicate variance estimation. These are not optional features for my work; they are the work.",
    "gut_reaction": "This page is not written for me, and that is fine \u2014 it's clearly written for a different user who has simpler data and less specialized needs. My specific concern is that researchers with complex survey data might land on this page and assume it handles their requirements when it almost certainly does not.",
    "unanswered_questions": "Does the tool support survey weights? Can it handle stratified and clustered sample designs? Does it correctly propagate design effects through analysis? Can it produce design-based standard errors rather than model-based ones? What variance estimation methods does it support?",
    "price_reaction": "Irrelevant \u2014 I evaluate tools on technical capability first. Price doesn't enter the conversation until technical requirements are met, and they demonstrably aren't here.",
    "name": "Karen Pierce",
    "clarity_score": "partial"
  },
  {
    "person_id": 58,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "The page is clear about what it does, which is general statistical analysis \u2014 and equally clear that it doesn't address psychometrics, which is my domain.",
    "intent": "disagree",
    "conversion_confidence": "strongly_disagree",
    "price_perception": "good_deal",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "I've been pitched tools like this at conferences for years. The paradigm case is always Pearson vs. Spearman or t-test vs. ANOVA \u2014 the kind of analysis a first-year methods student does. My work is IRT model specification, factor analysis, differential item functioning. None of that appears on this page. The 'click a question and get the answer' framing is genuinely dangerous if applied uncritically to measurement work, because the questions in psychometrics are not simple enough to be answered by automated test selection. The copy doesn't acknowledge that some domains require specialized methods that general-purpose automation doesn't reach.",
    "objections": "I use R with mirt, lavaan, and TestAnaR. These are specialized tools for specialized questions. A general-purpose AI analysis platform claiming to handle uploads of 'CSV, Excel, or interview transcripts' has not addressed my use case at all. I'm genuinely waiting to be impressed by an AI tool in my space \u2014 but this page gives me nothing to evaluate, because it's not trying to speak to my use case.",
    "dealbreaker": true,
    "dealbreaker_reason": "Psychometric analysis is not a feature this tool appears to offer or even acknowledge. My baseline requirement for evaluation is demonstration of IRT capability. This page doesn't come close.",
    "gut_reaction": "This seems like a competent product for generalist research needs, and that's fine \u2014 but I've declined to evaluate tools like this repeatedly at conferences precisely because the tool category hasn't reached my domain yet, and this page confirms that's still true.",
    "unanswered_questions": "Does the tool support IRT models? Can it run confirmatory factor analysis with appropriate fit indices? Does it handle DIF analysis? Can it be extended with custom analysis specifications for non-standard psychometric models?",
    "price_reaction": "Price is not the variable. A tool that cannot run the analyses I require at any price is not relevant to me. A tool that could do psychometric work at $100/month would be immediately interesting.",
    "name": "Ms. Kimberly Adkins",
    "clarity_score": "partial"
  },
  {
    "person_id": 59,
    "bucket": "skeptical_methodologist",
    "resonance": "neutral",
    "clarity_response": "The page clearly describes what the tool does in general terms, but it doesn't tell me anything about how it handles the messy, incomplete data I actually work with.",
    "intent": "neutral",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Clean \u2014 map values, fix inconsistencies, handle missing data with point-and-click tools.",
    "what_feels_off": "Missing data is mentioned exactly once, as step two of a five-step workflow: 'handle missing data with point-and-click tools.' I've written a board brief about why missing data handling in AI analysis tools is my primary concern for community health research \u2014 it is endemic in our datasets, it is consequential, and the way tools handle it (listwise deletion by default? single imputation? multiple imputation?) determines whether the results are defensible. One bullet point that says 'handle missing data' tells me nothing. What methods? What defaults? What does the user see when 30% of their outcome variable is missing?",
    "objections": "I've done two formal evaluations of AI analysis tools \u2014 one for a grant, one for internal use \u2014 and both ended in 'not yet' decisions. The common thread was missing data handling. This page doesn't give me anything new on that question. I'm analytically optimistic about AI in research and I genuinely want a tool that works \u2014 but optimism has to be grounded in evidence, and the evidence this page offers is too thin. The 'auditable' claim is interesting and I'd want to test it specifically against a dataset with systematic missingness.",
    "dealbreaker": false,
    "dealbreaker_reason": "Unlike some of my colleagues, I'm not at hard no \u2014 I've reached 'not yet' twice before, and I'm willing to evaluate again. The free tier makes this lower-risk to test.",
    "gut_reaction": "This page hits on something real \u2014 the gap between collecting data and being able to analyze it is a genuine problem in community health research \u2014 but I need to see how it handles the specific conditions that break other tools before I'd consider recommending it.",
    "unanswered_questions": "How does the platform handle missing data \u2014 what methods are available, what are the defaults, and what does it communicate to the user when missingness is high? How does it perform on non-normally distributed community health data? Has it been validated on datasets with the kind of item-level nonresponse typical in community surveys? Can I see the audit trail for a missing data analysis specifically?",
    "price_reaction": "The free tier is genuinely appealing for a structured evaluation \u2014 it lowers the cost of testing my specific concerns before committing. $10/month is reasonable if it passes those tests, though for organizational use I'd need to understand the Team tier more. The pricing isn't the barrier; the evidence is.",
    "name": "Christina Walters",
    "clarity_score": "partial"
  },
  {
    "person_id": 60,
    "bucket": "skeptical_methodologist",
    "resonance": "disagree",
    "clarity_response": "The page is clearly written and makes its argument competently \u2014 I just think the argument itself is epistemologically problematic.",
    "intent": "disagree",
    "conversion_confidence": "disagree",
    "price_perception": "fair",
    "strongest_line": "Every result shows the method used, the parameters, and what it means in plain language. Auditable. Exportable.",
    "what_feels_off": "I have been raising questions publicly in forums about what these tools do to statistical education, and this page is almost a case study in what I'm worried about. The value proposition is: you don't have to understand the method, you just click and get an answer. That is a coherent product strategy. It is also, in my view, a pedagogy problem at scale. When students circumvent the learning process \u2014 understanding why you choose a t-test versus a Wilcoxon, what it means when your residuals look a certain way \u2014 they don't just make one bad analysis. They develop a relationship to data where they trust outputs they can't evaluate. The copy does not address this at all. The 'auditable' trail is the only nod toward transparency, and I'd want to know whether the audit trail actually teaches anything or just documents that a decision was made.",
    "objections": "I've tested three platforms with deliberately challenging datasets. Each one communicated uncertainty inadequately. I've used R with Stan for Bayesian inference \u2014 my standard for uncertainty communication is high. 'What it means in plain language' is exactly the phrase that makes me suspicious, because the thing that plain language most often strips out is precisely the epistemic hedging that makes a statistical claim defensible. A p-value in plain language without the assumptions that underlie it is not an honest translation; it's a simplification that leaves out the part that matters.",
    "dealbreaker": true,
    "dealbreaker_reason": "I would not use or recommend this tool in my research center without substantially more evidence that the uncertainty communication and audit trail meet the standards I train researchers to apply \u2014 and I am actively raising concerns in public forums about tools in this category, so recommending one carries reputational weight I'm not willing to commit based on a landing page.",
    "gut_reaction": "Well-constructed copy targeting real pain, but the pain it's solving \u2014 the inconvenience of methodological expertise \u2014 is, from where I sit, not actually a problem to be solved; it's the competence that makes research trustworthy.",
    "unanswered_questions": "What does the audit trail actually contain \u2014 enough to reconstruct the analysis from scratch? How does the platform communicate uncertainty \u2014 does it report confidence intervals, effect sizes, and assumption checks, or just significance? Has the test-selection algorithm been published or validated externally? What happens when the automated selection is wrong \u2014 does the tool flag edge cases or proceed silently? Can advanced users override or inspect the selection logic?",
    "price_reaction": "$10/month and a free tier suggest this is targeted at individual researchers and students, which is consistent with the copy. My concern is not cost \u2014 it's that tools at this price point and accessibility level will be adopted by students and junior researchers who lack the background to evaluate the outputs, and I don't see anything on this page that addresses that downstream risk.",
    "name": "Dustin Jordan",
    "clarity_score": "partial"
  }
]